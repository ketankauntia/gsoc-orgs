{
  "id": "692251ce53dd9d7326d33d65",
  "slug": "cern-hsf",
  "name": "CERN-HSF",
  "category": "Science and medicine",
  "description": "Umbrella for Particle Physics-related projects",
  "image_url": "https://summerofcode.withgoogle.com/media/org/cern-hsf/cjus658sjzba5zhg-360.png",
  "img_r2_url": "https://pub-268c3a1efc8b4f8a99115507a760ca14.r2.dev/cern-hsf.webp",
  "logo_r2_url": null,
  "url": "http://hepsoftwarefoundation.org/activities/gsoc.html",
  "active_years": [
    2017,
    2018,
    2019,
    2020,
    2021,
    2022,
    2023,
    2024,
    2025
  ],
  "first_year": 2017,
  "last_year": 2025,
  "is_currently_active": true,
  "technologies": [
    "python",
    "c",
    "c++",
    "machine learning",
    "data analysis",
    "parallel algorithms",
    "parallelization",
    "concurrency",
    "container orchestration",
    "artificial intelligence"
  ],
  "topics": [
    "machine learning",
    "big data",
    "physics",
    "particle physics",
    "high-energy physics",
    "performance optimization",
    "algorithmics",
    "big data science",
    "Performance Optimisation"
  ],
  "total_projects": 214,
  "stats": {
    "avg_projects_per_appeared_year": 23.78,
    "projects_by_year": {
      "year_2016": null,
      "year_2017": 20,
      "year_2018": 26,
      "year_2019": 29,
      "year_2020": 34,
      "year_2021": 25,
      "year_2022": 21,
      "year_2023": 16,
      "year_2024": 17,
      "year_2025": 26
    },
    "students_by_year": {
      "year_2016": null,
      "year_2017": 20,
      "year_2018": 26,
      "year_2019": 29,
      "year_2020": 34,
      "year_2021": 25,
      "year_2022": 21,
      "year_2023": 16,
      "year_2024": 17,
      "year_2025": 26
    },
    "total_students": 206
  },
  "years": {
    "year_2016": null,
    "year_2017": {
      "num_projects": 20,
      "projects": [
        {
          "code_url": "https://sd57.github.io/g4dprng/",
          "description": "<p>Particle transport Monte Carlo simulations are a key tool for High Energy Physics experiments, including the LHC experiments at CERN. All Monte Carlo (MC) simulations depend vitally on Pseudo-Random Number Generators (PRNGs) used to sample many distributions.\nEach LHC experiments generates 5-10 Billion sampled physics events a year using around 10^18 sampled values from PRNGs. PRNGs take 1-5% of CPU time. PRNGs used must possess very large periods, fast execution, the ability to create a large number of streams, and very good correlation properties between streams and sampled values. It must be possible to reproduce a simulated event any time, for reexamination or debugging.\nThe transition from event-level parallelism in Geant4 to dynamical multithreading in GeantV requires associating the random generator states not with the threads but with the tracks, which are shuffled between \"baskets\" based on their type and energy. Maintaining reproducibility after such transition requires a drastic change in the way the random generators are used. We suggest solutions based on Scalable Pseudo Random Number Generators and on pedigrees introduced in CilkPlus, which are first tested in Geant4.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6471566527299584/",
          "proposal_id": null,
          "short_description": "Particle transport Monte Carlo simulations are a key tool for High Energy Physics experiments, including the LHC experiments at CERN. All Monte Carlo...",
          "slug": "using-pseudo-random-number-repeatably-in-a-fine-grain-multithreaded-simulation",
          "status": "completed",
          "student_name": "Dima Savin",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Using Pseudo-random number repeatably in a fine-grain multithreaded simulation"
        },
        {
          "code_url": "https://gist.github.com/Bastiantheone/03921bb8207c37e2e8306101e7e3bcaa",
          "description": "<p>Fads is a fast detector simulation toolkit in Go used for High Energy Physics analyses. The current version is not very scalable and it takes up too much CPU. To make fads competitive a faster jet clustering strategy using Delaunay triangulation will be implemented.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6624607083692032/",
          "proposal_id": null,
          "short_description": "Fads is a fast detector simulation toolkit in Go used for High Energy Physics analyses. The current version is not very scalable and it takes up too...",
          "slug": "jet-clustering-optimizations-in-fads",
          "status": "completed",
          "student_name": "Bastian Wieck",
          "student_profile": null,
          "tags": [],
          "title": "Jet Clustering Optimizations in Fads"
        },
        {
          "code_url": "https://sshekh.github.io/blog/gsoc/index",
          "description": "<p>Toolkit for Multivariate Analysis (TMVA) is a machine learning toolkit for the ROOT scientific software framework. It contains a variety of techniques for data analysis and prediction such as Likelihood Estimation, Support Vector Machines, Decision trees, etc. It is used mainly for particle physics applications. Currently only feed forward Deep Neural Networks with GPU support are available in the TMVA. Recently CNNs and other DNNs have proven useful in a variety of applications like classification and tracking of particles, detection of new particles, etc. The aim of this project is to expand the current library of TMVA DNN with optimized Convolutional Neural Networks (CNN)</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5404527133982720/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a machine learning toolkit for the ROOT scientific software framework. It contains a variety of...",
          "slug": "convolutional-deep-neural-networks-on-gpus-for-particle-physics-applications",
          "status": "completed",
          "student_name": "Saurav Shekhar",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Convolutional Deep Neural Networks on GPUs for Particle Physics Applications"
        },
        {
          "code_url": "https://gist.github.com/dvush/a589e94874d0db71e218f117f96140ca",
          "description": "<p>This is proposal for implementing suggested HPC aware infrastructure with workload balancer.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5475198807572480/",
          "proposal_id": null,
          "short_description": "This is proposal for implementing suggested HPC aware infrastructure with workload balancer.",
          "slug": "optimisation-of-geantv-hpc-workload-balancing",
          "status": "completed",
          "student_name": "Vitalii Drohan",
          "student_profile": null,
          "tags": [],
          "title": "Optimisation of GeantV HPC workload balancing"
        },
        {
          "code_url": "https://github.com/krishnan-r/sparkmonitor",
          "description": "<p>Jupyter Notebook is an interactive computing environment that is used to create notebooks which contain code, output, plots, widgets and theory. Jupyter notebook offers a convenient platform for interactive data analysis, scientific computing and rapid prototyping of code. A powerful tool used to perform complex computation intensive tasks is Apache Spark. Spark is a framework for large scale cluster computing in Big Data contexts. This project aims to leverage these existing big data tools for use in an interactive scientific analysis environment. Currently Spark jobs can be called from Jupyter Notebook using the pySpark module. However to know what is happening to a running job, it is required to connect separately to the Spark server. This project aims to develop a plugin to monitor jobs sent from a notebook application, from within the notebook itself. The plugin will have features to monitor tasks, stop ongoing jobs and detect errors. ROOT is a data analysis framework, widely used in the scientific community at CERN. The plugin designed will be used to monitor processing of ROOT objects using Spark. The plugin will also be used to monitor distributed machine learning tasks.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2017_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5045066422812672/",
          "proposal_id": null,
          "short_description": "Jupyter Notebook is an interactive computing environment that is used to create notebooks which contain code, output, plots, widgets and theory....",
          "slug": "big-data-tools-for-physics-analysis",
          "status": "completed",
          "student_name": "Krishnan R",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "ui"
          ],
          "title": "Big Data Tools for Physics Analysis"
        },
        {
          "code_url": "https://dmitrysorokin.github.io/GSoC2017/",
          "description": "<p>Track of a charged particle in a non-uniform electromagnetic field can be described by the first-order initial value problem (IVP). Robustness and efficiency of the integration process is important for overall performance of the simulation code.  Geant4 and GeantV use explicit Runge-Kutta (RK) methods with the adaptive stepsize control. I propose to implement several methods which may have a better performance for the problem of interest.</p>\n<ul>\n<li>New stepsize control algorithm </li>\n<li>Integration methods with stable equilibrium states</li>\n<li>Methods especially designed for periodic problems</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5253859681239040/",
          "proposal_id": null,
          "short_description": "Track of a charged particle in a non-uniform electromagnetic field can be described by the first-order initial value problem (IVP). Robustness and...",
          "slug": "new-error-control-methods-for-integration-of-trajectories",
          "status": "completed",
          "student_name": "Dmitry Sorokin",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "New error control methods for integration of trajectories"
        },
        {
          "code_url": "https://gist.github.com/giorgosp/3e9779299aca9737d038130898ee79b8#file-readme-md",
          "description": "<p>Assess the performance of ROOT's asynchronous data prefetching compared to standard prefetching mechanism for reading remote ROOT files.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5326115929325568/",
          "proposal_id": null,
          "short_description": "Assess the performance of ROOT's asynchronous data prefetching compared to standard prefetching mechanism for reading remote ROOT files.",
          "slug": "assess-roots-asynchronous-data-prefetching",
          "status": "completed",
          "student_name": "George Papadrosou",
          "student_profile": null,
          "tags": [],
          "title": "Assess ROOT's asynchronous data prefetching"
        },
        {
          "code_url": "https://medium.com/towards-data-science/gsoc-2017-working-on-anomaly-detection-at-cern-hsf-49766ba6a812",
          "description": "<p>This project is aimed towards building a framework which monitors incoming ATLAS Computing Operations data for anomalies and then autonomously, acts on this information by either solving the problem or by proposing the best solution. \nThe solution proposed for this has at its heart two machine learning models-</p>\n<ul>\n<li>One a recurrent network to actually predict anomalies in the incoming real-time data. These predicted anomalies will then be analysed to come up with a list of potential solutions to the problem</li>\n<li>Second a reinforcement learning algorithm to rank the proposed solutions based on a feedback. This algorithm will provide increasingly efficient outputs over time, thus improving the overall efficiency of the framework.</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5341432319574016/",
          "proposal_id": null,
          "short_description": "This project is aimed towards building a framework which monitors incoming ATLAS Computing Operations data for anomalies and then autonomously, acts...",
          "slug": "deep-anomaly",
          "status": "completed",
          "student_name": "Vyom Sharma",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Deep Anomaly"
        },
        {
          "code_url": "https://agarciamontoro.github.io/2017/08/25/gsoc-final-thoughts.html",
          "description": "<p>ROOT is a data analysis software used by scientists all over the world; therefore, its efficiency is critical for the scientific community in general and for CERN in particular. This project aims to improve the efficiency of one of the most used ROOT features: <strong>the mathematical functions and the fitting methods</strong>, which will be accomplished by making the corresponding code vectorized and/or parallelized. The main tasks that will be covered during the coding period are:</p>\n<ul>\n<li>Complete the parallelization and vectorization of all the fitting methods available in ROOT: <code>Chi2FCN</code>, <code>LogLikelihoodFCN</code> and <code>PoissonLikelihoodFCN</code>.</li>\n<li>Adapt the gradient function interfaces for thread-based parallelization and vectorization.</li>\n<li>Vectorize <code>TFormula</code> and predefined ROOT functions (<code>gaus</code>, <code>landau</code>, <code>expo</code>, <code>crystalball</code>, ...).</li>\n<li>Vectorize the most used mathematical and statistical functions in <code>ROOT::TMath</code> and <code>TMath</code>.</li>\n</ul>\n<p>If time allows it, the <em>special</em> and <em>statistical</em> functions implemented in ROOT will be also vectorized and/or parallelized.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5350915607363584/",
          "proposal_id": null,
          "short_description": "ROOT is a data analysis software used by scientists all over the world; therefore, its efficiency is critical for the scientific community in general...",
          "slug": "improvements-in-vectorization-and-parallelization-of-root-math-libraries",
          "status": "completed",
          "student_name": "Alejandro García Montoro",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Improvements in vectorization and parallelization of ROOT Math libraries"
        },
        {
          "code_url": "https://github.com/corona10/GSoC2017",
          "description": "<p>gopy is an excellent tool which generates (and compiles) a CPython extension module from a go package. And I hope more developers could make full use of gopy to migrate their go code into python code. \nTo make gopy more advanced, It is necessary to provide APIs for various Python compiler versions, such as CPython 2/3 and PyPy. This can be improved with CFFI or ctypes.\nMoreover, many go’s implementations/features are not yet implemented in gopy. \nSo we need to implement implementations such as slices, interfaces, and maps in the go.</p>\n<p>My goal is to update gopy by using CFFI to support Python3 and PyPy and write detailed documents.</p>\n<p>Github: <a href=\"https://github.com/go-python/gopy\" target=\"_blank\">https://github.com/go-python/gopy</a></p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2017_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5352263690223616/",
          "proposal_id": null,
          "short_description": "gopy is an excellent tool which generates (and compiles) a CPython extension module from a go package. And I hope more developers could make full use...",
          "slug": "updating-gopy-to-support-python3-and-pypy",
          "status": "completed",
          "student_name": "Dong-hee Na",
          "student_profile": null,
          "tags": [
            "python",
            "api",
            "ai"
          ],
          "title": "Updating gopy to support Python3 and PyPy"
        },
        {
          "code_url": "https://github.com/MultithreadCorner/Hydra.Python/blob/master/README.md",
          "description": "<p>‘Effective Python routines for analysis on massively multi-threaded platforms-Python bindings for the Hydra C++ library'. The title of this project suggests that we need effective Python routines for some kind of intensive heavy-duty data analysis on multi-threaded platforms. It also suggests that Python bindings for the Hydra C++ library. Hydra library is a C++ header-only library which is used for high performance and precision. Since in this project, we are writing Python package for the binding of Hydra library, we can expect to have the dynamicity of Python combined with the performance and precision of C++. In large experiments when we have terabytes of data to perform analysis on we need performance and at the same time, we want tools which are not too hard to use. Keeping this in mind, after completing this project we can expect to have such kind of Python package which anyone can use without any performance and precision lose.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6217497468469248/",
          "proposal_id": null,
          "short_description": "‘Effective Python routines for analysis on massively multi-threaded platforms-Python bindings for the Hydra C++ library'. The title of this project...",
          "slug": "efficient-python-routines-for-analysis-on-massively-multi-threaded-platforms-python-bindings-for-the-hydra-c-library",
          "status": "completed",
          "student_name": "Deepanshu Thakur",
          "student_profile": null,
          "tags": [
            "python"
          ],
          "title": "Efficient Python routines for analysis on massively multi-threaded platforms-Python bindings for the Hydra C++ library"
        },
        {
          "code_url": "https://briancylui.github.io/falcon/",
          "description": "<p>Toolkit for Multivariate Data Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software network, used in many particle physics data analysis and applications.  Current implementations of TMVA have expanded regression from the single-target level to the multi-target level, yet only restricted to two certain types of regression.  In order to fully accommodate the need for a well-supported, robust multi-target regression framework for particle physicists, this summer we are expanding the toolkit capabilities to cover multi-objective function estimation from more diverse algorithmic approaches.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6223742887788544/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Data Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software network, used...",
          "slug": "machine-learning-project-multi-target-regression-for-particle-physics",
          "status": "completed",
          "student_name": "Brian Lui",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Machine Learning Project: Multi-target Regression for Particle Physics"
        },
        {
          "code_url": "https://docs.google.com/document/d/1QkyN-NsQcOPOrnyOJqn1ehXkJCM756KkKv3Dj0ltDls/edit?usp=sharing",
          "description": "<p>The CMS (Compact Muon Solenoid) is a high-energy physics experiment at the LHC. The CMS PhEDEx(Physics Experiment Data Export) project is responsible for facilitating large-scale data transfers across the grid ensuring transfer reliability, enforcing data placement policy, and accurately reporting results and performance statistics. CMS tracks over 2 PB of data using PhEDEx. The architecture of PhEDEx is made up of the distributed, loosely coupled agents.</p>\n<p>The transfer2go is R&amp;D project within CMS collaboration. The aim of this project is to extend basic PhEDEX functionality to address upcoming challenges in exabyte HL-HLC era via implementation of modern Go programming language.</p>\n<p>This proposal offers to make transfer2go even better by adding simple router which can intelligently choose which agent to use based on some underlying conditions. This router will be helpful to reduce the traffic on the single agent and will be helpful to decrease the data transfer latency.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6256225993883648/",
          "proposal_id": null,
          "short_description": "The CMS (Compact Muon Solenoid) is a high-energy physics experiment at the LHC. The CMS PhEDEx(Physics Experiment Data Export) project is responsible...",
          "slug": "next-generation-of-cms-data-replication-system",
          "status": "completed",
          "student_name": "Rishi Shah",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Next generation of CMS data replication system"
        },
        {
          "code_url": "https://github.com/kunal-iitkgp/Inference",
          "description": "<p>Extracting shape information from simulated CMS events\nUsing Tensorflow to train a DNN based on the shape of the hits, detector information and Montecarlo Truth\nMeasuring efficiency and fake rejection\nIntegrate in the CMS software the trained network</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/4675205045682176/",
          "proposal_id": null,
          "short_description": "Extracting shape information from simulated CMS events\nUsing Tensorflow to train a DNN based on the shape of the hits, detector information and...",
          "slug": "automatic-code-generation-for-fast-inference-in-the-cms-tracking-software",
          "status": "completed",
          "student_name": "Kunal_Singh_",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Automatic code generation for fast inference in the CMS tracking software"
        },
        {
          "code_url": "https://ilievskiv.github.io/2017/08/23/final-submission/",
          "description": "<p>The Convolutional Neural Networks (CNNs) are one special type of a deep learning neural networks with an enormous discriminative power for image classification. In fact, they significantly outperform the standard Computer Vision techniques of manually extracting image features and then building classifiers on top of them.</p>\n<p>Recently, the physicist started to harness the power of the CNNs in the task of particle tagging. In their paper <em>Oliveira et al. 2015</em> used the jet images in combination with CNNs to obtain better results to jet tagging in comparison to the standard physically-motivated features. Moreover, using the simulations of particle smashing, which in fact produces an accurate 3D image, is another potential use of the CNNs.</p>\n<p>Since ROOT is the state-of-the-art data analysis tool extensively used in the High Energy Physics, it is of paramount importance to integrate a CNN implementation in its submodule called TMVA. For this reason, throughout this project I will implement such a solution.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/4801233076355072/",
          "proposal_id": null,
          "short_description": "The Convolutional Neural Networks (CNNs) are one special type of a deep learning neural networks with an enormous discriminative power for image...",
          "slug": "convolutional-deep-neural-networks-on-gpus-for-particle-physics-applications",
          "status": "completed",
          "student_name": "Vladimir Ilievski",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Convolutional Deep Neural Networks on GPUs for Particle Physics Applications"
        },
        {
          "code_url": "https://github.com/ssomesh/SALLOC/tree/master",
          "description": "<p>The CUDA language, used for programming NVIDIA GPUs, is not dynamic-data-structure friendly since it was designed for working with data structures of predictable size. The project targets applications, intended to run on GPU, that involve many dynamic data structures with many small allocations and deallocations. We want to design and implement a scheme for dynamically allocating memory for data structures (eg. vector of vector) on demand. The allocator on GPU will allocate a pre-defined amount of memory at a time. The design will try to use this predefined size of memory to service the upcoming requests for a smaller amount of memory from this larger pool. This way the service of memory allocation will be fast as the allocation is already done. We will see how the implementation handles the challenges posed by the GPU  for allocating as per demand.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6549039684255744/",
          "proposal_id": null,
          "short_description": "The CUDA language, used for programming NVIDIA GPUs, is not dynamic-data-structure friendly since it was designed for working with data structures of...",
          "slug": "smart-data-structures-in-cuda",
          "status": "completed",
          "student_name": "Somesh Singh",
          "student_profile": null,
          "tags": [],
          "title": "Smart Data Structures in CUDA"
        },
        {
          "code_url": "https://docs.google.com/document/d/16Ccz0NPMKdVZ6QPSkB5NzQqz3O92OKPbsJJUGAiga1o/edit?usp=sharing",
          "description": "<p>OpenML is an open source project that aims to create a novel ecosystem for machine learning experimentation. It is possible to integrate different machine learning tools through plugins and APIs to share datasets, experiments and results in the platform. The idea is to write an OpenML’s plugin in Java for ROOT, that allows the use TMVA in OpenML, and also create an C++ API for TMVA that permits the use of OpenML tools from within ROOT.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6041482192486400/",
          "proposal_id": null,
          "short_description": "OpenML is an open source project that aims to create a novel ecosystem for machine learning experimentation. It is possible to integrate different...",
          "slug": "integration-of-tmva-and-openml-platform",
          "status": "completed",
          "student_name": "Sami Ullah",
          "student_profile": null,
          "tags": [
            "java",
            "api",
            "ml",
            "ai"
          ],
          "title": "Integration of TMVA and OpenML platform"
        },
        {
          "code_url": "https://vikasnt.github.io/pythia8/",
          "description": "<p>PYTHIA is used for generation of events in high-energy collisions. Originally developed in fortran until Pythia6.4, it was rewritten in C++ starting from Pythia8. One such process of interest is electron-ion collision. Currently, most of the process have been implemented in the C++ version but still a few remain to be implemented/tested.This project will aim to complete the missing implementation and test it using experimental results. Furthermore, we will attempt to build a framework for input-beam polarisation option in pythia8.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/4878223586361344/",
          "proposal_id": null,
          "short_description": "PYTHIA is used for generation of events in high-energy collisions. Originally developed in fortran until Pythia6.4, it was rewritten in C++ starting...",
          "slug": "electron-ion-collision-in-pythia8",
          "status": "completed",
          "student_name": "vikas gupta",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Electron-ion collision in Pythia8"
        },
        {
          "code_url": "https://medium.com/@akshayvashistha1995/google-summer-of-code-summary-92ce7ca48d18",
          "description": "<p>In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Due to the lack of training data and computing power in early days, it is hard to train a large high-capacity convolutional neural network without overfitting. After the rapid growth in the amount of the annotated data and the recent improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. GPUs are much more effective in utilizing parallelism and pipelining than general purpose CPUs, as they are designed for high performance rendering where repeated operations are common. This proposal proposes quick and efficient implementation of CNN on both GPU and multi-core CPU. I’ll use CUDA (compute unified device architecture) that can be easily programmed due to its simple C language-like style.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6326594738061312/",
          "proposal_id": null,
          "short_description": "In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and...",
          "slug": "convolutional-deep-neural-networks-on-gpus-for-particle-physics-applications",
          "status": "completed",
          "student_name": "ajatgd",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "ui"
          ],
          "title": "Convolutional Deep Neural Networks on GPUs for Particle Physics Applications"
        },
        {
          "code_url": "https://gist.github.com/vibhavp/f6041cf6e21c2b2285f4056b3230a2b8",
          "description": "<p>WebAssembly (wasm) is a low level bytecode binary file format made for execution on the Web, designed as a compilation target for various programming languages. The aim of this project is to implement a tooling environment for executing WebAssembly, specifically geared towards the <a href=\"https://golang.org/\" target=\"_blank\">Go Programming Language</a>.\nThis includes packages for:</p>\n<ul>\n<li>Reading wasm and wast files, displaying various information about the module - displaying their sections (imports, data, tables/functions, etc).</li>\n<li>Interpret wasm bytecode to execute simple functions.</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2017_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/5164191635734528/",
          "proposal_id": null,
          "short_description": "WebAssembly (wasm) is a low level bytecode binary file format made for execution on the Web, designed as a compilation target for various programming...",
          "slug": "launching-wagon-a-webassembly-interpreter-in-go",
          "status": "completed",
          "student_name": "Vibhav Pant",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "Launching Wagon, a WebAssembly interpreter in Go"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2017/organizations/6360460286754816/"
    },
    "year_2018": {
      "num_projects": 26,
      "projects": [
        {
          "code_url": "https://docs.google.com/document/d/1ih76CKmj2pA0aYA5LLG98v-4ejYpe_P1lPaF_jbhVX0/edit?usp=sharing",
          "description": "<p><a href=\"https://docs.google.com/document/d/1OenktDPSVAIAlj5E_o1WOkG-vPAkBbEo2oCtzGAKfGo/edit\" target=\"_blank\">googledoc</a></p>\n<h3>Timeline:</h3>\n<h4>Before 5/22/2018:</h4>\n<p>Get familiar with ROOT.</p>\n<p>Get in touch with community.</p>\n<p>Have a preliminary to-do list of possible optimizing solutions after researching.</p>\n<p>Finish parallelizing KDTree with multi-threading.</p>\n<h4>5/22/2018-6/x/2018:</h4>\n<p>(the x depends on the time of final exam, mostly between 10 to 20)</p>\n<p>Try distributed calculation and other general ways.</p>\n<p>Keep editing the to-do list.</p>\n<h4>6/x/2018-7/1/2018:</h4>\n<p>Integrate multi-target regression capability.</p>\n<p>Add more capabilities if possible.</p>\n<h4>7/1/2018-7/18/2018:</h4>\n<p>Expend Falcon by trying to use Scapegoat trees</p>\n<p>Expend Falcon by trying to use ball tree in high dimensions problem.Try other optimizing solutions that is specified for KDTree.</p>\n<p>Try other ways that is found during research.</p>\n<h4>7/18/2018-7/26/2018:</h4>\n<p>Optimize the code structure and writing form of existing codes.</p>\n<p>Write documents of code.</p>\n<h4>7/26/2018-8/2/2018:</h4>\n<p>Add “decorations” to make it easier to use.</p>\n<h4>8/2/2018-8/8/2018:</h4>\n<p>Final check of all new functions and introductions.</p>\n<p>Write report and other paperwork.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6373557118435328/",
          "proposal_id": null,
          "short_description": "googledoc\nTimeline:\nBefore 5/22/2018:\nGet familiar with ROOT.\nGet in touch with community.\nHave a preliminary to-do list of possible optimizing...",
          "slug": "falcon",
          "status": "completed",
          "student_name": "Hanlin Tang",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "FALCON"
        },
        {
          "code_url": "https://medium.com/@harshit.prasad/gsoc-2018-final-work-report-of-lstm-networks-372894d63cf9",
          "description": "<p>Toolkit for Multivariate Analysis (TMVA) is a machine learning toolkit for the ROOT scientific software framework used in many particle physics data analysis and applications. The CNNs and DNNs has been proven in the variety of applications like classification, tracking of particles etc. The aim of the project is to expand the current library of TMVA DNN by implementing efficient Recurrent Neural Networks and LSTM Networks and get the production ready GPU version of convolutional deep learning library along with support for GPU training.</p>\n<p>GPUs are much more effective in terms of high performance when compared with CPUs. In this project, CUDA (Compute Unified Device Architecture) technology will be used by NVIDIA for proper implementation of RNN and LSTM to support GPU architecture.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6458274811478016/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a machine learning toolkit for the ROOT scientific software framework used in many particle physics data...",
          "slug": "recurrent-neural-networks-and-lstms-on-gpus-for-particle-physics-applications",
          "status": "completed",
          "student_name": "Harshit Prasad",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Recurrent Neural Networks and LSTMs on GPUs for Particle Physics Applications"
        },
        {
          "code_url": "https://github.com/cvmfs-contrib/cvmfs-emscripten",
          "description": "<p>The goal of this project is to build a JavaScript client for the CernVM filesystem implemented as a pluggable backend library for the Emscripten compiler. This will allow C/C++ programs compiled using Emscripten to perform POSIX read-only I/O on files, directories, and symbolic links stored on remote CernVM-FS repositories. Core features of the client will include: fetching and decompressing file data chunks on-demand, verifying data integrity of downloaded content, parsing X.509 repository certificates, verifying RSA signatures on repository manifests, seamlessly accessing nested catalogs and chunked files, and persistently caching file data and metadata on the browser's local storage.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6629992301592576/",
          "proposal_id": null,
          "short_description": "The goal of this project is to build a JavaScript client for the CernVM filesystem implemented as a pluggable backend library for the Emscripten...",
          "slug": "cernvm-fs-powered-webassembly-io",
          "status": "completed",
          "student_name": "Saurav Sachidanand",
          "student_profile": null,
          "tags": [
            "java",
            "javascript",
            "web",
            "ml",
            "ui"
          ],
          "title": "CernVM-FS powered WebAssembly I/O"
        },
        {
          "code_url": "https://medium.com/@anushreerankawat110/gsoc-2018-generative-adversarial-networks-in-tmva-554cda974584",
          "description": "<p>Deep Learning Networks have proven to perform well for a wide range of problems, especially those requiring large labelled dataset to learn patterns. A new algorithm that has taken the Deep Learning research community by a storm is that of Generative Adversarial Networks (GANs) introduced by Ian Goodfellow in 2015. This algorithm has a lot of potential owing to the fact that it can generate data that is quite similar to the data given for learning in addition to faster generation of samples when compared to fully visible belief nets. It therefore makes GANs immensely useful in simulation of particle physics and astrophysical data.</p>\n<p>Since ROOT is a data analysis tool extensively used for applications in particle physics and features a dedicated machine learning submodule, Toolkit for Multivariate Analysis (TMVA), it is essential to include a GAN implementation in the toolkit.</p>\n<p>My project would focus on integrating an optimized GAN implementation in the TMVA DNN library with the help of already existing implementations of Deep Network Models. It would also involve enabling GPU Implementation of GANs using Nvidia’s CUDA library.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6692252550168576/",
          "proposal_id": null,
          "short_description": "Deep Learning Networks have proven to perform well for a wide range of problems, especially those requiring large labelled dataset to learn patterns....",
          "slug": "generative-adversarial-networks-for-particle-physics-applications",
          "status": "completed",
          "student_name": "Anushree Rankawat",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "Generative Adversarial Networks for Particle Physics Applications"
        },
        {
          "code_url": "https://gitlab.in2p3.fr/Antares/xtensor_experiments",
          "description": "<p>Investigate the viability of xtensor, a multi-dimensional array package which aims to bring the API feeling of NumPy to C++, in the ACTS cross-experiment particle tracking package. Contribute any required functionality to xtensor and ACTS in the process, ideally also cutting constants in the handling of small matrices.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6739749788188672/",
          "proposal_id": null,
          "short_description": "Investigate the viability of xtensor, a multi-dimensional array package which aims to bring the API feeling of NumPy to C++, in the ACTS...",
          "slug": "cpu-race-for-particle-hunting",
          "status": "completed",
          "student_name": "Antares",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "ui"
          ],
          "title": "CPU Race for Particle Hunting"
        },
        {
          "code_url": "https://mayurdb.github.io/gsoc2018/",
          "description": "<p>A large amount of 3D data is generated in High Energy Physics &amp; Astrophysics experiments. To process this data efficiently, one would need state-of-the-art tools. Already a lot of development has been done in processing 2D data with projects like spatial Hadoop and GeoSpark but, there are very few frameworks to process the 3D data. The idea is to follow the footsteps of GeoSpark and provide a way to load, process and analyse 3D data sets economically and efficiently by leveraging the distributed computation functionality of the spark. Spark3D would provide the set of out-of-the-box 3D Spatial RDD (3D SRDD) to partition the data across machines.\nUltimately, Spark3D would be available as an open-source library which works with all recent versions of the Spark (2.0+), has user friendly APIs (in Scala, Java and Python), works on top of all major platforms out of the box (HDFS, S3, Cassandra, etc.) and supports all major file formats (CSV, Parquet, JSON, Avro, etc.) including popular scientific file formats such as FITS.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5678205671309312/",
          "proposal_id": null,
          "short_description": "A large amount of 3D data is generated in High Energy Physics & Astrophysics experiments. To process this data efficiently, one would need...",
          "slug": "spark3d-extend-apache-spark-to-support-3d-spatial-datasets",
          "status": "completed",
          "student_name": "Mayur Bhosale",
          "student_profile": null,
          "tags": [
            "python",
            "java",
            "api",
            "ai"
          ],
          "title": "Spark3D: Extend Apache Spark to support 3D Spatial Datasets"
        },
        {
          "code_url": "https://gitlab.com/Jayd_1234/GSoC_vectorized_proof_of_concepts/blob/master/markdown%20files/Introduction.md",
          "description": "<p>High Energy Particle physics experiments typically generate a large amount of data. To analyze them requires significant computing resources. This is however, limited and expensive to get. Thus, it is necessary to make optimum utilization of the resources to keep research fast and active. This requires an alternative approach to procedural programming, utilizing a vectorized, concurrent programming in a functional style, while maintaining a easy to use user interface for programming.</p>\n<p>This Project aims to  mitigate some of the issues, by building vectorized, fast primitives as well as several higher level functions, for fast prototyping, while maintaining speed. Furthermore, an experimental GPU offloading for concurrent processing of selected algorithms will also be implemented, and can be extended in he future.  The library will be documented, and interfaces can be provided, to allow integration with other projects.</p>\n",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2018_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5721645104758784/",
          "proposal_id": null,
          "short_description": "High Energy Particle physics experiments typically generate a large amount of data. To analyze them requires significant computing resources. This is...",
          "slug": "diana-hep-analysis-functions-implementation-and-optimization-of-common-particle-physics-analysis-algorithms-in-a-functional-paradigm",
          "status": "completed",
          "student_name": "Jaydeep Nandi",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "DIANA-HEP: Analysis Functions:: Implementation and optimization of common particle physics analysis algorithms in a functional paradigm"
        },
        {
          "code_url": "http://chouak.me/2018-08-04-gsoc18-third-phase-closes/",
          "description": "<h3>Modular YAMPL</h3>\n<p>The project fundamentally consists in developing a flexible plugin system that allows YAMPL to selectively load essential components at runtime. This makes YAMPL extremely modular and decoupled from the IPC backend modules, making it easy for anyone to add new backends or edit the existing ones without compromising the core YAMPL code.</p>\n<p>In addition to the plugin system, the project proposes an overhaul and development of a python binding generation procedure which allows for continuous generation of the bindings whenever the core code is modified. The ultimate goal of the project is to make YAMPL more flexible, modular and extensible.</p>\n<h4>How is Modular YAMPL possible?</h4>\n<ul>\n<li>An ABI-agnostic plugin middleware handles the runtime dynamic loading of the plugins, the communication between the YAMPL core code and the plugin as well as the marshalling of data. This system is designed with simplicity in mind and aims to be as <strong>solid</strong> and <strong>reliable</strong> as possible.</li>\n<li>A specifically designed open-source library (<a href=\"https://github.com/pybind/pybind11\" target=\"_blank\">PyBind11</a>) allows for seamless operability between C++11 and Python. This allows for an easy generation of the python bindings for all the core YAMPL primitives and data structures.</li>\n</ul>\n",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2018_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5101807772631040/",
          "proposal_id": null,
          "short_description": "Modular YAMPL\nThe project fundamentally consists in developing a flexible plugin system that allows YAMPL to selectively load essential components at...",
          "slug": "modular-yampl",
          "status": "completed",
          "student_name": "Ayoub Chouak",
          "student_profile": null,
          "tags": [
            "python",
            "ml",
            "ai",
            "backend"
          ],
          "title": "Modular YAMPL"
        },
        {
          "code_url": "https://github.com/allpix-squared/allpix-squared/pull/1",
          "description": "<p>Allpix-Squared is a free and open-source simulation framework for silicon tracker and vertex detectors written in modern C++. Its goal is to facilitate in-depth studies of silicon-based particle and radiation detectors widely used in high-energy physics. During simulation, Allpix-Squared run events that contains a series of modules that operate on input data — usually in the form of a previous module’s output data. These events aim to mirror the behavior of a particle interacting with a specific detector hardware. Some modules are independent of others, so they can be run in parallel; this is the current state of the multithreading used in the framework, but it does not make good use of available CPU cores. The aim of this proposal is thus to run full events in parallel instead, greatly enhancing CPU core utilization. This however requires some internal restructure and presents new problems not shared with the current approach, such as the issue of event execution order. I aim to solve this issue with dependency graphs. When done, the new multithreading approach should allow better execution time, independent on core count and simulation parameters.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5127307865882624/",
          "proposal_id": null,
          "short_description": "Allpix-Squared is a free and open-source simulation framework for silicon tracker and vertex detectors written in modern C++. Its goal is to...",
          "slug": "improve-multi-threading-support-for-cerns-allpix-squared-project-using-dependency-graphs",
          "status": "completed",
          "student_name": "tmplt",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Improve multi-threading support for CERN’s Allpix-Squared project using dependency graphs"
        },
        {
          "code_url": "https://www.sravikiran.com/GSOC18/",
          "description": "<p>The existing TMVA submodule has always used gradient descent to update the parameters and minimize the cost of the neural networks. More advanced optimization methods can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.</p>\n<p>The project aims to implement various <strong>Optimization Modules</strong> ( Momentum-based, Nesterov accelerated momentum, Adagrad, RMSProp, Adadelta, Adamax, Adam, Nadam, AMSGrad etc )  in Machine learning.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2018_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5333821369090048/",
          "proposal_id": null,
          "short_description": "The existing TMVA submodule has always used gradient descent to update the parameters and minimize the cost of the neural networks. More advanced...",
          "slug": "development-of-the-deep-learning-optimization-algorithms-in-tmva",
          "status": "completed",
          "student_name": "Ravi Kiran Selvam",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Development of the Deep Learning Optimization Algorithms in TMVA."
        },
        {
          "code_url": "https://github.com/OshanIvantha/phoenix-docs",
          "description": "<p>CERNBox is a cloud storage synchronisation service for CERN users: it allows syncing and sharing files on all major mobile and desktop platforms (Linux, Windows, MacOSX, Android, iOS) aiming to provide offline availability to any data stored in the CERN EOS infrastructure. CERNBox is based on ownCloud, a cloud sync and share platform written in PHP following a Model-View-Controller architecture software pattern.</p>\n<p>This project aims to provide a prototype of a new web UI for CERNBox that will provide an immersive user experience. To achieve this, a milestone of this project is to provide offline manipulation of the data from the browser. The use case is that when a user goes offline in his browser, he could continue to work on the document and perform actions like renaming and browsing his files.  Once network is re-established, the application will reconcile the changes in the browser cache with the server, offering a non-disruptive work environment for the end-user.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5334913901395968/",
          "proposal_id": null,
          "short_description": "CERNBox is a cloud storage synchronisation service for CERN users: it allows syncing and sharing files on all major mobile and desktop platforms...",
          "slug": "petabyte-scale-cloud-storage-file-manager",
          "status": "completed",
          "student_name": "Ivantha",
          "student_profile": null,
          "tags": [
            "android",
            "ios",
            "web",
            "mobile",
            "ai"
          ],
          "title": "Petabyte-Scale Cloud Storage File Manager"
        },
        {
          "code_url": "https://gist.github.com/efremale/8f52484e382c4a799b3739fa24af2b67",
          "description": "<p>CLAD is a Clang plugin that implements automatic differentiation. Automatic differentiation avoids usual disadvantages of symbolic and numerical differentiation, by transforming the source code of functions. The goal of the project is to extend CLAD by adding the functionality for computing gradient of functions of several variables in a\nsingle function call.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6218216741273600/",
          "proposal_id": null,
          "short_description": "CLAD is a Clang plugin that implements automatic differentiation. Automatic differentiation avoids usual disadvantages of symbolic and numerical...",
          "slug": "extend-clad-the-automatic-differentiation",
          "status": "completed",
          "student_name": "Aleksandr Efremov",
          "student_profile": null,
          "tags": [],
          "title": "Extend clad - The Automatic Differentiation"
        },
        {
          "code_url": "https://kiryteo.github.io/2018-08-12-GSoC-2018-Report/",
          "description": "<p>ROOT is the data processing framework created at CERN - at the heart of the research on high-energy physics. Every day, thousands of physicists use ROOT applications to analyze their data or to perform simulations. The ROOT software framework is foundational for the HEP ecosystem, providing capabilities such as IO, a C++ interpreter, GUI, and math libraries. It uses object-oriented concepts and build-time modules to layer between components.\nThis project aims to provide additional functionality using a package manager over the minimal base install of core features. It involves defining ROOT modules, packages and package manager, mainly to scale the large codebase of the project. The current development involves creating a modular version of ROOT that provides a minimal base install of core features, then later adding functionality using the package manager. This requires introducing new layering mechanisms and extending the functionality of the existing ROOT package manager prototype.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6244789099954176/",
          "proposal_id": null,
          "short_description": "ROOT is the data processing framework created at CERN - at the heart of the research on high-energy physics. Every day, thousands of physicists use...",
          "slug": "root-package-manager",
          "status": "completed",
          "student_name": "Ashwin Samudre",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "ROOT package manager"
        },
        {
          "code_url": "https://github.com/apsknight/gangaextension",
          "description": "<p>Jupyter Notebook is an interactive computing environment that creates notebooks which contains computer code as well as rich text elements like equations, figures, plots, widgets and theory. Ganga is an open source iPython based interface tool to the computing grid which leverage the power of distributed computing grid and provide scientists an interface supported by a powerful backend where they can submit their computation intensive programs to Ganga as a batch job. HTCondor is a workload management system created by University of Wisconsin-Madison. It is based on High-Throughput Computing which effectively utilizes the computing power of idle computers on a network or on a computing grid and offload computing intensive tasks on the idle machines available on a network or computing grid. This project aims to create a plugin for Jupyter Notebook and also integrate it to SWAN Notebook service which is a cloud data analysis service developed and powered by CERN. This plugin will easily submit and monitor batch computation jobs to HTCondor using Ganga toolkit. The plugin will display status of ongoing job in Notebook itself and will also allow termination of ongoing jobs.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/4671150864990208/",
          "proposal_id": null,
          "short_description": "Jupyter Notebook is an interactive computing environment that creates notebooks which contains computer code as well as rich text elements like...",
          "slug": "large-scale-computing-backend-for-jupyter-notebooks-htcondor-batch-job-submission-and-monitoring-using-the-ganga-toolkit",
          "status": "completed",
          "student_name": "Aman Pratap Singh",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "cloud",
            "backend"
          ],
          "title": "Large-scale computing backend for Jupyter notebooks - HTCondor batch job submission and monitoring using the Ganga toolkit"
        },
        {
          "code_url": "https://gist.github.com/asketagarwal/d32cfb4d582da57d929534fbb6601d23",
          "description": "<p>Rucio produces large amounts of metadata for it’s files and datasets which is stored in a central Rucio server. However there is a fixed set of metadata attributes that can be stored currently. Rucio would like a generic metadata catalogue with no restriction on the kind of metadata stored for the files. The project is to to design and  implement a generic and scalable metadata component that integrates with the core transactional model of Rucio.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/4819316230848512/",
          "proposal_id": null,
          "short_description": "Rucio produces large amounts of metadata for it’s files and datasets which is stored in a central Rucio server. However there is a fixed set of...",
          "slug": "rucio-billion-row-scalable-and-flexible-metadata",
          "status": "completed",
          "student_name": "Asket Agarwal",
          "student_profile": null,
          "tags": [],
          "title": "Rucio : Billion-row scalable and flexible metadata"
        },
        {
          "code_url": "https://medium.com/@srk97/gsoc-2018-vaes-iii-summary-c18d0478111d",
          "description": "<p>Deep Learning relies heavily on a large number of linear operations. Data parallelism is a consequence of this property of Deep Learning.GPUs are very fast at computing these linear operations. Therefore, significant speedup can be achieved if this computation is delegated to the GPU. This project aims to provide a framework for training Variational Autoencoders. The plan is to extend the current Deep Auto Encoder module to be a generative framework that supports Convolutional and possibly Recurrent Encoder-Decoder architectures. The framework written will be compatible with GPUs</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/4830332285091840/",
          "proposal_id": null,
          "short_description": "Deep Learning relies heavily on a large number of linear operations. Data parallelism is a consequence of this property of Deep Learning.GPUs are...",
          "slug": "variational-autoencoders-on-gpus-for-particle-physics-applications",
          "status": "completed",
          "student_name": "SiddharthaRao Kamalakara",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Variational Autoencoders on GPUs for particle physics applications"
        },
        {
          "code_url": "https://gist.github.com/EgorMatirov/66fe626b1f6849d511c21322445fc1e0",
          "description": "<p>This project aims at implementing a pure-Go BSD-3 client library for XRootD using Go builtin features for concurrency and scalability.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6484663090544640/",
          "proposal_id": null,
          "short_description": "This project aims at implementing a pure-Go BSD-3 client library for XRootD using Go builtin features for concurrency and scalability.",
          "slug": "pure-go-xrootd-client-implementation",
          "status": "completed",
          "student_name": "Mikhail Ivchenko",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Pure-Go XRootD client implementation"
        },
        {
          "code_url": "https://github.com/BigBang0072/HAhRD/wiki/GSOC-18-Work-Summary",
          "description": "<p>One of the challenges faced in Particle Physics Experiment after the collision of particles in LHC is the reconstruction of the events.This includes finding the type of daughter particles created and other important characteristics associated with particles like energy, from the data recorded by Detectors like CMS or ATLAS.</p>\n<p>This project is targeted on event reconstruction of particles produced after the proton-proton collision, from data recorded in one of future sub-detector of CMS named as HGCAL(High Granularity Calorimeter). We will be using CNN (Convolutional Neural Network) for reconstructing the rare processes by classifying and learning other characteristics of the particles from the hits (energy deposits) recorded in the detector which are generated after the collision.</p>\n<p>The main goal of this project is to develop a software pipeline, compatible with HGCAL sub-detector, which can be used by Physicist or other developers to create and train a CNN architecture on GPU to get additional insights in event reconstruction.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6501701494243328/",
          "proposal_id": null,
          "short_description": "One of the challenges faced in Particle Physics Experiment after the collision of particles in LHC is the reconstruction of the events.This includes...",
          "slug": "hahrd-deepreconstruction",
          "status": "completed",
          "student_name": "Abhinav Kumar",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "HAhRD: DeepReconstruction"
        },
        {
          "code_url": "https://gist.github.com/steremma/048549de16cc48610233c943ecf495b4",
          "description": "<p>The project's ultimate goal is to provide a GPU implementation for the existing Convolutional Neural Network package within <code>root/tmva</code>.  During my preliminary work with the codebase, I discovered that the current package's public interface can be further improved. Since performing this change can significantly reduce the complexity of my main task, as well as any future extensions to the package, I plan to work on it during the first phase of the summer period. The first part of the present proposal goes through the necessary changes to achieve a clean API. The rest of the proposal iterates through the main modules that need to be ported into the GPU implementation. These are the different layer types that can be included in a convolutional neural network, as well as a number of generic helper functions. Attention is drawn on guaranteeing the deliverable's quality, both in terms of correctness and in terms of speed-up. This will be achieved through extensive testing and standardized benchmarking respectively.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2018_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6556700429516800/",
          "proposal_id": null,
          "short_description": "The project's ultimate goal is to provide a GPU implementation for the existing Convolutional Neural Network package within root/tmva.  During my...",
          "slug": "convolutional-deep-neural-networks-on-gpus-for-particle-physics-applications",
          "status": "completed",
          "student_name": "Emmanouil Stergiadis",
          "student_profile": null,
          "tags": [
            "api",
            "ai"
          ],
          "title": "Convolutional Deep Neural Networks on GPUs for Particle Physics Applications"
        },
        {
          "code_url": "https://radonys.blogspot.com/2018/08/my-gsoc-experience-with-cern-hsf-summer.html",
          "description": "<p>The <strong>DIRAC Interware</strong> can handle hundreds of thousands of “jobs” daily. LHCb (the “beauty” experiment at the LHC) is DIRAC’s main user and developer. DIRAC’s jobs may have several parameters, and carry important information which should be kept, for users and administrators alike. The information gathered from the parameters is currently stored in a relational database system in a key-value pair format. <strong>Traceability of the jobs becomes difficult</strong> as it becomes very hard to perform queries on such database.</p>\n<p>Hence, I would like to <strong>extend the current job monitoring system, currently based upon relational databases, by using non-relational database (NoSQL), which is ElasticSearch</strong>, a state of the art solution on which queries become easier on a large number of jobs.</p>\n<p>Also, the current DIRAC development is in python2, hence I would like to make my <strong>code 2to3 tool complaint</strong>, so that my submissions can be easily ported to python3 using this tool.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5820528271032320/",
          "proposal_id": null,
          "short_description": "The DIRAC Interware can handle hundreds of thousands of “jobs” daily. LHCb (the “beauty” experiment at the LHC) is DIRAC’s main user and developer....",
          "slug": "monitoring-and-traceability-of-jobs-using-elasticsearch-dirac",
          "status": "completed",
          "student_name": "Yash Srivastava",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "database"
          ],
          "title": "Monitoring and traceability of jobs  using ElasticSearch - DIRAC"
        },
        {
          "code_url": "https://fireballpoint1.github.io/GasSimulator/?shell#progress-report",
          "description": "<p>Magboltz solves the Boltzmann transport equations with numerical integration in order to simulate the interactions of electrons in gas mixtures under the influence of electric and magnetic fields.</p>\n<p>Degrad calculates the cluster size distribution and primary cluster distribution in gas mixtures for minimum ionizing particles and X-rays.</p>\n<p>Both of these programs were originally made in Fortran, the code is available in the links bellow.</p>\n<p>The goal of this project is to begin with a test python interface for Degrad and Magboltz, to design optimized python implementations of their processes, joint functionality, and can extend to new functionality related to the calculation as well as extensive reports of the results.</p>\n<p>R&amp;D of gaseous detectors requires simulations like those provided by Magboltz and Degrad.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_021",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5855936115638272/",
          "proposal_id": null,
          "short_description": "Magboltz solves the Boltzmann transport equations with numerical integration in order to simulate the interactions of electrons in gas mixtures under...",
          "slug": "open-source-simulations-for-gas-detector-on-python",
          "status": "completed",
          "student_name": "Mayank Modi",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "Open-Source Simulations for Gas Detector on Python"
        },
        {
          "code_url": "https://sd57.github.io/vgdmlio/",
          "description": "<p>VecGeom is the new geometry library developed within the high-energy physics community, aiming to replace the legacy geometry navigation functionality provided by Geant4 and ROOT with optimized and vectorized algorithms. In this project we aim to develop I/O allowing to read geometry from application-independent formats such as the Geometry Description Markup Language (GDML). We propose Xerces-C as the backend to parse the files and concentrate on converting between the DOM tree and the VecGeom volume hierarchy.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_022",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5984876603899904/",
          "proposal_id": null,
          "short_description": "VecGeom is the new geometry library developed within the high-energy physics community, aiming to replace the legacy geometry navigation...",
          "slug": "gdml-io-for-vecgeom-geometry-package",
          "status": "completed",
          "student_name": "Dima Savin",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "backend"
          ],
          "title": "GDML I/O for VecGeom Geometry Package"
        },
        {
          "code_url": "https://gist.github.com/tellenbach/0b9723d9e740d280963503160777803b",
          "description": "<p>Eigen is a C++ template library for linear algebra that aims for high performance in combination of high reliablity and good compiler support. A lot of remarkable projects rely on it, including Google's Tensorflow. Another successful project using Eigen is the high-energy physics experiment ATLAS at the LHC.</p>\n<p>At the LHC, millions of particles collide every second and each collision creates a huge amount of data that has to be classified and analyzed by software. Most algorithms in ATLAS software use symmetric matrices, i.e., matrices where the upper triangular part is equal to the lower triangular part. Unfortunalty Eigen currently misses support for symmetric matrices.</p>\n<p>This Google Summer of Code 2018 project aims to implement a class for handling symmetric matrices in Eigen. The goal is to provide a working implementation that can be submitted as a patch for Eigen.</p>\n<p>This project proposal contains implementaions ideas and plans, a detailed implementation timeline, consisting of 13 weekly tasks and some short biographical information about me.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_023",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6284595058180096/",
          "proposal_id": null,
          "short_description": "Eigen is a C++ template library for linear algebra that aims for high performance in combination of high reliablity and good compiler support. A lot...",
          "slug": "faster-matrix-algebra-for-atlas",
          "status": "completed",
          "student_name": "David Tellenbach",
          "student_profile": null,
          "tags": [
            "angular",
            "ai"
          ],
          "title": "Faster Matrix Algebra for ATLAS"
        },
        {
          "code_url": "https://github.com/ssomesh/sixtracklib_gsoc18/tree/master/",
          "description": "<p>The project targets optimizing the data structures and source code of a standalone tracking library, SixTrackLib, written in C. The aim is to restructure the code such that it enables auto vectorization in GCC and CLang for CPU. In addition, the code should also be amenable to execution on the GPU, having low register pressure. The idea is to keep the code structure relatively simple and maximize the shared code between CPU and GPU. The optimized SixTrackLib module will be integrated into the legacy SixTrack code.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_024",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5152248673861632/",
          "proposal_id": null,
          "short_description": "The project targets optimizing the data structures and source code of a standalone tracking library, SixTrackLib, written in C. The aim is to...",
          "slug": "optimize-and-integrate-standalone-tracking-library-sixtracklib",
          "status": "completed",
          "student_name": "Somesh Singh",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Optimize and Integrate Standalone Tracking Library (SixTrackLib)"
        },
        {
          "code_url": "http://shravanmurali.com/PyRDF/",
          "description": "<p>The main objective of this project is to make it easier for researchers/developers to submit distributed jobs for analyzing datasets using <em><a href=\"https://root.cern.ch/doc/master/classROOT_1_1Experimental_1_1TDataFrame.html\" target=\"_blank\">TDataFrame</a></em> in <a href=\"https://root.cern.ch/\" target=\"_blank\">ROOT</a> library and a Distributed Computing framework like <a href=\"https://spark.apache.org\" target=\"_blank\">Apache Spark</a>. This project proposes a Python library with tidy abstractions to perform distributed analysis as well as to select appropriate distributed environments [like <a href=\"https://spark.apache.org\" target=\"_blank\">Apache Spark</a>].<br>\nAlso, Jupyter notebook has become quite popular these days to carry out numerical/graphical analysis tasks. Hence, a new <a href=\"http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Distributing%20Jupyter%20Extensions%20as%20Python%20Packages.html\" target=\"_blank\">Jupyter extension</a> would also be implemented as a part of this project. The extension gives users a graphical interface to select various parameters for launching a Distributed job. This extension also allows users to select cells for constructing analysis functions for datasets.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_025",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5169858777972736/",
          "proposal_id": null,
          "short_description": "The main objective of this project is to make it easier for researchers/developers to submit distributed jobs for analyzing datasets using TDataFrame...",
          "slug": "distributed-big-data-analysis-with-tdataframe",
          "status": "completed",
          "student_name": "shravan97",
          "student_profile": null,
          "tags": [
            "python",
            "ml",
            "ai",
            "ui"
          ],
          "title": "Distributed Big Data Analysis with TDataFrame"
        },
        {
          "code_url": "https://github.com/WLCG-Lightweight-Sites/wlcg_lightweight_sites_ansible_module/wiki/GSoC-2018-:-Final-Report-by-Tarang-Mahapatra",
          "description": "<p>Lightweight Sites is a project at CERN that enables the quick setup of new sites on the grid. Often, a lot of time is spent in setting up new sites with significant inputs from teams at CERN and at the organization operating the site. Lightweight Sites abstracts away and consolidates resources to aid the setup. To achieve this, a Lightweight Sites Specification document has been created that will allow the software configuration of the entire site through a central module. This would be the Level 1 configuration. In addition, other Lightweight Sites Components would be created according to specifications. One such component includes the repositories for containers of different compute elements (CE), batch systems, worker nodes (WN), etc. These containers are also configured and comprise the Level 2 configurations. In the past, YAIM, an in-house configuration tool has been used for Level 2 configurations. In this project, in addition to setting up Level 1 configuration, popular configuration tools like Ansible and Puppet would be used for Level 2 configuration, replacing YAIM. This migration would contribute to the project by making it easier to configure new Lightweight Sites.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2018_026",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5213490679644160/",
          "proposal_id": null,
          "short_description": "Lightweight Sites is a project at CERN that enables the quick setup of new sites on the grid. Often, a lot of time is spent in setting up new sites...",
          "slug": "configuration-of-lightweight-sites-components-using-ansible",
          "status": "completed",
          "student_name": "Tarang Mahapatra",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Configuration of Lightweight Sites Components using Ansible"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2018/organizations/4841240159846400/"
    },
    "year_2019": {
      "num_projects": 29,
      "projects": [
        {
          "code_url": "https://drive.google.com/drive/folders/1klBhpreyA9LCRIG8L2BlJceZY-Uhqdju?usp=sharing",
          "description": "<p>At CERN, the data from LHC collisions requires complex data types and functions to be processed. As a solution, the awkward-array library makes python implementation of these data types possible in a way which is portable to GPUs. Making this library work through C++ code would give it the ability to have precompiled C++ routines for faster implementation (past the initial load time) and later compatibility with vectorization primitives from C++ libraries.</p>\n<p>As part of this project, I will be working on expanding the awkward-array python library to include C++ compatible functions using pybind11. This will entail creating compiled C++ code in the form of python extension modules to be an expansion of the already-existing package. The classes I will be writing will be C++/python versions of the classes which have been already written in the main directory of the awkward-array project.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2019_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6412014505689088/",
          "proposal_id": null,
          "short_description": "At CERN, the data from LHC collisions requires complex data types and functions to be processed. As a solution, the awkward-array library makes...",
          "slug": "cern-awkward-array-project",
          "status": "completed",
          "student_name": "Charles Escott",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "CERN Awkward Array Project"
        },
        {
          "code_url": "https://ishanrai05.github.io/blog/2019-08-22-gsoc/",
          "description": "<p>GANGA (Gaudi/Athena and Grid Alliance) is an interface used by scientists to interface with huge amount of computing power and storage available to them as part of the LHC computing grid.  Ganga provides a simple yet powerful interface for submitting and managing jobs to a variety of computing backends.\nThe project aims to evaluate the CPU usage, Memory Usage and Persistent Memory usage by Ganga framework. It further aims to reduce the memory consumption by Ganga when executing jobs. FInally it aims to implement a new persistent modal that will store the metadata in more compact form thus reducing both the memory consumption as well as time taken to read the data.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6423086646165504/",
          "proposal_id": null,
          "short_description": "GANGA (Gaudi/Athena and Grid Alliance) is an interface used by scientists to interface with huge amount of computing power and storage available to...",
          "slug": "optimisation-of-the-ganga-toolkit-in-terms-of-memory-consumption-and-persistent-storage",
          "status": "completed",
          "student_name": "Ishan Rai",
          "student_profile": null,
          "tags": [
            "ai",
            "backend"
          ],
          "title": "Optimisation of the Ganga toolkit in terms of memory consumption and persistent storage."
        },
        {
          "code_url": "https://haozturk.github.io/home/",
          "description": "<p>Athena framework is being upgraded to run in multithreaded environment and the aim of this project is to create a new Atlas performance monitoring infrastructure which observes the performance of MT processes. What performance measures to implement, how to visualize the results and how to test the system are discussed throughout this paper. I also present my biographical information and related experience.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6581712404873216/",
          "proposal_id": null,
          "short_description": "Athena framework is being upgraded to run in multithreaded environment and the aim of this project is to create a new Atlas performance monitoring...",
          "slug": "proposal-for-atlas-experiment-hasan-öztürk",
          "status": "completed",
          "student_name": "Hasan Öztürk",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Proposal for Atlas Experiment - Hasan Öztürk"
        },
        {
          "code_url": "https://gist.github.com/fylux/69079f3496d1afa150ed5b60566733e1",
          "description": "<p>This project aims to investigate application of ZSTD within the ROOT framework; benchmark it in comparison to the other algorithms; test it against real LHC data files; and investigate schemes to integrate dictionary-based compression in ROOT files. This will require careful analysis, research and benchmarking through all the stages of the project in order to ensure that the proposed changes are properly verified, documented and result in well-defined benefits for ROOT.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6721979325874176/",
          "proposal_id": null,
          "short_description": "This project aims to investigate application of ZSTD within the ROOT framework; benchmark it in comparison to the other algorithms; test it against...",
          "slug": "novel-applications-of-zstandard-zstd-compression-algorithm-to-root",
          "status": "completed",
          "student_name": "Alfonso",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Novel Applications of Zstandard (ZSTD) compression algorithm to ROOT"
        },
        {
          "code_url": "https://akashravi.github.io/SWAN-Package-Manager/",
          "description": "<p>This proposal promises to develop a Jupyter notebook extension, that will allow the users to specify python modules (and their respective versions) via a user interface and make them available inside the notebook automatically. This extension would improve the user experience for interactive programming and data analysis using tools like SWAN.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5684214832300032/",
          "proposal_id": null,
          "short_description": "This proposal promises to develop a Jupyter notebook extension, that will allow the users to specify python modules (and their respective versions)...",
          "slug": "package-manager-for-jupyter-notebook-swan",
          "status": "completed",
          "student_name": "Akash Ravi",
          "student_profile": null,
          "tags": [
            "python",
            "ai"
          ],
          "title": "Package manager for Jupyter Notebook / SWAN"
        },
        {
          "code_url": "https://gist.github.com/agarzaro/6bcf2f8e75ef2a71204601d23c245b33",
          "description": "<p>A C++ adapter API for integrating vectorized components in a scalar workflow.</p>\n<p>Many FLOP-intensive algorithms may profit from the vector pipelines of modern processors; they don’t because they don’t have vectorizable inner loops. The project idea is to implement and benchmark a generic vector flow service that can non-intrusively integrate with arbitrary data processing frameworks and can expose algorithms to the higher-level event loop of these frameworks.</p>\n<p>This library aims to provide an API and a set of examples demonstrating the transformation of the scalar algorithm in a vectorized one. Depending on the intrinsic algorithm gain from SIMD vectorization and better data caching, the overheads introduced by the extra data transformations can be much smaller than the benefits.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5730913743273984/",
          "proposal_id": null,
          "short_description": "A C++ adapter API for integrating vectorized components in a scalar workflow.\nMany FLOP-intensive algorithms may profit from the vector pipelines of...",
          "slug": "vectorflow",
          "status": "completed",
          "student_name": "Arturo Garza Rodriguez",
          "student_profile": null,
          "tags": [
            "api",
            "ai"
          ],
          "title": "VectorFlow"
        },
        {
          "code_url": "https://jkguiang.github.io/tuda/",
          "description": "<p>Over the course of Run 2, from 2016 to 2018, the CMS detector produced an unparalleled amount of data, resulting in an intricate optimization problem in data access and storage infrastructure as well as distributed computing that is one of the fundamental challenges of running an experiment like the LHC. Dedicated physicists and engineers have constructed a system that has served the collaboration well, but the approaching HL-LHC upgrade, which will produce about an <em>exabyte</em> of data per year, demands a more economic solution. Fortunately, the HEP Software Foundation (HSF) has been collecting data describing global and local access patterns that can be used to model the response of alternative, novel infrastructures that may better serve High Energy Physics for decades. Furthermore, with the advent and ever-growing popularity of \"Big Data\" in industry, the optimization philosophy of the CMS data infrastructure, as well as the predictive power of the project itself, will have relevance far beyond experimental physics.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5441440799260672/",
          "proposal_id": null,
          "short_description": "Over the course of Run 2, from 2016 to 2018, the CMS detector produced an unparalleled amount of data, resulting in an intricate optimization problem...",
          "slug": "tools-for-understanding-cms-data-access",
          "status": "completed",
          "student_name": "Jonathan Guiang",
          "student_profile": null,
          "tags": [],
          "title": "Tools for Understanding CMS Data Access"
        },
        {
          "code_url": "https://gitlab.cern.ch/allpix-squared/allpix-squared/merge_requests/227",
          "description": "<p>Pursuing the goal of running Allpix-Squared simulation’s events -independent by nature- in parallel, have led to the identification of performance bottlenecks that prevent Allpix-Squared from fully utilizing available CPU cores and from scaling the execution time relative to the number of used cores. In this year GSoC, I propose to continue working on solving these bottlenecks, most importantly the Geant4 dependency by implementing a custom run manager encapsulated within Allpix-Squared that would fix the scalability issues and allow for running events in parallel.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5124542089920512/",
          "proposal_id": null,
          "short_description": "Pursuing the goal of running Allpix-Squared simulation’s events -independent by nature- in parallel, have led to the identification of performance...",
          "slug": "implement-event-based-seeding-and-multi-threading",
          "status": "completed",
          "student_name": "Mohamed Moanis Ali",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Implement Event based Seeding and Multi-Threading"
        },
        {
          "code_url": "https://medium.com/singh96aman/https-medium-com-singh96aman-gsoc-2019-cern-hsf-pyawake-153b1218ba25",
          "description": "<p>Building a library that reads a large number of HDF files and builds a database. Add support for searching and loading multiple datasets, intelligently identifying dataset dependent parameters, visualize and analyze images to meet the requirements of scientists at AWAKE and port the existing analysis. Deploy the library for open source community.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5287700498743296/",
          "proposal_id": null,
          "short_description": "Building a library that reads a large number of HDF files and builds a database. Add support for searching and loading multiple datasets,...",
          "slug": "building-a-python-based-analysis-tool-for-awake-experiment",
          "status": "completed",
          "student_name": "Aman Singh Thakur",
          "student_profile": null,
          "tags": [
            "python",
            "database",
            "ui"
          ],
          "title": "Building a Python-based Analysis tool for AWAKE experiment"
        },
        {
          "code_url": "https://github.com/acts-trk/rust-kf",
          "description": "<p>The Kalman Filter is a method of iteratively predicting the future state of a system based on previous information. Not only is a Kalman Filter more reliable about predicting future state than traditional extrapolation techniques, It also provides a confidence for the estimate. A Kalman Filter is used both to reduce the impact of sensor noise on estimations, and to determine which sensors can be “trusted” more than others. Whereas more primitive methods for estimation and extrapolation rely on some form of averaging, a Kalman Filter forecasts by developing a weighted covariance for each sensor input.</p>\n<p>The aim of this project is to implement a Kalman Filter in Rust. Rust has gained popularity for providing more compile-time checks than other systems-level languages, namely C and C++. Rust’s memory model ensures that there is little to no room for many of the memory pitfalls common in other low level languages, such as double-freeing memory, dangling pointers, and user-after-free errors. This, in conjunction with high runtime performance, leads writing components of a codebase in Rust to be favorable for both speed and stability.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5579073160478720/",
          "proposal_id": null,
          "short_description": "The Kalman Filter is a method of iteratively predicting the future state of a system based on previous information. Not only is a Kalman Filter more...",
          "slug": "kalman-filter-in-rust",
          "status": "completed",
          "student_name": "Brooks Karlik",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Kalman Filter in Rust"
        },
        {
          "code_url": "https://sharad24.github.io/trackml/",
          "description": "<p>Porting and Analysis of top solution algorithms from the TrackML challenge to ACTS framework. The algorithms include the combinatorial Mikado tracker, Cloudkitchen's Neural Network and DAG-based tracker and top-quarks' logistic regression and outlier density estimation algorithm. Added an example of running a PyTorch model in ACTS and testing genetic algorithms for optimization of the Mikado Tracker</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5610317520830464/",
          "proposal_id": null,
          "short_description": "Porting and Analysis of top solution algorithms from the TrackML challenge to ACTS framework. The algorithms include the combinatorial Mikado...",
          "slug": "implementation-and-optimisation-in-acts-of-algorithms-exposed-in-trackml-challenge",
          "status": "completed",
          "student_name": "Sharad Chitlangia",
          "student_profile": null,
          "tags": [
            "ml",
            "cloud"
          ],
          "title": "Implementation and Optimisation in ACTS of algorithms exposed in TrackML challenge"
        },
        {
          "code_url": "https://github.com/schwarzschild-radius/spmdfy/",
          "description": "<p>High Level Trigger 1(HLT1) is the first and critical stage in software reconstruction of collisions at the LHCb experiment in the Large Hadron Collider at CERN. Allen aims to do full software reconstruction on GPUs.</p>\n<p>However the reconstruction must also be able to run on the LHCb baseline x86 architecture. Since Allen's algorithm are designed to be efficient on SIMD architectures, a natural translation to support x86 architectures is possible. The SPMD programming model bears resemblance to the SIMT programming model of CUDA, and is a natural target for code translation. Such an automated conversion would be beneficial not only for Allen, but for any CUDA projects seeking cross-architecture support.</p>\n<p>Exploiting vectorization on modern CPU is a hard task . Manual vectorization requires lot of developer time and maintenance can be hard. ISPC raises abstraction of SIMD via the SPMD programming model. Since SIMT programming model on GPUs closely resemble SIMD, the code conversion can be automated using an intermediate translation engine. The project aims to convert from CUDA to ISPC using translation engine based on LLVM.</p>\n",
          "difficulty": "medium",
          "id": "proj_cern-hsf_2019_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5637309913890816/",
          "proposal_id": null,
          "short_description": "High Level Trigger 1(HLT1) is the first and critical stage in software reconstruction of collisions at the LHCb experiment in the Large Hadron...",
          "slug": "simt-to-spmd-translation",
          "status": "completed",
          "student_name": "Pradeep Kumar S",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "SIMT to SPMD Translation"
        },
        {
          "code_url": "https://emiliocortina.github.io/phoenix_report/",
          "description": "<p>Developing a new data format to represent event data, unifying the needs of the different experiments that will make use of it. Improvement on the data visualisation of the application, adding new functionality to its GUI and adapting the existing application to this new file format developed.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6207853369491456/",
          "proposal_id": null,
          "short_description": "Developing a new data format to represent event data, unifying the needs of the different experiments that will make use of it. Improvement on the...",
          "slug": "experiment-independent-display-framework-and-data-format",
          "status": "completed",
          "student_name": "Emilio Cortina Labra",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "Experiment independent display framework and data format"
        },
        {
          "code_url": "https://docs.google.com/document/d/1ryqNseURDZiL71anRMV0JL4DU7o9SrXG4nXZ4kj510Y/edit?usp=sharing",
          "description": "<p>ROOT has several features which interact with libraries and require implicit header inclusion. These headers are often immutable, and reparsing is redundant. C++ Modules are designed to minimize the reparsing of the same header content by providing an efficient on-disk representation of C++ code. Although C++ modules support in ROOT has been implemented in the last few years, there is still room for performance improvement, and GlobalModuleIndex implementation is one such possible solution. It is a mechanism to create the table of symbols and PCM names so that ROOT will be able to load a corresponding library when a symbol lookup failed. It is expected to improve ROOT’s performance by speeding up its startup time.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6212391858077696/",
          "proposal_id": null,
          "short_description": "ROOT has several features which interact with libraries and require implicit header inclusion. These headers are often immutable, and reparsing is...",
          "slug": "implement-a-globalmoduleindex-in-root-and-cling",
          "status": "completed",
          "student_name": "Arpitha Raghunandan",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Implement a GlobalModuleIndex in ROOT and Cling"
        },
        {
          "code_url": "https://medium.com/@ashishkshirsagar10/cern-gsoc19-generative-adversarial-networks-for-particle-physics-applications-c3da13a3f44b",
          "description": "<p>The project aims implementation of GANs in the Machine Learning toolkit, TMVA of the ROOT framework would be immensely useful because of the advent, popularity and versatile nature of GANs. GANs can essentially be used for simulation and physical/mathematical modeling of patterns learned from training data substantially faster and more accurate than any other generative model. The model can be used for generating training data and finds many applications in high particle physics and astrophysical research realms.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4504948144865280/",
          "proposal_id": null,
          "short_description": "The project aims implementation of GANs in the Machine Learning toolkit, TMVA of the ROOT framework would be immensely useful because of the advent,...",
          "slug": "generative-adversarial-networks-for-particle-physics-applications",
          "status": "completed",
          "student_name": "Ashish Kshirsagar",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Generative Adversarial Networks for Particle Physics Applications"
        },
        {
          "code_url": "https://github.com/AIDASoft/podio/blob/hdf5_writer/doc/HDF5.md",
          "description": "<p>PODIO is a C++ library that allows the creation of event data models and efficient I/O code for HEP experiments. It does so by avoiding deep-object hierarchies and virtual inheritance.\nOn the other hand, HDF5 is a data format that allows one to manage extremely large and complex data collections. Due to its versatility as a data model and rich set of performance features, it is an ideal format to store PODIO data in. The aim of this project is to implement an HDF5 backend for PODIO.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2019_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4558314925457408/",
          "proposal_id": null,
          "short_description": "PODIO is a C++ library that allows the creation of event data models and efficient I/O code for HEP experiments. It does so by avoiding deep-object...",
          "slug": "proposal-for-the-implementation-of-an-hdf5-io-layer-for-podio",
          "status": "completed",
          "student_name": "Shrey Aryan",
          "student_profile": null,
          "tags": [
            "ai",
            "backend"
          ],
          "title": "Proposal for the Implementation of an HDF5 IO Layer for PODIO"
        },
        {
          "code_url": "https://wlcg-lightweight-sites.github.io/gsoc_2019_python_components.html",
          "description": "<p>The SIMPLE Grid project is an extension of the SIMPLE Framework that combines popular configuration management technologies such as Puppet/Ansible and container orchestration technologies such as Docker Swarm/Kubernetes to allow deployment of complex computing clusters using a single site level configuration file. The proposed project aims to improve functionality, correctness, and efficiency of different stages of SIMPLE Grid Framework.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2019_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4648849883267072/",
          "proposal_id": null,
          "short_description": "The SIMPLE Grid project is an extension of the SIMPLE Framework that combines popular configuration management technologies such as Puppet/Ansible...",
          "slug": "python-components-for-the-simple-grid-framework",
          "status": "completed",
          "student_name": "Sneha Sinha",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "docker",
            "kubernetes"
          ],
          "title": "Python Components for the SIMPLE Grid Framework"
        },
        {
          "code_url": "https://himanshusahu31.github.io/blog/gsoc-submission/",
          "description": "<p>In the view of LHC Run 3, we want to extend the functionalities of Molr so that it will be ready to use in production to control various operational systems.</p>\n<p>Features to be implemented:</p>\n<ul>\n<li>Persistence and runtime cleanup of the missions on the moles.</li>\n<li>Registration of new mole instances at runtime.</li>\n<li>Typed Molr missions.</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4650709218230272/",
          "proposal_id": null,
          "short_description": "In the view of LHC Run 3, we want to extend the functionalities of Molr so that it will be ready to use in production to control various operational...",
          "slug": "molr-operational",
          "status": "completed",
          "student_name": "Himanshu Sahu",
          "student_profile": null,
          "tags": [],
          "title": "Molr - Operational"
        },
        {
          "code_url": "https://ruturaj123.github.io/gsoc/2019/08/19/GSoC-Work-Report.html",
          "description": "<p>Rucio is a data management system that provides the functionality to organize, manage and access a large amount of scientific data (in the order of petabytes) using customizable policies. Rucio also provides monitoring and data analytics. \nAs a part of my GSoC program, I aim to create a Single-Sign On (SSO) authentication for CERN. SSO is a user authentication service which permits a user to use one set of login credentials to access multiple domains. It will eliminate the need to login to every CERN service that the user is registered to.\nThe next task will be to develop a Collection following mechanism. The aim of this task is to develop a service which will inform the end-users of any event affecting the dataset. For eg. dataset deletion, file loss, change in metadata, lifetime expiration, etc.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4699748483727360/",
          "proposal_id": null,
          "short_description": "Rucio is a data management system that provides the functionality to organize, manage and access a large amount of scientific data (in the order of...",
          "slug": "rucio-exascale-data-management",
          "status": "completed",
          "student_name": "Ruturaj Gujar",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Rucio - Exascale Data Management"
        },
        {
          "code_url": "https://github.com/root-project/root/pull/3924",
          "description": "<p>This project is about development of Long Short Term Memory(LSTM) and Gated Recurrent Unit(GRU) layers in TMVA, both of which belong to a general class of neural networks called the Recurrent Neural Networks(RNN). These layers have many important applications in the realm of data analysis for particle physics experiments. As an example, LSTMs can be used for track reconstruction of charged particles in the Large Hadron Collider(LHC). They can also be used for analyzing the voltage time series from the electronic monitoring system present in superconducting LHC magnets.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4829175494475776/",
          "proposal_id": null,
          "short_description": "This project is about development of Long Short Term Memory(LSTM) and Gated Recurrent Unit(GRU) layers in TMVA, both of which belong to a general...",
          "slug": "development-of-lstm-and-gru-layers-in-tmva",
          "status": "completed",
          "student_name": "Surya S Dwivedi",
          "student_profile": null,
          "tags": [],
          "title": "Development of LSTM and GRU layers in TMVA"
        },
        {
          "code_url": "https://pujanm.github.io/2019-08-19-gsoc-work-report/",
          "description": "<p>DIRAC is a highly-scalable software used for accessing distributed resources from various distributed systems. DIRAC’s main contributor is LHCb and also its initiator. LHCb uses the different type of computing technologies in order to distribute and process the collected physics data and DIRAC is one the software which is used because of its scalability and the level of orchestration and monitoring it provides for the distributed resources which is the main requirement of the LHCb collaboration.\nFurther, my task will be to upgrade DIRAC’s monitoring system further by assuring high-scalability as when the LHCb gets upgraded there can be an unpredictable type of data which needs to be molded easily by DIRAC for which we will use ElasticSearch which is one of the widely used NoSQL technology.\nWith this, I will have to migrate the Service and Agent components and DIRAC's RequestManagementSystem and DataManagementSystem to support ES backend along with tests and documentation.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_021",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6488258630909952/",
          "proposal_id": null,
          "short_description": "DIRAC is a highly-scalable software used for accessing distributed resources from various distributed systems. DIRAC’s main contributor is LHCb and...",
          "slug": "monitoring-dirac-components",
          "status": "completed",
          "student_name": "Pujan Mehta",
          "student_profile": null,
          "tags": [
            "ai",
            "ui",
            "backend"
          ],
          "title": "Monitoring DIRAC Components"
        },
        {
          "code_url": "https://github.com/dosarudaniel/ReliableMulticastForALICE",
          "description": "<p>ALICE (A Large Ion Collider Experiment) is a heavy-ion detector on the Large Hadron Collider (LHC) ring. It is designed to study the physics of strongly interacting matter at extreme energy densities, where a phase of matter called quark-gluon plasma forms.\nThe new ALICE synchronous data reconstruction facility for Run 3 needs a real-time conditions and calibration data distribution mechanism. New calibration objects are produced at up to 50Hz and have to be propagated to about 2000 servers. For efficient data distribution in this environment a network multicast delivery mechanism has to be used. There will be two sides to be implemented for this project: a library to send the newly produced objects and a caching service to run on each of the 2000 servers to receive and keep in memory the objects, making them available to the localhost running processes via a REST API.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_022",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6498641026482176/",
          "proposal_id": null,
          "short_description": "ALICE (A Large Ion Collider Experiment) is a heavy-ion detector on the Large Hadron Collider (LHC) ring. It is designed to study the physics of...",
          "slug": "real-time-conditions-data-distribution-for-the-online-data-processing-of-the-alice-experiment",
          "status": "completed",
          "student_name": "danieldo",
          "student_profile": null,
          "tags": [
            "api",
            "ai"
          ],
          "title": "Real-time conditions data distribution for the Online data processing of the ALICE experiment"
        },
        {
          "code_url": "https://github.com/Mohitty/GSoC2019",
          "description": "<p>CERNBox provides cloud data storage to all CERN users to store, share and synchronize their data across all devices. It is integrated with variety of application providers which empowers the users to work collaboratively. In a non-cloud environment, end users have the freedom to choose the application they wish to manipulate a particular file.</p>\n<p>With the move to the cloud, this freedom was lost; it is the administrator of the cloud that decides for the user what applications are available on the cloud. This lack of freedom creates two problems: the first one is that it may reduce the productivity as cloud users have to learn a new tool they were not used to. The second one is that often times the application provided does not satisfy the requirements of users, and these are left performing an inefficient cycle of downloading a file from the cloud to the local computer, manipulate the file locally and uploading the file again to the cloud, dismissing the ubiquity pillar of cloud-based computing.</p>\n<p>It is in the goal of this proposal to prototype the concept of Bring Your Own Application and to demonstrate how to regain the right of freedom for cloud end-users</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_023",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6551465701670912/",
          "proposal_id": null,
          "short_description": "CERNBox provides cloud data storage to all CERN users to store, share and synchronize their data across all devices. It is integrated with variety of...",
          "slug": "cernbox-bring-your-own-application",
          "status": "completed",
          "student_name": "Mohit Tyagi",
          "student_profile": null,
          "tags": [
            "ai",
            "cloud",
            "ui"
          ],
          "title": "CERNBox: Bring Your Own Application"
        },
        {
          "code_url": "https://github.com/sahiljajodia01/k8s-selection",
          "description": "<p>This project aims to develop a Jupyter notebook plugin which deploys Spark required services to a kubernetes cluster on OpenStack cloud at CERN.</p>\n<p>Kubernetes provides scaling when the traffic or computation increases by launching a Spark driver pod in the cluster which in turn creates multiple Spark executer pods which executes the application code.</p>\n<p>The services that will be attached to the Kubernetes cluster are CERN CVMFS, Spark shuffle service, and Spark history server. These services are needed for running Spark on Kubernetes. Physicists can then use Spark running in the background to perform scalable interactive data analysis and visualization.</p>\n<p>Also, a proper UI will be provided inside the Jupyter notebook so that a user can attach various services to the cluster. This plugin then will be integrated with SWAN notebook service which CERN provides.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_024",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6076428561416192/",
          "proposal_id": null,
          "short_description": "This project aims to develop a Jupyter notebook plugin which deploys Spark required services to a kubernetes cluster on OpenStack cloud at CERN....",
          "slug": "creation-and-usage-of-disposable-spark-on-kubernetes-cluster-from-swan-notebook",
          "status": "completed",
          "student_name": "Sahil Jajodia",
          "student_profile": null,
          "tags": [
            "ai",
            "kubernetes",
            "cloud",
            "ui"
          ],
          "title": "Creation and usage of disposable Spark on Kubernetes cluster from SWAN notebook"
        },
        {
          "code_url": "https://github.com/Divya063/TestingFramework",
          "description": "<p>SWAN (Service for Web-based ANalysis) is a cloud data analysis service developed and powered by CERN that provides Jupyter notebooks on demand. It is based on Jupyter upstream technology but it is deeply integrated with CERN-specific services, e.g., EOS, CERNBox, CVMFS.\nThe project aim is to create a testing framework for both upstream Jupyter components and SWAN-specific components which will allow the addition of new tests to cover new features of the SWAN service and would be self-contained and distributable by means of Docker containers. The testing framework should include functional tests, regression tests and performance tests.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_025",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4839789902168064/",
          "proposal_id": null,
          "short_description": "SWAN (Service for Web-based ANalysis) is a cloud data analysis service developed and powered by CERN that provides Jupyter notebooks on demand. It is...",
          "slug": "testing-framework-for-jupyter-notebooks",
          "status": "completed",
          "student_name": "Divya Rani",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "docker",
            "cloud"
          ],
          "title": "Testing framework for Jupyter notebooks"
        },
        {
          "code_url": "https://inzamamiqbal.blogspot.com/2019/08/google-summer-of-code-2019-inzamam.html",
          "description": "<p>Ganga is used to execute a user defined computational task on a distributed back-end. Through this project we let the users define the environment in which their task need to be executed. So the worker node will pull the user defined container and execute the task on it.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_026",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/4949605840584704/",
          "proposal_id": null,
          "short_description": "Ganga is used to execute a user defined computational task on a distributed back-end. Through this project we let the users define the environment in...",
          "slug": "create-a-user-interface-for-ganga-that-allows-for-the-execution-of-tasks-inside-user-specified-virtual-machines",
          "status": "completed",
          "student_name": "Inzamam Iqbal",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Create a user interface for Ganga that allows for the execution of tasks inside user specified virtual machines."
        },
        {
          "code_url": "https://gist.github.com/97amarnathk/7aa601c4272ac6405cc550c6c0a8aa77",
          "description": "<p>DIRAC is an open source interware platform whose roles are submission of jobs, the management of the data produced, to the orchestration of the distributed resources. It is a generic software, used and extended by several Virtual Organizations (VO).</p>\n<p>It monitors resources like computing, storage, catalogue resources, networks, information providers, file transfer services, message queues or databases services. Members of a VO can use a mask composed of services exposed by local resources. Experienced Grid administrators apply procedures for managing such services, based on their status, as it is reported by an ever-growing set of monitoring tools. When a procedure is agreed and well-exercised, a formal policy could be derived.</p>\n<p>For this reason, using the DIRAC framework is developed as a policy system that can enforce management and operational policies, in a VO-specific fashion.</p>\n<p>This project proposal offers :</p>\n<ul>\n<li>A reliable working implementation of a multi-VO Dirac Resource Status System (RSS).</li>\n<li>A base database access module for the RSS which would unify the SQLAlchemy frontend and also reduce code duplication.</li>\n<li>Remove sqlite3 from some RSS modules and port them to MySQL backend</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_027",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5784571189133312/",
          "proposal_id": null,
          "short_description": "DIRAC is an open source interware platform whose roles are submission of jobs, the management of the data produced, to the orchestration of the...",
          "slug": "distributed-computing-resources-aggregation-usage-monitoring",
          "status": "completed",
          "student_name": "Amarnath Karthi",
          "student_profile": null,
          "tags": [
            "database",
            "frontend",
            "backend"
          ],
          "title": "Distributed Computing Resources: aggregation, usage, monitoring"
        },
        {
          "code_url": "https://gist.github.com/cAbhi15/70619824a8a24dc70cf061a766788934",
          "description": "<p>A huge volume of data is generated every night by large astronomical telescopes around the world. A robust and scalable software infrastructure is necessary to be able to leverage such high volume and high velocity of data. <a href=\"https://fink-broker.readthedocs.io/en/latest\" target=\"_blank\">Fink</a> is an Apache Spark based broker infrastructure to receive, process and redistribute such high-velocity astronomical data obtained from telescopes in real-time such as the <a href=\"https://www.lsst.org\" target=\"_blank\">LSST</a>. The aim of this project is to develop an Alert Redistribution System for Fink using the state-of-the-art technologies of Big Data processing and distribution (Apache Spark, Apache Avro and Apache Kafka). This Alert Redistribution system will help scientific users to access real-time data and carry out follow up research at their ends.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_028",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/5848657855774720/",
          "proposal_id": null,
          "short_description": "A huge volume of data is generated every night by large astronomical telescopes around the world. A robust and scalable software infrastructure is...",
          "slug": "alert-redistribution-system-for-fink-an-apache-spark-based-broker-for-astronomy",
          "status": "completed",
          "student_name": "Abhishek Chauhan",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Alert Redistribution System for Fink : an Apache Spark based Broker for Astronomy"
        },
        {
          "code_url": "https://gist.github.com/540KJ/18e7452da0df7f5af584234bd1c308f6",
          "description": "<p>Clad is a C++ Clang compiler plugin that employs automatic differentiation to derive user-defined functions, performing source code transformations so that users do not have to conform to custom types to comply with external libraries.</p>\n<p>With the integration of clad::gradient to CLAD, a reverse accumulation method for automatic differentiation was introduced. Now, it makes sense to move on to second partial derivatives, in particular, to calculating the Hessian matrix. My work is to build on the existing framework that uses Clang AST to do source transformations on functions, and to implement an efficient Hessian calculation method that extends the capabilities of CLAD using the edge pushing algorithm and Hessian reverse accumulation method. I will also try to extend existing CLAD functions to calculate the Jacobian matrix.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2019_029",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6004151392141312/",
          "proposal_id": null,
          "short_description": "Clad is a C++ Clang compiler plugin that employs automatic differentiation to derive user-defined functions, performing source code transformations...",
          "slug": "generating-hessians-and-jacobians-via-clad",
          "status": "completed",
          "student_name": "Jack Qiu",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "Generating Hessians and Jacobians via CLAD"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2019/organizations/4534645092253696/"
    },
    "year_2020": {
      "num_projects": 34,
      "projects": [
        {
          "code_url": "https://gist.github.com/gargvaibhav64/1fd2095e5229bd898a6edbb6a3830dc4",
          "description": "<p>ROOT has several features that interact with libraries and require implicit header inclusion. This can be triggered by reading or writing data on disk, or user actions at the prompt. Exposing the full shared library descriptors to the interpreter at runtime translates into an increased memory footprint. ROOT’s exploratory programming concepts allow implicit and explicit runtime shared library loading. It requires the interpreter to load the library descriptor. Re-parsing of descriptors’ content has a noticeable effect on runtime performance. \nC++ Modules are designed to minimize the reparsing of the same header content by providing an efficient on-disk representation of the C++ Code. C++ Modules have been implemented for Unix and OS X systems already and it is expected that with next release of ROOT, C++ modules will be default on OS X. This project aims to extend the C++ Modules support for Windows, by implementing compatible solutions to the UNIX baseline and also display corresponding performance results.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6406031539699712/",
          "proposal_id": null,
          "short_description": "ROOT has several features that interact with libraries and require implicit header inclusion. This can be triggered by reading or writing data on...",
          "slug": "enable-modules-on-windows",
          "status": "completed",
          "student_name": "Vaibhav Garg",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Enable Modules on Windows"
        },
        {
          "code_url": "https://docs.google.com/document/d/1gSLThWUqDj5DUfIdCHBZZsam7a3EKSfqQD2jEQ2P73U/edit?usp=sharing",
          "description": "<p>The goal of this project is to implement the Gilbert-Johnson-Keerthi algorithm on convex objects in VecGeom along with a ray casting algorithm that is based on GJK. This would lead to simplifying and potentially optimizing the code base as well as paving the way for other uses in VecGeom, such as checking for illegal overlapping and detection of neighbor volumes for optimizing navigation. It will stand as a general utility in VecGeom available for all supported primitive solids.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6594413402259456/",
          "proposal_id": null,
          "short_description": "The goal of this project is to implement the Gilbert-Johnson-Keerthi algorithm on convex objects in VecGeom along with a ray casting algorithm that...",
          "slug": "implementation-of-gilbert-johnson-keerthi-algorithm-for-convex-shapes-in-vecgeom",
          "status": "completed",
          "student_name": "Andrei Mihailescu",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Implementation of Gilbert-Johnson-Keerthi Algorithm for Convex Shapes in VecGeom"
        },
        {
          "code_url": "https://gist.github.com/saucam/7f7d4291c56c8f9e0b7ac2345e30947c",
          "description": "<p>AstroLab Software has developed Fink, an apache-spark based broker infrastructure, which is able to analyze this large stream of alerts data from the telescopes like LSST, and then redistribute it to subscribers, enabling a wide range of applications and services to consume this data.\nThis processed data needs to be stored for visualizing and post-processing. The efficient manipulation and visualization of patterns in this extremely large dataset is a real challenge for which we need this project.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6681318307397632/",
          "proposal_id": null,
          "short_description": "AstroLab Software has developed Fink, an apache-spark based broker infrastructure, which is able to analyze this large stream of alerts data from the...",
          "slug": "manipulation-of-massive-astronomical-data-using-graphs",
          "status": "completed",
          "student_name": "Yash Datta",
          "student_profile": null,
          "tags": [],
          "title": "Manipulation of massive astronomical data using graphs"
        },
        {
          "code_url": "http://9inpachi.github.io/reports/gsoc2020/",
          "description": "<p>Phoenix is a web-based, experiment independent event display framework in JavaScript for visualizing HEP experiment data from multiple experiments. This project focuses on the following improvements to Phoenix.</p>\n<h5>Improvements</h5>\n<ul>\n<li>A more feature rich, control-oriented and intuitive GUI</li>\n<li>Easier selection of 3D objects and event data (like Tracks, Jets etc.)</li>\n<li>Keyboard controls for controlling the event display through keys</li>\n<li>Runge-kutta propagator for using Track parameters to calculate Track positions and decreasing data format size</li>\n<li>Extension of the current common JSON data format to support more diverse experiments and their event data</li>\n<li>Support for more CERN experiments like CMS</li>\n<li>Loaders for loading event data from \".root\" and \".ig\" files</li>\n<li>Loading and processing event data from cloud servers</li>\n<li>Animation of events to simulate the propagation of particles</li>\n<li>Improvement of VR features</li>\n<li>Investigating TGeo/GEANT4 geometries and implementing converters to display them in Phoenix</li>\n<li>Extending unit tests and creating UI tests</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5680564721418240/",
          "proposal_id": null,
          "short_description": "Phoenix is a web-based, experiment independent event display framework in JavaScript for visualizing HEP experiment data from multiple experiments....",
          "slug": "phoenix-experiment-independent-event-display",
          "status": "completed",
          "student_name": "Fawad",
          "student_profile": null,
          "tags": [
            "java",
            "javascript",
            "web",
            "cloud",
            "ui"
          ],
          "title": "Phoenix - Experiment Independent Event Display"
        },
        {
          "code_url": "https://github.com/HSF/Qt3DExaminerViewer",
          "description": "<p>Interactive visualization can provide intuitive insights for experiment setups and results in many domains, including High Energy Physics. Therefore, Virtual Point1 (VP1) was developed by ATLAS for 3D event display. Nevertheless, it seems a pity that it is based on an old graphics engine(OpenInventor) and resides in a big project(full ATLAS experiment software stack) with many dependencies.</p>\n<p>To get free of these limitations, GeoModelExplorer, a super-lighter version of VP1 has been developed, with a strong focus on the visualization of geometry data. Some of the important next steps are to replace the old 3D engine, improve existing functions and add more features. All of them are waiting for development.</p>\n<p>In this project, I will take these steps one by one, integrate the modern graphics engine Qt3D, improve the geometry rendering effects, extend the mesh I/O abilities, and add smooth animation for view inspections. There will be a large amount of design and engineering work but I will take extreme care. This project will eventually make contributions as part of the current software modernization effort in ATLAS.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5749988472127488/",
          "proposal_id": null,
          "short_description": "Interactive visualization can provide intuitive insights for experiment setups and results in many domains, including High Energy Physics. Therefore,...",
          "slug": "geomodelexplorer-improving-interactive-3d-geometry-visualization-tool-based-on-qt3d",
          "status": "completed",
          "student_name": "Huajian Qiu",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "GeoModelExplorer: improving interactive 3D geometry visualization tool based on Qt3D"
        },
        {
          "code_url": "https://shivanshs9.me/summers/gsoc/2020-cern-hsf-report",
          "description": "<p>This project aims to develop a Kubernetes Operator for XRootD, along with its related documentation, in order to ease and fully automate deployment and management of XRootD clusters. This Operator targets the whole field of existing infrastructure where XRootD can run: development workstations, continuous integration platforms, bare-metal clusters in academic datacenters, and also public cloud platforms.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5388572846194688/",
          "proposal_id": null,
          "short_description": "This project aims to develop a Kubernetes Operator for XRootD, along with its related documentation, in order to ease and fully automate deployment...",
          "slug": "kubernetes-operator-for-xrootd",
          "status": "completed",
          "student_name": "Shivansh Saini",
          "student_profile": null,
          "tags": [
            "ai",
            "kubernetes",
            "cloud"
          ],
          "title": "Kubernetes operator for XRootD"
        },
        {
          "code_url": "https://github.com/cms-sw/cmssw/pulls?q=is%3Apr+is%3Aclosed+author%3Acamolezi",
          "description": "<p>This project has the goal to find and decrease boost dependencies in CMSSW. Modern C++ introduced a lot of new features that were only available through boost packages. Thus, some boost code can be replaced with similar C++ standard library features. Using standard features is a good practice, this project will move the CMSSW codebase in that direction.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5397144158076928/",
          "proposal_id": null,
          "short_description": "This project has the goal to find and decrease boost dependencies in CMSSW. Modern C++ introduced a lot of new features that were only available...",
          "slug": "reduce-boost-dependence-in-cmssw",
          "status": "completed",
          "student_name": "Lucas Camolezi",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Reduce boost dependence in CMSSW"
        },
        {
          "code_url": "https://github.com/ahariri13/FALCON",
          "description": "<p>Prior to the HL-LHC operation, detector simulations undergo further developments in order to adapt to the increasing amounts of events. Current infrastructure still faces limitations to tackle the expected increase in data in terms of storage capacity and computation time. Early event reconstructions were based on matrix element methods followed by Monte Carlo techniques. Recent advances focused on boosting the speed of event simulations. For instance, C++ - based DELPHES uses simplified detector geometries and particle-material interactions, hence mapping the detector response into a parametric function. Nevertheless, this technique is limited by its exclusiveness to a specific detector geometry at a time, with any detector changes requiring the framework to be adjusted accordingly through hand-coding. Falcon (previously Turbosim), a fast stimulation framework that uses non-parametric methods to discern detector responses without the need for hand-coding. That being said, we aim to investigate the efficiency of deep generative models in simulating event reconstructions in a given detector, hence potentially replacing conventional complex algorithms.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2020_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4965683756007424/",
          "proposal_id": null,
          "short_description": "Prior to the HL-LHC operation, detector simulations undergo further developments in order to adapt to the increasing amounts of events. Current...",
          "slug": "falcon-fast-simulation-using-deep-generative-models",
          "status": "completed",
          "student_name": "Ali Hariri",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Falcon: Fast Simulation using Deep Generative Models"
        },
        {
          "code_url": "https://docs.google.com/document/d/1_ZSMS4NyRWAJHsfdPReiDncD3sBu0tGNGOfO-XPdJr8/edit?usp=sharing",
          "description": "<p>Graph neural networks is a powerful generalization of a whole class of neural (and not only neural) architectures on the data, organized in a graph. The graph representation is on itself a powerful tool, allowing to explicitly show dependencies between features or attributes and abstract this prior from the architecture level to the level of the data.</p>\n<p>Particular special cases of graph networks are already successfully used in particle physics and, therefore, implementing such tools for ROOT in general as a leading particle physics data analytics framework, and TMVA in particular is of utmost importance.</p>\n<p>Implementing a generalized version of graph networks as we propose in this context, as opposed to implementing every possible architecture like Deep Sets or Message Passing NNs, will ensure the wide applicability and the ease of modification and adaptation for a particular task.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4998027577655296/",
          "proposal_id": null,
          "short_description": "Graph neural networks is a powerful generalization of a whole class of neural (and not only neural) architectures on the data, organized in a graph....",
          "slug": "graph-neural-networks",
          "status": "completed",
          "student_name": "Stanislav Lukyanenko",
          "student_profile": null,
          "tags": [],
          "title": "Graph Neural Networks"
        },
        {
          "code_url": "https://gist.github.com/mxxo/2a2ecbc92a62d3893b43d10459454ef1",
          "description": "<p>The <code>RNTuple</code> interface is an experimental I/O subsystem for the <code>ROOT</code> analysis framework. Preliminary benchmarks have demonstrated significant performance and space improvements over the legacy <code>TTree</code> system. <code>RNTuple</code> files are split into storage and metadata. Using metadata offset information, <code>RNTuple</code> files can be merged together without a costly storage decompression step. This project will implement a fast merge function to combine <code>RNTuple</code> data sets. The <code>hadd</code> command-line tool will be updated to take advantage of the new <code>RNTuple</code> merge features.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5032727826399232/",
          "proposal_id": null,
          "short_description": "The RNTuple interface is an experimental I/O subsystem for the ROOT analysis framework. Preliminary benchmarks have demonstrated significant...",
          "slug": "fast-merging-of-rntuple-data-sets",
          "status": "completed",
          "student_name": "Max Orok",
          "student_profile": null,
          "tags": [],
          "title": "Fast Merging of RNTuple Data Sets"
        },
        {
          "code_url": "https://github.com/root-project/root/pull/5757",
          "description": "<p>Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in particle physics data analysis and applications. TMVA provides functionality for building deep neural networks including fully connected, convolutional and recurrent layers. In addition, TMVA provides interfaces to other deep learning frameworks (scikit-learn, Keras). This project will focus on the development of an interface to PyTorch which will leverage existing PyMVA interface.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5085504384532480/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in...",
          "slug": "tmva-interfaces-pytorch",
          "status": "completed",
          "student_name": "Anirudh Dagar",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "TMVA Interfaces - Pytorch"
        },
        {
          "code_url": "https://alkaidcheng.github.io/quple.github.io/docs/Quple_GSoC_report.pdf",
          "description": "<p>This project serves to implement a common framework for applying quantum machine learning algorithms to high energy physics analysis. A major focus will be placed on classifier algorithms that are important for distinguishing signal and background events in high energy physics experiments. The framework will use the quantum variational method as a basic quantum machine learning classifier algorithm.The framework will feature a modular and extensible architecture where its components can be reused to design different machine learning algorithms such as quantum neural networks and a user may contribute to the framework by providing new algorithms and components.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5091840300154880/",
          "proposal_id": null,
          "short_description": "This project serves to implement a common framework for applying quantum machine learning algorithms to high energy physics analysis. A major focus...",
          "slug": "qmlhep-proposal-quple-quantum-machine-learning-framework-for-high-energy-physics",
          "status": "completed",
          "student_name": "Chi Lung Cheng",
          "student_profile": null,
          "tags": [
            "ml",
            "ui"
          ],
          "title": "(QMLHEP Proposal) Quple - Quantum Machine Learning Framework for High Energy Physics"
        },
        {
          "code_url": "https://github.com/uccross/skyhookdm-ceph/wiki/Google-Summer-of-Code-2020-Report",
          "description": "<p>SkyhookDM supports dynamic data management in the cloud by enabling data management tasks to be executed directly within the storage. It uses customised C++ object classes to offer support for offloading database operations  directly to the object storage layer. The project aims to improvise and extend SkyhookDM’s capabilities by incorporating the following functionalities:</p>\n<ol>\n<li>GROUPBY and ORDERBY database operations: Extend current aggregation method to include GROUPBY and sort (ORDERBY) for an object’s formatted data partitions</li>\n<li>Statistics collection: Implement a custom method for data statistics collection of an object’s formatted data partitions in form of histograms</li>\n<li>Data compaction: Implement compaction of multiple formatted sub-partitions within an object into a single partition.</li>\n</ol>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5382644348485632/",
          "proposal_id": null,
          "short_description": "SkyhookDM supports dynamic data management in the cloud by enabling data management tasks to be executed directly within the storage. It uses...",
          "slug": "extend-skyhookdm-programmable-object-storage-with-statistics-sortaggregate-or-data-compaction-functions",
          "status": "completed",
          "student_name": "aditigupta17",
          "student_profile": null,
          "tags": [
            "ai",
            "database",
            "cloud"
          ],
          "title": "Extend SkyhookDM programmable object storage with statistics, sort/aggregate or data compaction functions."
        },
        {
          "code_url": "https://medium.com/@kalana.16/gsoc-2020-cern-hsf-framework-for-automating-the-retrieval-and-analysis-of-system-information-6a59bf5f036d",
          "description": "<p>JAliEn introduces automatic containerization for jobs submitted to the WLCG. To fully utilize this feature, and to plan ahead for how new software will be deployed in the future, it is required to know what and how many Grid sites of WLCG support this feature. It is necessary to check the compatibility of each worker node in each Grid site of WLCG to identify the Grid sites that support containerization. In addition to that, it is important to collect the hardware and software information of the nodes in each Grid site to fully identify the facilities provided by each Grid site. At the moment, a job can be submitted manually to the Grid to retrieve system information of a node, but it is a very tedious and time-consuming task to keep submitting jobs manually until the jobs cover all the Grid sites in WLCG. Also, it is very complex to analyze the result of each job manually. As a solution, this project intends to create a new framework to automate collecting, filtering and storing of information of computing nodes across the WLCG.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2020_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6250330150404096/",
          "proposal_id": null,
          "short_description": "JAliEn introduces automatic containerization for jobs submitted to the WLCG. To fully utilize this feature, and to plan ahead for how new software...",
          "slug": "framework-for-automating-the-retrieval-and-analysis-of-system-information-of-grid-sites-in-wlcg",
          "status": "completed",
          "student_name": "Kalana Wijethunga",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Framework for Automating the Retrieval and Analysis of System Information of Grid sites in WLCG"
        },
        {
          "code_url": "https://github.com/ganga-devs/ganga/issues/1662",
          "description": "<p>This project will replace the old xml-based metadata system with a database, which will be assisted by a caching mechanism that  will allow for a more responsive user interface. The containerized database will also allow for remote access to job metadata, provided the user has access to the host of the container. The project will enhance the process of large scale computing of batch jobs at CERN and other similar organizations.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6259599360917504/",
          "proposal_id": null,
          "short_description": "This project will replace the old xml-based metadata system with a database, which will be assisted by a caching mechanism that  will allow for a...",
          "slug": "upgrading-the-ganga-user-interface-to-use-a-relational-database-for-persistent-storage",
          "status": "completed",
          "student_name": "Ratin Kumar",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "database"
          ],
          "title": "Upgrading the Ganga user interface to use a relational database for persistent storage"
        },
        {
          "code_url": "https://gist.github.com/adityaddy/87e3b60ecaee1907c19fb9d3ac8fa14e",
          "description": "<p>Rivet is a software package for performing data analysis on simulated particle collision events like those in the Large Hadron Collider. This project will focus on making Rivet performant on modern CPU architectures in HEP and HPC compute facilities, particularly ensuring that the central result caching and dispatching system is thread-safe and amenable to vectorized compiler optimizations. We will be working to Improve the Rivet's computational performance, make it thread-safe, profile and optimize the expensive computational components and ensure continuity of physics behaviour via the Gitlab CI engine.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4662564862033920/",
          "proposal_id": null,
          "short_description": "Rivet is a software package for performing data analysis on simulated particle collision events like those in the Large Hadron Collider. This project...",
          "slug": "mcnetrivet-speed-and-accuracy-in-the-lhcs-mc-analysis-tool",
          "status": "completed",
          "student_name": "Aditya Kumar-2",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "MCnet/Rivet - Speed and accuracy in the LHC's MC analysis tool"
        },
        {
          "code_url": "https://github.com/nidhihegde001/CRISMIS",
          "description": "<p>CRISMIS is an open-source, AI-based tool for the identification and classification of cosmic-ray artifacts in imaging data. Its purpose is to cater to planetary scientists and space science teams and enable them to automatically filter and search for the presence of cosmic ray artifacts in their data.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4795402060038144/",
          "proposal_id": null,
          "short_description": "CRISMIS is an open-source, AI-based tool for the identification and classification of cosmic-ray artifacts in imaging data. Its purpose is to cater...",
          "slug": "cosmic-ray-imaging-studies-via-mission-imagery-from-space-crismis",
          "status": "completed",
          "student_name": "Nidhi Hegde",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Cosmic-Ray Imaging Studies via Mission-Imagery from Space (CRISMIS)"
        },
        {
          "code_url": "https://indrarahul2013.github.io/2020/07/24/google-summer-of-code.html",
          "description": "<p>This project aims to develop  an intelligent and reliable monitoring system for large distributed services to monitor their status and reduce operational costs. The distributed computing infrastructure is the backbone of all computing activities of the CMS experiment at CERN. These distributed services include central services for authentication, workload management, data management, databases, etc.</p>\n<p>Very large amounts of information are produced from this infrastructure. These include various anomalies, issues, outages, and those involving scheduled maintenance. The sheer volume and variety of information make it too large to be handled by the operational team. Hence we aim to build an intelligent system that will detect, analyze and predict the abnormal behaviors of the infrastructure.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4827645050617856/",
          "proposal_id": null,
          "short_description": "This project aims to develop  an intelligent and reliable monitoring system for large distributed services to monitor their status and reduce...",
          "slug": "intelligent-alert-management-system-for-hep-experiments",
          "status": "completed",
          "student_name": "Rahul Indra",
          "student_profile": null,
          "tags": [
            "ai",
            "database",
            "ui"
          ],
          "title": "Intelligent Alert Management System For HEP experiments"
        },
        {
          "code_url": "https://gist.github.com/NickRoz1/98a6ef0bbf4e6b9d28d52906d0f96df1",
          "description": "<p>Enhance a statistical toolkit for physics analysis interpretation.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6521364598489088/",
          "proposal_id": null,
          "short_description": "Enhance a statistical toolkit for physics analysis interpretation.",
          "slug": "mcnetyoda",
          "status": "completed",
          "student_name": "Nick Rozinsky",
          "student_profile": null,
          "tags": [],
          "title": "MCnet/YODA"
        },
        {
          "code_url": "https://gist.github.com/varunbankar/1d292808cd5de204602e68c5888145be",
          "description": "<p>The idea of this project is to create an additional interface for users to interact with Ganga. The new responsive Graphical User Interface (GUI) will improve the user experience without disturbing the existing workflow and allow users to create, monitor, and manage Ganga jobs in real-time locally as well as remotely. The powerful and secure RESTful APIs will add modularity to the software by allowing it to be integrated with other applications and be controlled by them. This project will also benefit the new users in the computing community in adopting/adapting the tool with relative ease.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6539053387743232/",
          "proposal_id": null,
          "short_description": "The idea of this project is to create an additional interface for users to interact with Ganga. The new responsive Graphical User Interface (GUI)...",
          "slug": "implementation-of-graphical-user-interface-for-ganga",
          "status": "completed",
          "student_name": "Varun Bankar",
          "student_profile": null,
          "tags": [
            "api",
            "ui"
          ],
          "title": "Implementation of Graphical User Interface for Ganga"
        },
        {
          "code_url": "https://github.com/Mohitty/GSoC2020-Report",
          "description": "<p>It has been shown that only a small portion of all the files in a container image is necessary to run the image itself. This is even more accentuated in scientific container images since they usually include complex software stacks comprising hundreds of thousands of files, and often not all the files are needed for each task.\nCernVM-FS is a globally-distributed file system used to distribute software to data centers and end-user workstations in a fast, scalable and reliable way. Files and file metadata are downloaded on demand and aggressively cached.\nHence we can utilize the lazy load capabilities of CVMFS with various container engines to get the containers running faster than usual. There is already a plugin for Docker available for this purpose and a prototype for Kubernetes is also being developed. This project aims to utilize CVMFS capabilities in podman workflow, to quickly load big scientific container images while maintaining the isolation and convenience of containers.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2020_021",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6547926152642560/",
          "proposal_id": null,
          "short_description": "It has been shown that only a small portion of all the files in a container image is necessary to run the image itself. This is even more accentuated...",
          "slug": "cernvm-fs-podman-cvmfs-integration",
          "status": "completed",
          "student_name": "Mohit Tyagi",
          "student_profile": null,
          "tags": [
            "ai",
            "docker",
            "kubernetes",
            "ui"
          ],
          "title": "CernVM-FS: Podman cvmfs integration"
        },
        {
          "code_url": "https://kamahori.github.io/gsoc2020/",
          "description": "<p>This project aims to validate the possibility of using pre-conditioners in compressing ROOT file formats (TTree and RNTuple); to understand how we can improve the functionality in the context of both lossy and lossless compression algorithms; and to investigate new BYTE_STREAM_SPLIT encoding.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_022",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4854834844401664/",
          "proposal_id": null,
          "short_description": "This project aims to validate the possibility of using pre-conditioners in compressing ROOT file formats (TTree and RNTuple); to understand how we...",
          "slug": "pre-conditioners-applied-to-root-compression-algorithms",
          "status": "completed",
          "student_name": "Keisuke Kamahori",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Pre-conditioners applied to ROOT compression algorithms"
        },
        {
          "code_url": "https://medium.com/@viveknigam3003/how-i-improved-scientific-data-management-during-gsoc-20-ffdf995266e6?source=friends_link&sk=003aeade0cba10632c24f7117a27907b",
          "description": "<p>Rucio is a data management framework which provides a multilevel organization, management, and access to large scale data generated during scientific experiments, such as Large Hadron Collider (LHC). It also allows data analytics and monitoring of such data and its resources. The data is processed in globally distributed data centres.</p>\n<p>Currently, Rucio uses a Command Line Interface (CLI) and a WebUI to interact with the users at the client and admin level. However, using the CLI Client might often need technical expertise and also prove overwhelming for some of its users. And the WebUI provides limited functionality due to memory limitations of the browser. I propose to solve this problem by developing a native desktop application for Rucio, which provides an elegant graphical user interface (GUI) with drag and drop downloading of files.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_023",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/4873083892006912/",
          "proposal_id": null,
          "short_description": "Rucio is a data management framework which provides a multilevel organization, management, and access to large scale data generated during scientific...",
          "slug": "native-desktop-application-for-rucio",
          "status": "completed",
          "student_name": "Vivek Nigam",
          "student_profile": null,
          "tags": [
            "web",
            "ui"
          ],
          "title": "Native Desktop Application for Rucio"
        },
        {
          "code_url": "https://indico.cern.ch/event/946427/#4-anish-biswas",
          "description": "<p>The goal of this project is to add a GPU backend to Awkward Array 1.x. With such a backend, the library would have a unique capability of processing complex, JSON-like data that is wholly resident on GPUs, in an efficient way, without specialized compilation or even leaving the Python prompt. Deployment is a major part of this project and the aim is to have a seamless transition from the CPU backend to the GPU backend, with the dependent projects having to make minimal changes to accomodate for the new backend.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2020_024",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5762964205862912/",
          "proposal_id": null,
          "short_description": "The goal of this project is to add a GPU backend to Awkward Array 1.x. With such a backend, the library would have a unique capability of processing...",
          "slug": "awkward-array-gpu-kernels",
          "status": "completed",
          "student_name": "Anish Biswas",
          "student_profile": null,
          "tags": [
            "python",
            "ml",
            "ai",
            "backend"
          ],
          "title": "Awkward Array GPU Kernels"
        },
        {
          "code_url": "https://github.com/root-project/root/pull/6263",
          "description": "<p>This project is about the development of  3D CNN functionality in TMVA. I shall be developing both 3D convolution as well as 3D pooling layers. 3D CNNs have very promising applications in particle physics, such as imaging calorimetry and particle tracking, allowing physicists to use new techniques to identify particles and search for new physics. In addition, they can be used as Generative Adversarial Networks for fast imaging detector simulations.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_025",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5765608429322240/",
          "proposal_id": null,
          "short_description": "This project is about the development of  3D CNN functionality in TMVA. I shall be developing both 3D convolution as well as 3D pooling layers. 3D...",
          "slug": "development-of-3d-cnn-in-tmva",
          "status": "completed",
          "student_name": "Surya S Dwivedi",
          "student_profile": null,
          "tags": [],
          "title": "Development of 3D CNN in TMVA"
        },
        {
          "code_url": "https://github.com/DeepLense-Unsupervised/unsupervised-lensing",
          "description": "<p>Gravitational lensing has been a cornerstone in many cosmology experiments, and studies since it was discussed in Einstein’s calculations back in 1936 and discovered in 1979, and one area of particular interest is the study of dark matter via substructure in strong lensing images. While statistical and supervised machine learning algorithms have been implemented for this task, the potential of unsupervised deep learning algorithms is yet to be explored and could prove to be crucial in the analysis of LSST data. The primary aim of this GSoC 2020 project is to design a python-based framework for implementing unsupervised deep learning architectures to study strong lensing images.</p>\n<p>Refer to the <a href=\"https://arxiv.org/abs/2008.12731\" target=\"_blank\">paper</a> for more details.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_026",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5812807469105152/",
          "proposal_id": null,
          "short_description": "Gravitational lensing has been a cornerstone in many cosmology experiments, and studies since it was discussed in Einstein’s calculations back in...",
          "slug": "building-a-python-based-framework-for-unsupervised-deep-learning-applications-in-strong-lensing-cosmology",
          "status": "completed",
          "student_name": "K Pranath Reddy",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "Building a Python-based framework for Unsupervised Deep Learning applications in strong lensing cosmology"
        },
        {
          "code_url": "https://github.com/PRATEEKKUMARAGNIHOTRI/CMS_muon_transverse_momentum_estimation",
          "description": "<p>CMS is a general-purpose detector at LHC. During a run, it generates about 40 TB data per second. Since It is not feasible to readout and store such a vast amount of data, so it selects and stores only interesting events or events likely to reveal new physics phenomena. Currently, Boosted Decision Trees (BDTs) with handcrafted features are used for that.</p>\n<p>The goal of the project is to test the performance of different deep learning algorithms and minimize the latency in inference. The best models in each algorithmic type will be wrapped into a functional prototype emulator. Other than this, a significant emphasis will be given on proper documentation.</p>\n<p>Links - <a href=\"https://github.com/PRATEEKKUMARAGNIHOTRI/Muon-Momentum-Estimation-and-Particle-Identification\" target=\"_blank\">Submitted pre-tasks</a> | <a href=\"https://drive.google.com/file/d/1mzLp3aWz1Kb2UADOphoS1ILmlzo35BPI\" target=\"_blank\">CV</a> | <a href=\"https://github.com/PRATEEKKUMARAGNIHOTRI/\" target=\"_blank\">GitHub handle</a></p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_027",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5949448733065216/",
          "proposal_id": null,
          "short_description": "CMS is a general-purpose detector at LHC. During a run, it generates about 40 TB data per second. Since It is not feasible to readout and store such...",
          "slug": "deep-learning-algorithms-for-muon-momentum-estimation-in-the-cms-trigger-system",
          "status": "completed",
          "student_name": "Prateek Agnihotri",
          "student_profile": null,
          "tags": [
            "ml"
          ],
          "title": "Deep Learning Algorithms for Muon Momentum Estimation in the CMS Trigger System"
        },
        {
          "code_url": "https://gist.github.com/mageirakos/8b470edb28a39b392943bcabf999d315",
          "description": "<p>The aim of the project is to use Natural Language Processing (NLP) to develop an intelligent bot able to provide satisfying answers to users and handle support requests up to a certain level of complexity, forwarding only the remaining ones to the experts.</p>\n<p>For this purpose, the support requests sent as emails to the support team need to be downloaded through IMAP client and then parsed and transformed into a text corpus for further analysis.</p>\n<p>Addional data sources besides support emails will be used as input such as Rucio documentation pages and GitHub issues.</p>\n<p>Python 3.x will be the programming language used, preferably with NLTK library for the implementation of text mining algorithms.</p>\n<p>The key deliverables in the final codebase include fetcing and parsing of the input data, data storage, as well as machine learning and information retrieval and extraction methods. All applied to the task of supporting Rucio User requests.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2020_028",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5966219707940864/",
          "proposal_id": null,
          "short_description": "The aim of the project is to use Natural Language Processing (NLP) to develop an intelligent bot able to provide satisfying answers to users and...",
          "slug": "support-for-rucio-users-with-natural-language-processing",
          "status": "completed",
          "student_name": "Vasilis Mageirakos",
          "student_profile": null,
          "tags": [
            "python",
            "ai"
          ],
          "title": "Support for Rucio Users with Natural Language Processing"
        },
        {
          "code_url": "https://medium.com/@animesh.leo/google-summer-of-code-2020-finale-4557564b9996",
          "description": "<p>In preparation of LHC's next run, CERN has been developing JAliEn, a successor to their current grid computing middleware, AliEn. To ease development and give individual developers a robust local development framework with all necessary capabilities, the core of the JAliEn system (JCentral) must be deployed individually, on a single host, with all necessary configurations. Therefore the project aims to develop a command line utility for JAliEn developers to quickly start, develop and teardown JCentral replicas locally.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_029",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5984973615529984/",
          "proposal_id": null,
          "short_description": "In preparation of LHC's next run, CERN has been developing JAliEn, a successor to their current grid computing middleware, AliEn. To ease development...",
          "slug": "local-replica-of-jalien-central-services-for-research-and-development",
          "status": "completed",
          "student_name": "drholmie",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Local replica of JAliEn central services for research and development"
        },
        {
          "code_url": "https://obviy.us/blog/gsoc-part-2/",
          "description": "<p>The data-driven workflow dumps generated by the DPL (O2 Data Processing Layer) cannot be directly imported into AliECS. These workflow templates are JSON files describing a set of processes to run and how they communicate and are thus, essential. The same input workflow template does not follow a formal schema.</p>\n<p>This project aims to develop an importer that converts a DPL workflow dump and convert it into a format that AliECS can work with. The importer would further benefit from a formally defined schema as well as building a package for validation of files against said schema.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_030",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6311694998962176/",
          "proposal_id": null,
          "short_description": "The data-driven workflow dumps generated by the DPL (O2 Data Processing Layer) cannot be directly imported into AliECS. These workflow templates are...",
          "slug": "workflow-configuration-import-and-validation-for-aliecs",
          "status": "completed",
          "student_name": "Ayaan Zaidi",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Workflow configuration import and validation for AliECS"
        },
        {
          "code_url": "https://didithilmy.github.io/gsoc2020/",
          "description": "<p>CERN and the high energy physics community stores their data in various storage, with varying locations and medium (from object storage to magnetic tapes). To move their large data around, CERN uses a service known as Rucio. In order for scientists to be able to perform analyses on Rucio-managed data using SWAN, it must be made accessible from within the notebook. This could be done in several ways, one of which is telling Rucio to replicate the requested data to a specific storage location. To do this, scientists must use Rucio CLI or web-based UI, which could be somewhat overwhelming for some. In addition to that, they must know which storage location the data must be replicated to, and the file path to access the data once replicated. Also, the path can change after sometime, forcing them to go through the process again.</p>\n<p>To address the issue, this project aims to develop a JupyterLab extension that integrates with Rucio. It takes care of creating a replication rule and providing an easy way of getting the file path. If the data becomes unavailable, the extension can make it available again automatically.</p>\n",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2020_031",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6320550214893568/",
          "proposal_id": null,
          "short_description": "CERN and the high energy physics community stores their data in various storage, with varying locations and medium (from object storage to magnetic...",
          "slug": "integration-of-rucio-in-jupyterlab-for-swan",
          "status": "completed",
          "student_name": "Muhammad Aditya Hilmy",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "Integration of Rucio in JupyterLab for SWAN"
        },
        {
          "code_url": "https://shra1-25.github.io/E2eDLrecReport/",
          "description": "<p>Developing End-to-End Deep Learning Models and optimizing them for the Reconstruction of single particles, jets and event topologies of interest in collision during collision experiment. This involves classifying electromagnetic showers of the particles developed during the experiment and their event classification. The code will be further integrated with the CMSSW inference engine. The inference of the model will be integrated with the CMSSW Particle Flow (PF) classes and will be tested on GPUs for faster Computation. This proposal shows the proposed methodology and model(which may be improvised while working on it depending on the performance and new approaches may be used) for the reconstruction for CMS experiment. It also includes timeline and some basic concepts of Physics to familiarize with the CMS experiment at Large Hadron Collider (LHC).</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_032",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5187650517991424/",
          "proposal_id": null,
          "short_description": "Developing End-to-End Deep Learning Models and optimizing them for the Reconstruction of single particles, jets and event topologies of interest in...",
          "slug": "end-to-end-deep-learning-reconstruction-for-cms-experiment",
          "status": "completed",
          "student_name": "Shravan Chaudhari-1",
          "student_profile": null,
          "tags": [],
          "title": "End-to-End Deep Learning Reconstruction for CMS Experiment"
        },
        {
          "code_url": "https://gist.github.com/LovelyBuggies/390fef467f319aefdcf95705390d7f78",
          "description": "<p>The <a href=\"https://scikit-hep.org/\" target=\"_blank\">Scikit-HEP</a> project is a collection of several dozen packages intended to facilitate the use of Python in High Energy Physics. One of the major fronts of development is in histogramming; a majority of HEP analysis is heavily reliant on histograms. To this end, a new Python package was introduced for histogramming in Scikit-HEP, <a href=\"https://boost-histogram.readthedocs.io/\" target=\"_blank\">boost-histogram</a>. This package is intended to be a core package for histogramming with no dependencies. In this summer, we are preparing to implement an analysis-friendly package, called “<a href=\"https://github.com/scikit-hep/hist\" target=\"_blank\">hist</a>”, built on top of boost-histogram and providing commonly expected histogramming features. This package will allow a wider range of dependencies and features not admissible in the core package.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_033",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5208113386356736/",
          "proposal_id": null,
          "short_description": "The Scikit-HEP project is a collection of several dozen packages intended to facilitate the use of Python in High Energy Physics. One of the major...",
          "slug": "hist-histogramming-for-analysis-powered-by-boost-histogram",
          "status": "completed",
          "student_name": "Shuo Liu",
          "student_profile": null,
          "tags": [
            "python",
            "ui"
          ],
          "title": "Hist: histogramming for analysis powered by boost-histogram"
        },
        {
          "code_url": "https://drive.google.com/file/d/159QRCM8-c3FUy-y6c0-SHkgtbNJNr9H8/view?usp=sharing",
          "description": "<p>Storage is one of the main limiting factors to the recording of information from proton-proton collision events at the Large Hadron Collider at CERN. This project aims to implement an autoencoder based deep-compression algorithm on an ATLAS trigger machine to reduce this storage requirement and improve accessibility time of the collision data captured by a trigger system. We also plan to design an autoencoder model that has a better compression factor and adequate execution time and memory requirements in order to be deployed on a trigger system.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2020_034",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5208521374695424/",
          "proposal_id": null,
          "short_description": "Storage is one of the main limiting factors to the recording of information from proton-proton collision events at the Large Hadron Collider at CERN....",
          "slug": "deep-compression-for-hep-data",
          "status": "completed",
          "student_name": "Honey Gupta",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Deep-compression for HEP data"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2020/organizations/5668658770083840/"
    },
    "year_2021": {
      "num_projects": 25,
      "projects": [
        {
          "code_url": "https://drive.google.com/file/d/1TxD_QDafASffrOeSY7TD_LKyscx4VPGj/view?usp=sharing",
          "description": "<p>The goal of the project is to develop and test a suite of ready-to-run benchmarks to measure the performances of various partitioning options on industry grade GPUs while performing the baseline performance measurements that are necessary to test the solutions developed within OpenForBC.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6334460873670656/",
          "proposal_id": null,
          "short_description": "The goal of the project is to develop and test a suite of ready-to-run benchmarks to measure the performances of various partitioning options on...",
          "slug": "openforbc",
          "status": "completed",
          "student_name": "Aneesh Chawla",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "OpenForBC"
        },
        {
          "code_url": "https://gitlab.cern.ch/hflav/b2charm-webpage/-/blob/master/README.md",
          "description": "<p>The Heavy Flavour AVeraging (HFLAV) group is responsible for collecting and combining measurements made at different High Energy Physics (HEP) experiments, at CERN and other particle physics laboratories, and combining them using robust statistical procedures.</p>\n<p>The HFLAV website provides a live snapshot of the latest data obtained from these latest publications. This project aims to improve the experience of users using the HFLAV website. These averages could be of greater value to the particle physics community if the accessibility, interactivity and visibility of the current website can be improved. Thus this project seeks to transform user interaction with HFLAV’s ‘Beauty to Charm’ averages.</p>\n<p>The plan to increase the accessibility is by redesigning the whole user interface, making it easier to find a relevant data point ( branching fraction) by the use of filters. As the old website is static, thus in order to make the site responsive to the user's input, the website will be refactored and the framework will be changed to Django. The application would be hosted using CERN's OpenShift service instead of the older EOS filesystem it was using.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6410808212324352/",
          "proposal_id": null,
          "short_description": "The Heavy Flavour AVeraging (HFLAV) group is responsible for collecting and combining measurements made at different High Energy Physics (HEP)...",
          "slug": "add-support-for-in-browser-interactive-averaging-of-physics-results",
          "status": "completed",
          "student_name": "Harsh Prakash Gupta",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "Add support for in-browser interactive averaging of physics results"
        },
        {
          "code_url": "https://docs.google.com/document/d/132ibaH56hYjgkCIQZWKsCo89CIPSnyk_w4F1Ge8oyFo/edit?usp=sharing",
          "description": "<p>This project is about adding optimised and accurate high-order interpolators to the  LHAPDF C++ library. This is required for adding support for Chebyshev polynomials, which are a very efficient way to parametrise Parton Density functions. Parametrising PDFs is essential to studying the high energy collision experiments that happen between protons in the LHC, as they encode details on the internal structure of a proton.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6578748882681856/",
          "proposal_id": null,
          "short_description": "This project is about adding optimised and accurate high-order interpolators to the  LHAPDF C++ library. This is required for adding support for...",
          "slug": "accuracy-and-parallel-computation-in-parton-density-calculation",
          "status": "completed",
          "student_name": "Tushar Jain-2",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Accuracy and parallel computation in parton density calculation"
        },
        {
          "code_url": "https://gist.github.com/grimmmyshini/8226a8eeda5dfe0cf0a3ed2b845bf87e",
          "description": "<p>In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Automatic differentiation is an alternative technique to Symbolic differentiation and Numerical differentiation (the method of finite differences). Clad is based on Clang which provides the necessary facilities for code transformation. The AD library can differentiate non-trivial functions, find a partial derivative for trivial cases, and has good unit test coverage. In several cases, due to different limitations, it is either inefficient or impossible to differentiate a function, as such instead of issuing an error Clad should fall back to its future numerical differentiation abilities.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6621494813130752/",
          "proposal_id": null,
          "short_description": "In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function...",
          "slug": "add-numerical-differentiation-support-in-clad",
          "status": "completed",
          "student_name": "grimmmyshini",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "Add numerical differentiation support in Clad"
        },
        {
          "code_url": "https://gist.github.com/gurudattapatil/a71155c48faa2d06032b12b7d67a79f2",
          "description": "<p>Ganga is a tool to run data analysis jobs along with managing associated data files easier. Ganga also allows for submission, bookkeeping,specification, and post processing in computational task management. In today's world where computation and bookkeeping is a monumental task, the requirement for a task-management tool is important. Ganga GUI comes into picture in this field with added resources of Ganga as well as the ease to use it with the help of its GUI. Improvement in the functionality as well as improvements in the visual aspects of GUI are important.  This will improve adaptability, reliability and ease to use. The GUI will also help the community by attracting new users, because of the easy to use GUI interface.</p>\n",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2021_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5672776735653888/",
          "proposal_id": null,
          "short_description": "Ganga is a tool to run data analysis jobs along with managing associated data files easier. Ganga also allows for submission,...",
          "slug": "upgrading-the-ganga-graphical-user-interface",
          "status": "completed",
          "student_name": "Gurudatta Patil",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "Upgrading the Ganga Graphical User Interface"
        },
        {
          "code_url": "https://docs.google.com/document/d/15BVInw-GI1uFrm-21aPmkw87dHq27_GXVpqmEtJUovE/edit?usp=sharing",
          "description": "<p>The project aims to implement single-precision floating-point support for GPU acceleration in the VecGeom library. This will be achieved by first analyzing and locating the current errors, then fixing them for the most general cases. The process will be done for all primitive solids, starting with the most frequently used, and then for the global navigation algorithms.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5720904595668992/",
          "proposal_id": null,
          "short_description": "The project aims to implement single-precision floating-point support for GPU acceleration in the VecGeom library. This will be achieved by first...",
          "slug": "single-precision-support-for-gpu-acceleration-in-vecgeom",
          "status": "completed",
          "student_name": "Martin Kostelník",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Single precision support for GPU acceleration in VecGeom"
        },
        {
          "code_url": "https://anuragakella.github.io/gsoc21/",
          "description": "<p>The supercomputer that is currently constructed at the LHCb will allegedly be the largest real-time data acquisition system in the world in 2021 and operates at very high speeds (up to 32 Tb/s). To avoid losing critical physics data through the network and to ascertain its high-speed real-time operation, the network has to be monitored carefully.</p>\n<p>This project aims to design and build a GUI application that can visualize the DAQ network, the routing configuration in the switches, and conflicts with relevant details in the network for a given input traffic pattern. The end product can be used to monitor the high-speed network in real-time.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5735048157855744/",
          "proposal_id": null,
          "short_description": "The supercomputer that is currently constructed at the LHCb will allegedly be the largest real-time data acquisition system in the world in 2021 and...",
          "slug": "implementing-an-application-for-visualizing-the-lhcb-daq-network",
          "status": "completed",
          "student_name": "Anurag Akella",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Implementing an application for visualizing the LHCb DAQ network"
        },
        {
          "code_url": "https://github.com/antoniopetre/pixeltrack-standalone/blob/gsoc/gsoc-documentation.md",
          "description": "<p>The Compact Muon Solenoid (CMS) experiment is one of the largest experiments at Large Hadron Collider (LHC) that has been built to search for new physics. CMS Software (CMSSW) is the framework utilized by the CMS experiment for Data Acquisition, Trigger and Event Reconstruction. The future upgrade of the LHC will add new challenges for CMS detector due to the larger amount of data that will be produced. \nOne solution is to look towards a heterogeneous High-Level Trigger (HLT) Computing Farm, where the computing load can be distributed among different hardware (CPU, GPU, FPGAS). This project aims to use Portability Libraries like Alpaka to write code for the CMS Software (CMSSW), in order to have a single implementation which will be compiled on different architectures. \nIn the last years, a new pixel track reconstruction that can run on GPUs (CUDA) has been implemented. The goals are to port the current CUDA implementation with Alpaka, in order to understand if this library is feasible for CMSSW and to compare the timing performances between Alpaka implementation and native CUDA.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5472555158208512/",
          "proposal_id": null,
          "short_description": "The Compact Muon Solenoid (CMS) experiment is one of the largest experiments at Large Hadron Collider (LHC) that has been built to search for new...",
          "slug": "portability-for-the-patatrack-pixel-track-reconstruction-with-alpaka",
          "status": "completed",
          "student_name": "Adrian-Antonio Petre",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Portability for the Patatrack Pixel Track Reconstruction with Alpaka"
        },
        {
          "code_url": "https://gist.github.com/parth-07/a51a32bfa8b435f41e915b040149f7cd",
          "description": "<p>This proposal aims to add support for directly differentiating functors and lambda\nexpressions and increase clad coverage and overcome its shortcomings by fixing various\nexisting issues and adding support for various currently unsupported syntax.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5495594906288128/",
          "proposal_id": null,
          "short_description": "This proposal aims to add support for directly differentiating functors and lambda\nexpressions and increase clad coverage and overcome its...",
          "slug": "add-support-for-functor-objects-in-clad",
          "status": "completed",
          "student_name": "parth-07",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Add support for functor objects in clad"
        },
        {
          "code_url": "https://github.com/Autoencoders-compression-anomaly/Deep-Autoencoders-Data-Compression-GSoC-2021",
          "description": "<p>Storage is one of the main limiting factors to the recording of information from proton-proton collision events at the Large Hadron Collider (LHC), at CERN in Geneva. Hence, the ATLAS experiment at the LHC uses a trigger system, which selects and transfers interesting events to the data storage system. However, if those events are buried in large backgrounds and difficult to identify as a signal by the trigger system, they will also be discarded together with the background. To alleviate this problem, various compression algorithms are proposed to reduce the size of the data that is recorded. One of those algorithms is an autoencoder (AE) network that tries to implement an approximation to the identity, f(x) = x. Its goal is to create a lower-dimensional representation of the input data in a latent space. Then using this latent representation the model can reconstruct its input. Moreover, two advanced families of AEs are proposed, the Variational and the Adversarial AE. Next, various combinations of different compression algorithms are tested to compress and construct accurate representations of the input data. Finally, the studied AEs are implemented for anomaly detection.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2021_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5047857987452928/",
          "proposal_id": null,
          "short_description": "Storage is one of the main limiting factors to the recording of information from proton-proton collision events at the Large Hadron Collider (LHC),...",
          "slug": "deep-autoencoders-for-atlas-data-compression",
          "status": "completed",
          "student_name": "George Dialektakis",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Deep autoencoders for ATLAS data compression"
        },
        {
          "code_url": "https://simonthor.github.io/GSoC-2021/",
          "description": "<p>Rivet is a tool used by particle physicists to test and develop Monte Carlo event generators, i.e., simulation of particle accelerator collisions. Simulations are crucial for particle physics research, since they can be used to compare theoretical predictions with experimental measurements to discover new physics.</p>\n<p>Visualizing the output is a fundamental part of Rivet, as a good visualization makes it intuitive and easy to understand the results. Currently, plots are created by generating and compiling <code>LaTeX</code> commands. While this method does create high-quality plots, it can be slow, have memory issues, be sensitive to the deployment platform.</p>\n<p>The goal of this project is to solve the above mentioned issues by replacing the <code>LaTeX</code> plotting backend with a new high-level API that can support multiple rendering backends. The API should make it easy to export high-quality plots, while still allowing full customization for advanced users. <code>Matplotlib</code> will become the first rendering backend that will be added to this API, and options for exporting figures into interactive plots in the browser will also be investigated.</p>\n",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2021_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5279618952593408/",
          "proposal_id": null,
          "short_description": "Rivet is a tool used by particle physicists to test and develop Monte Carlo event generators, i.e., simulation of particle accelerator collisions....",
          "slug": "enhancing-the-plotting-functionality-in-mcnetrivet",
          "status": "completed",
          "student_name": "Simon Thor",
          "student_profile": null,
          "tags": [
            "api",
            "ui",
            "backend"
          ],
          "title": "Enhancing the Plotting Functionality in MCnet/Rivet"
        },
        {
          "code_url": "https://gist.github.com/quantum-shift/39d10b22f7e6d8980cb7607a40bc8ac1",
          "description": "<p>PRocess MONitor (PRMON) is a utility programme used by the Worldwide LHC Computing Grid (WLCG) to monitor the resource consumption and performance of millions of jobs run by the ATLAS experiment in particular. The output generated is then used to detect anomalies in individual jobs or task groups. As the functionality of PRMON is extended, the complexity of errors that need to be tested and handled increases. Further, it is presently designed to read the data of processes while they are running. This project aims to implement a logging framework for dynamic and configurable logging. Further, the functionality of PRMON will be extended to read data from preprocessed sources. Each monitor will be adapted to read optionally from these precooked sources. The unit test infrastructure will be developed for testing against complex and unusual errors. Precooked sources will be generated to serve as unit tests consisting of these unusual cases.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2021_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5312835692789760/",
          "proposal_id": null,
          "short_description": "PRocess MONitor (PRMON) is a utility programme used by the Worldwide LHC Computing Grid (WLCG) to monitor the resource consumption and performance of...",
          "slug": "logging-unit-test-infrastructure-and-precooked-sources-for-prmon",
          "status": "completed",
          "student_name": "Anubhab Das",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Logging, Unit Test Infrastructure and Precooked Sources for PRMON"
        },
        {
          "code_url": "https://rohittopi.github.io/gsoc/gsoc21-project-report",
          "description": "<p>CernVM-FS (CVMFS) is a service for fast and reliable software distribution on a global scale. Data is organized in repositories. Files and metadata are downloaded on-demand by means of HTTP requests and take advantage of several layers of caches. The current per-file granularity of distribution and caching leads to poor performance especially in case of cold caches. In particular, loading of interdependent files and libraries required for certain applications can be improved. The goal of this project is to introduce the concept of bundles, which would improve the startup performance of applications.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5539477996437504/",
          "proposal_id": null,
          "short_description": "CernVM-FS (CVMFS) is a service for fast and reliable software distribution on a global scale. Data is organized in repositories. Files and metadata...",
          "slug": "cernvm-fs-preload-capability",
          "status": "completed",
          "student_name": "Rohit Topi",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "CernVM-FS preload capability"
        },
        {
          "code_url": "https://docs.google.com/document/d/1qtoglXoyIfsngJVpidsTegpQlybpxd6mFH6doP4myLo/edit?usp=sharing",
          "description": "<p>Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in many particle physics data analysis and applications. Currently, we are developing a fast inference system in TMVA that takes the ONNX model as input and produces compilation-ready standalone C++ scripts as output. These scripts will then provide users an easy way to deploy their deep learning models in their physics software and analysis frameworks.</p>\n",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2021_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5625061863587840/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in...",
          "slug": "tmva-deep-learning-developments-inference-code-generation-for-batch-normalization",
          "status": "completed",
          "student_name": "Aaradhya Saxena",
          "student_profile": null,
          "tags": [],
          "title": "TMVA Deep Learning Developments - Inference Code Generation for Batch Normalization"
        },
        {
          "code_url": "https://gist.github.com/sudo-panda/5d5416c05821d1d47b85e4c3c6f5ca46",
          "description": "<p>In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Automatic differentiation is an alternative technique to Symbolic differentiation and Numerical differentiation (the method of finite differences). Clad is based on Clang which provides the necessary facilities for code transformation.</p>\n<p>ROOT is a framework for data processing, born at CERN, at the heart of the research on high-energy physics. Every day, thousands of physicists use ROOT applications to analyze their data or to perform simulations. ROOT has a clang-based C++ interpreter Cling and integrates with Clad to enable flexible automatic differentiation facility.</p>\n<p><code>TFormula</code> is a ROOT class which bridges compiled and interpreted code.</p>\n<p>The project aims to add second order derivative support in <code>TFormula</code> using <code>clad::hessian</code>. The PR that added support for gradients in ROOT is taken as a reference and can be accessed <a href=\"https://github.com/root-project/root/pull/2745\" target=\"_blank\">here</a>.</p>\n<p>Optionally, if time permits, this project will also convert the gradient function into CUDA device kernels.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5629838773190656/",
          "proposal_id": null,
          "short_description": "In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function...",
          "slug": "utilize-second-order-derivatives-from-clad-in-root",
          "status": "completed",
          "student_name": "Baidyanath Kundu",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Utilize second order derivatives from Clad in ROOT"
        },
        {
          "code_url": "https://gist.github.com/nahimilega/80d678df491745e0fdb0e4b4e2456430",
          "description": "<p>This project aims to address obsolescence in the underlying RooUnfold implementation that would allow the RooUnfold package to act as a lightweight member class to ROOT whilst retaining its functionality and the ability for users to define new algorithms including machine learning based solutions, and data intensive tasks.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6236887538204672/",
          "proposal_id": null,
          "short_description": "This project aims to address obsolescence in the underlying RooUnfold implementation that would allow the RooUnfold package to act as a lightweight...",
          "slug": "roounfold-efficient-deconvolution-using-state-of-the-art-algorithms",
          "status": "completed",
          "student_name": "Archit_Agrawal",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "RooUnfold - Efficient deconvolution using state of the art algorithms"
        },
        {
          "code_url": "https://gist.github.com/rak108/30ddf7396995312bb662e8cd8108a46b",
          "description": "<p>Rucio is an open-source software framework that provides functionality to\nscientific collaborations to organize, manage, monitor, and access their distributed\ndata and dataflows across heterogeneous infrastructures. Rucio was originally\ndeveloped to meet the requirements of the high-energy physics experiment ATLAS\nand is continuously enhanced to support diverse scientific communities.\nThis project seeks to enhance Rucio clients primarily by enabling the availability of different\ntransfer tools to make them easier to use in heterogeneous environments. This project aims to implement protocol support for SSH, rsync, and rclone, along with the ability to choose the optimal transfer protocol based upon the local and remote configuration, unless specifically mentioned otherwise.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6541159261798400/",
          "proposal_id": null,
          "short_description": "Rucio is an open-source software framework that provides functionality to\nscientific collaborations to organize, manage, monitor, and access their...",
          "slug": "new-protocols-for-exascale-data-management-with-rucio",
          "status": "completed",
          "student_name": "Rakshita Varadarajan",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "New protocols for exascale data management with Rucio"
        },
        {
          "code_url": "https://gist.github.com/monalisha31/42f3688fcfa8496bd3d24420c2229b3d",
          "description": "<p>Ganga is an open source job management tool for processing and bookkeeping of intensive computational tasks on a wide set of distributed resources. It effectively provides a homogeneous environment for processing data on inhomogeneous resources. It provides a simple way of preparing, organising and executing jobs on different computing infrastructures. Currently, Ganga can be accessed using an iPython prompt , file based scripting or a webGUI.  GUI adds a great advantage for all the first time users of Ganga . Furthermore , the core developers and scientists can also use the GUI to get the thorough analysis of their tasks and perform their tasks with ease. The aim here is to upgrade the current GUI with new and improved functionalities. Implementation of queues, automation of connection to remote machine , interactive virtual IDE for the users, setting up the terminal inside the GUI and improving the visual aspects of the Ganga GUI are the main objectives. The major challenge of this project is to make the Ganga GUI more efficient, interactive and attractive.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6567046975848448/",
          "proposal_id": null,
          "short_description": "Ganga is an open source job management tool for processing and bookkeeping of intensive computational tasks on a wide set of distributed resources....",
          "slug": "upgrading-the-ganga-graphical-user-interface",
          "status": "completed",
          "student_name": "Monalisha Ojha",
          "student_profile": null,
          "tags": [
            "python",
            "web",
            "ai",
            "ui"
          ],
          "title": "Upgrading the Ganga graphical user interface"
        },
        {
          "code_url": "https://gist.github.com/axmat/fb100f10fc8d8e27a5e59706befd062b",
          "description": "<p>Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in many particle physics data analysis and applications. A fast inference system that takes takes ONNX model as input and produces compilation-ready standalone C++ scripts as output is under development in TMVA.\nThis project will focus on the development of ONNX RNN, LSTM and GRU operators in the code generation format.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6054106212335616/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in...",
          "slug": "inference-code-generation-for-recurrent-neural-networks",
          "status": "completed",
          "student_name": "Ahmat Hamdan",
          "student_profile": null,
          "tags": [],
          "title": "Inference Code Generation for Recurrent Neural Networks"
        },
        {
          "code_url": "https://github.com/sanjibansg/GSoC21-RootStorage/wiki",
          "description": "<p>The Toolkit for Multivariate Data Analysis with ROOT (TMVA) provides a machine learning environment for the processing and evaluation of multivariate classification, both binary and multi-class, and regression techniques targeting applications in high-energy physics. The latest development in TMVA is a fast inference system, with its own intermediate representation of the deep learning model compliant with the ONNX standard. With this new inference system, TMVA aims to provide convenience for users in High Energy Physics in the deployment of their Deep Learning models in a production environment. To facilitate the usage, storage and exchange of these models,this project aims to develop the storage functionality of Deep Learning models in the ROOT format, popular in the High Energy Physics community.</p>\n",
          "difficulty": "medium",
          "id": "proj_cern-hsf_2021_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6076072721907712/",
          "proposal_id": null,
          "short_description": "The Toolkit for Multivariate Data Analysis with ROOT (TMVA) provides a machine learning environment for the processing and evaluation of multivariate...",
          "slug": "root-storage-of-deep-learning-models-in-tmva",
          "status": "completed",
          "student_name": "Sanjiban Sengupta",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "ROOT Storage of Deep Learning models in TMVA"
        },
        {
          "code_url": "https://github.com/Aman027/GSOC-21-Report",
          "description": "<p>Zfit is a highly scalable and customizable model manipulation and fitting library. Using Tensorflow as its backend, it has been optimised for simple and direct manipulation of probability density functions with usage target High energy physics analysis ecosystem. The main focus is on scalability, parallelisation and a user friendly experience framework (no cython, no C++ needed to extend).\nZfit currently misses library functions which are especially useful in physics. My aim is to implement a new library function named Faddeeva function or Kramp function that can simplify the process of calculation of scaled complex complementary error function using low level functionalities of TensorFlow. This function arises in various physical problems, typically relating to electromagnetic responses in complicated media.The purpose of this new library function is to cater to the needs of scientists and researchers with minimal knowledge of low-level implementation of the function with high performance and accuracy.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2021_021",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/4841753579880448/",
          "proposal_id": null,
          "short_description": "Zfit is a highly scalable and customizable model manipulation and fitting library. Using Tensorflow as its backend, it has been optimised for simple...",
          "slug": "implementation-of-physical-shape-function",
          "status": "completed",
          "student_name": "Aman  Verma",
          "student_profile": null,
          "tags": [
            "ai",
            "backend"
          ],
          "title": "Implementation of Physical Shape Function"
        },
        {
          "code_url": "https://gist.github.com/dynamic-entropy/ce664532ab28f8a13b7cf8f68e2d0f2a",
          "description": "<p>In recent years Sync and Share Services have become more relevant than ever in our day to day lives. The scientists at CERN  are not an exception to this, however, due to the sensitivity of data they are wary of using commercial platforms.  To fulfil this requirement the CERN IT-ST team introduced CERNBox, which allows scientists to collaborate and use their favourite tools in their workflows with ease. This project extends the capabilities of CERNBox by making large data stores readily available within CERNBox. Rucio has decades of experience in scientific data management for various experiments at CERN. Adding CernBox as one of its storage systems will allow users to efficiently share very large datasets across cloud instances and use that data in web-based notebooks making data analysis and experiments reproducible.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_022",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/4859720166277120/",
          "proposal_id": null,
          "short_description": "In recent years Sync and Share Services have become more relevant than ever in our day to day lives. The scientists at CERN  are not an exception to...",
          "slug": "rucio-and-cs3api-to-enable-data-management-for-the-sciencemesh-cloud",
          "status": "completed",
          "student_name": "Rahul Chauhan-1",
          "student_profile": null,
          "tags": [
            "web",
            "api",
            "ai",
            "cloud",
            "ui"
          ],
          "title": "Rucio and CS3API to enable data management for the ScienceMesh cloud"
        },
        {
          "code_url": "https://github.com/Harshalzzzzzzz/Final_Report_GSoC-21",
          "description": "<p>RooFit is a C++ library for statistical data analysis and for modeling the expected distribution of observables measured in particle physics experiments. The pythonization work proposed here is vital to facilitate the transition from C++ to python to write analysis code.  This project aims to simplify complex workflows and enhancement of the python interface, greatly reducing the amount of code that has to be written.</p>\n",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2021_023",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6264092129492992/",
          "proposal_id": null,
          "short_description": "RooFit is a C++ library for statistical data analysis and for modeling the expected distribution of observables measured in particle physics...",
          "slug": "roofit-development-intuitive-python-bindings-for-roofit",
          "status": "completed",
          "student_name": "Harshal Anil Shende",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "RooFit Development - Intuitive Python bindings for RooFit"
        },
        {
          "code_url": "https://matthewfilipovich.com/GSoC-2021-CERN/",
          "description": "<p>The Rivet toolkit is a software package for performing data analysis on simulated particle collision events like those in the Large Hadron Collider (LHC). Rivet covers all aspects of collider physics and is the LHC's principle tool for model testing using events simulated with Monte Carlo methods. The current framework for visualizing experimental datasets can be slow and error-prone, and our proposed project will focus on designing and implementing a replacement for this visualization system based on modern rendering technologies while preserving high quality outputs. The goal of this project is to create an intuitive Python plotting package that complements and incorporates a single coherent treatment of the existing classes within the Rivet toolkit. The new visualization architecture will focus on finding a balance between power and simplicity in the user interface, enabling researchers to quickly share their results in beautiful and easily accessible formats. The ability to design interactive web-based visualizations will allow researchers to create engaging and high-quality educational resources for students and individuals interested in physics around the world.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_024",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5139248516169728/",
          "proposal_id": null,
          "short_description": "The Rivet toolkit is a software package for performing data analysis on simulated particle collision events like those in the Large Hadron Collider...",
          "slug": "modern-plotting-machinery-for-the-large-hadron-colliders-monte-carlo-event-analysis-tool",
          "status": "completed",
          "student_name": "Matthew Filipovich",
          "student_profile": null,
          "tags": [
            "python",
            "web",
            "ui"
          ],
          "title": "Modern Plotting Machinery for the Large Hadron Collider's Monte Carlo Event Analysis Tool"
        },
        {
          "code_url": "https://github.com/jimil749/GSoC-Report",
          "description": "<p>This project aims to add runtime pluggability to the Reva framework to enhance developer experience at AARNet, ownCloud and CERN. This would involve:</p>\n<ol>\n<li>Studying and Researching various golang plugin frameworks.</li>\n<li>Migrating the existing build-time plugins to runtime model using the selected framework.</li>\n<li>Implementing Hot-Reloading of go-plugins for fast feedback loop.</li>\n</ol>\n<p>This project would help developers at oCIS/CERN to add plugin drivers to Reva in a much faster and hassle-free manner.</p>\n",
          "difficulty": null,
          "id": "proj_cern-hsf_2021_025",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5151620706336768/",
          "proposal_id": null,
          "short_description": "This project aims to add runtime pluggability to the Reva framework to enhance developer experience at AARNet, ownCloud and CERN. This would involve:...",
          "slug": "runtime-plugin-ecosystem-support-for-ocis",
          "status": "completed",
          "student_name": "Jimil Desai",
          "student_profile": null,
          "tags": [
            "ai",
            "cloud",
            "ui"
          ],
          "title": "Runtime plugin ecosystem support for OCIS"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2021/organizations/6240588592054272/"
    },
    "year_2022": {
      "num_projects": 21,
      "projects": [
        {
          "code_url": "https://github.com/scikit-hep/uproot5/pulls?q=+is%3Apr+author%3Akkothari2001+",
          "description": "This project aims to create an API for users that can provide the data in ROOT files\ndirectly in a “delayed” form that is supported by Dask. It will reimplement the\nuproot.lazy function which will now be called uproot.dask. This function will support all\nof the Dask backends, leveraging the dask.array, dask.dataframe and dask-awkward\ndelayed types in Dask.\n\nUproot is expanding to include Dask for smooth integration with other common\ndata processing libraries. This project is a major revamp of the structure and\ncodebase of Uproot and the changes will also include updating Uproot to use\nAwkward Array v2. This will result in a new major version of Uproot i.e Uproot v5.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/JF7I75hH/",
          "proposal_id": null,
          "short_description": "This project aims to create an API for users that can provide the data in ROOT files directly in a “delayed” form that is supported by Dask. It will...",
          "slug": "iris-hep-uproot-dask",
          "status": "completed",
          "student_name": "Kush Kothari",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "backend"
          ],
          "title": "[IRIS-HEP] Uproot + Dask"
        },
        {
          "code_url": "https://github.com/luozf14/TTreeToRNTuple",
          "description": "TTree is the ROOT’s legacy columnar storage that has been used to store more than 1 exabyte of high-energy physics data during the last 25 years. RNTuple classes provide ROOT’s new, experimental I/O subsystem for HEP data. This project will consist of the implementation of an automatic conversion tool that migrates both the schema (i.e. fields and their types) and the user data. \n\nThis project consists of four milestones 1) parse the schema of a TTree; 2) convert TTree with simple variables; 3) convert TTree with STL container; 4) convert TTree with user-defined class.\n\nThere are three deliverables 1) a parser that automatically parses the schema of an input TTree; 2) a TTree-to-RNTuple converter; 3) proper documentation and tests.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/yhvtRC8Q/",
          "proposal_id": null,
          "short_description": "TTree is the ROOT’s legacy columnar storage that has been used to store more than 1 exabyte of high-energy physics data during the last 25 years....",
          "slug": "root-automatic-conversion-of-data-stored-in-ttree-form-to-rntuple",
          "status": "completed",
          "student_name": "Zephyr",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "ROOT - Automatic conversion of data stored in TTree form to RNTuple"
        },
        {
          "code_url": "https://chamodya-ka.github.io/blog/GSOC-2022/",
          "description": "Traccc is a demonstrator for GPU tracking algorithms. This project focuses on optimizing GPU utilization of traccc using CUDA Multi Process Service (MPS) and improving compute times by introducing caching allocators, moreover if time permits, using CUDA-MIG to improve GPU utilization as well. Upon completion a report comparing the benchmarking results between the CUDA implementation using MPS and multi-process CPU implementation will be submitted.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/SEravaYf/",
          "proposal_id": null,
          "short_description": "Traccc is a demonstrator for GPU tracking algorithms. This project focuses on optimizing GPU utilization of traccc using CUDA Multi Process Service...",
          "slug": "acts-gpu-rd-optimization-of-gpu-tracking-pipeline",
          "status": "completed",
          "student_name": "Kavishka Attanayake",
          "student_profile": null,
          "tags": [],
          "title": "Acts GPU R&D - Optimization of GPU tracking pipeline"
        },
        {
          "code_url": "https://github.com/SimonB00/clue",
          "description": "The CLUE algorithm is a clustering algorithm written by Rovere et al. in C++, which is used in High-Energy Physics to reconstruct particle showers by connecting hits detected by a calorimeter.\nThis project aims to generalize the clustering CLUE algorithm to N-dimensions, by introducing a new set of coordinates and a new metric, bind it with Python using the Cython or PyBind11 tools and interface it with Pandas DataFrames and NumPy data structures, and finally to test its performance and compare it with other clustering algorithms'.\nThis project will take approximately 350 hours, from mid June to mid September.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/h8Np6Hjm/",
          "proposal_id": null,
          "short_description": "The CLUE algorithm is a clustering algorithm written by Rovere et al. in C++, which is used in High-Energy Physics to reconstruct particle showers by...",
          "slug": "implementation-of-a-python-library-that-generalizes-the-clue-clustering-algorithm",
          "status": "completed",
          "student_name": "SimoneBalducci",
          "student_profile": null,
          "tags": [
            "python",
            "ai"
          ],
          "title": "Implementation of a Python library that generalizes the CLUE clustering algorithm"
        },
        {
          "code_url": "https://gist.github.com/Neel-Shah-29/ea1efde4dd5a51522f4aeb77b7e1fe12",
          "description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in many particle physics data analysis and applications. Currently, we are developing a fast inference system in TMVA, called SOFIE, that takes takes ONNX model as input and produces compilation-ready standalone C++ scripts as output. These scripts will then provide users an easy way to deploy their deep learning models in their physics software and analysis frameworks.\n\nThis project will focus on development of some missing deep learning operations which will allow to build more complex networks within TMVA. Specifically, we propose to implement the inference functionality of some ONNX operators in the code generation format. The student can choose to build this based on existing implementations in TMVA or other existing machine learning software tools or build their own from scratch. The expected result is a working implementation of modular operators classes that implement the operators as defined by the ONNX standards in the code generation format. The project requires also to write the corresponding unit tests need to validate the written code",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2022_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/vEuHzl6G/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in...",
          "slug": "root-tmva-sofie-developments-inference-code-generation-for-deep-learning-models",
          "status": "completed",
          "student_name": "Neel Shah",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "ROOT - TMVA SOFIE Developments - Inference Code Generation for Deep Learning models"
        },
        {
          "code_url": "https://gist.github.com/Nirhar/0e2a2970629765182c91fa48a90bb988",
          "description": "Clad is an open source plugin to the Clang compiler that detects from the parsed Abstract syntax tree, calls to differentiate a defined function, generates code that differentiates the function using the concept of Automatic Differentiation(AD) and modifies the Abstract Syntax Tree(AST) to insert the generated code. While clad works in the frontend of the compilation process, Enzyme, another LLVM based AD plugin works in the backend, where it takes in code in LLVM IR form and then differentiates the code. \n\nThis proposal aims to integrate Clad with Enzyme, and give the user the option of selecting Enzyme for Automatic Differentiation, based on his/her needs. This will give the user the same User Interface as clad for writing his/her code, but the option of using Enzyme as the backend with all its optimisations to calculate the Derivative/Gradient of the requested function.  This proposal also briefly gives insights into how this can be achieved by tapping into the existing code base of Clad.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/JQIwOjSz/",
          "proposal_id": null,
          "short_description": "Clad is an open source plugin to the Clang compiler that detects from the parsed Abstract syntax tree, calls to differentiate a defined function,...",
          "slug": "add-initial-integration-of-clad-with-enzyme",
          "status": "completed",
          "student_name": "Manish Kausik",
          "student_profile": null,
          "tags": [
            "ai",
            "frontend",
            "backend"
          ],
          "title": "Add Initial Integration of Clad with Enzyme"
        },
        {
          "code_url": "https://github.com/tmvadnn/tmva-batch-generator/wiki",
          "description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in many particle physics data analysis and applications. Since it is part of the ROOT data analysis framework, it comes with an automatically generated Python interface, which closely follows the C++ interface. The goal of this project is to develop a generator in C++ and Python to read data from the ROOT I/O and input them to the Python machine learning tools such as Tensorflow/Keras and PyTorch. The main aim of the generator is to efficiently input data from the ROOT I/O system to train machine learning models, and keep in memory only the data required to train a batch of events and not all the data set.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/0x5YD6Ig/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in...",
          "slug": "root-machine-learning-developments-batch-generator-for-training-machine-learning-models",
          "status": "completed",
          "student_name": "Sanchi Mittal",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "ROOT - Machine Learning Developments - Batch Generator for training machine learning models"
        },
        {
          "code_url": "https://hepsoftwarefoundation.org/gsoc/blogs/2022/blog_Acts_TirthankarMazumder.html",
          "description": "This project is for implementing a vectorized linear algebra backend. The current linear algebra implementation uses scalar instructions, but we can achieve a ~4x speed up if we use vector instructions when the relevant instructions sets (SSE, AVX) are available.\n\nAt the end, we expect to have a working SIMD implementation of the linear algebra code (addition and multiplication) for the relevant sizes (4×4, 6×6, 6×8, 8×8), a benchmarking and testing suite, and documentation which explains the code and the API.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/H0ek5yGF/",
          "proposal_id": null,
          "short_description": "This project is for implementing a vectorized linear algebra backend. The current linear algebra implementation uses scalar instructions, but we can...",
          "slug": "vectorized-linear-algebra-implementation-for-acts",
          "status": "completed",
          "student_name": "Tirthankar Mazumder",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "ui",
            "backend"
          ],
          "title": "Vectorized Linear Algebra Implementation for ACTS"
        },
        {
          "code_url": "https://divyanshtiwari237.github.io/open_source/2022/09/08/gsoc-project-report.html",
          "description": "The Geat4 toolkit lacks numerical integrators that preserve energy over the long course of simulations. Specifically, the g-2 physics experiment at Fermilab seeks to uncover whether a yet undiscovered force influences how the spin of the muon behaves.To do this it measures the polarization of the muon to seek new physics over thousands of revolutions in its custom accelerator ring. To replicate this in Geant4, the toolkit must track muons for 5,000 turns with near-perfect energy conservation and no drift in particle momentum.This necessitates the use of symplectic integration schemes to maintain the level of accuracy and precision required in the calculation. \n                       \nHence this projects aims to implement the following symplectic methods in the toolkit:\n1.) The Leapfrog Algorithm\n2.) Boris Algorithm\n3.)ESSRK method for non-separable Hamiltonians.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/YahUUBlV/",
          "proposal_id": null,
          "short_description": "The Geat4 toolkit lacks numerical integrators that preserve energy over the long course of simulations. Specifically, the g-2 physics experiment at...",
          "slug": "symplectic-integrators",
          "status": "completed",
          "student_name": "Divyansh Tiwari",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Symplectic Integrators"
        },
        {
          "code_url": "https://gist.github.com/joj0s/4d09e4c4489f36e88c8c4f76a95ecb80",
          "description": "Ganga routinely handles a tremendous amount of computational tasks simultaneously and keeping track of their status is prone to lagging behind with its current implementation, particularly regarding remote jobs submitted through DIRAC.\nFurthermore, it is the project mentors’ belief that the current implementation of the monitoring service could be simplified and brought up to date.\nMy goal at the end of the project’s timeline would be to deliver a new and efficient monitoring service using the asyncio framework. The main goal is for each backend’s monitoring to be rewritten using asyncio to make it as efficient as possible, but the service will also support multithreading and multiprocessing executors. It will be backend agnostic and allow any new backend to easily hook up to it. I also plan to specifically address DIRAC’s current inefficient monitoring implementation by using its REST API to retrieve running job information.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/fkDCJ5Ec/",
          "proposal_id": null,
          "short_description": "Ganga routinely handles a tremendous amount of computational tasks simultaneously and keeping track of their status is prone to lagging behind with...",
          "slug": "a-concurrency-model-for-the-monitoring-in-ganga",
          "status": "completed",
          "student_name": "Giorgos Apostolopoulos",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "backend"
          ],
          "title": "A concurrency model for the monitoring in Ganga"
        },
        {
          "code_url": "https://g4fastsim.web.cern.ch/docs/ML_Deployment_Kubeflow/Inference_Optimization_pipeline",
          "description": "Geant4 is a highly accurate and detailed simulation toolkit used for simulating the  passage of particles through matter. Due to its strict precision requirements, simulation is slow and has been proven to be a bottleneck for physics analysis. To overcome this bottleneck, popular machine learning techniques like generative modelling have been employed as a fast simulation alternative. This project focused on the Optimization and Inference components of the project. Very limited work is done on the model in terms of Memory Footprint Reduction. This project was aimed at implementing different post training  Memory Footprint Reduction techniques by creating a pipeline using KubeFlow, consolidating all the insights from different experiments and use them to extend KubeFlow pipeline built for carrying out the experiments into a more generalized form which can be used on different ML models at CERN to output the best optimized model.\n\nI integrated different Onnx Runtime Execution Providers - MLAS (default CPU),  CUDA, TensorRT,  oneDNN and OpenVINO into the Geant4 Par04 inference module. I have implemented an end-to-end KubeFlow pipeline which contains CPU and CUDA optimization modules which leverage different quantization and graph optimization workflows and reduce memory footprint of models while maintaining similar performance. Optimization modules for oneDNN and TensorRT are currently in-development at the time when GSoC is ending.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/EDxEGz4q/",
          "proposal_id": null,
          "short_description": "Geant4 is a highly accurate and detailed simulation toolkit used for simulating the passage of particles through matter. Due to its strict precision...",
          "slug": "geant4-fastsim-memory-footprint-optimization-for-ml-fast-shower-simulation",
          "status": "completed",
          "student_name": "Priyam Mehta",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui"
          ],
          "title": "Geant4-FastSim - Memory footprint optimization for ML fast shower simulation"
        },
        {
          "code_url": "https://gitlab.cern.ch/fastsim/kubeflow/geant4-kubeflow-pipeline",
          "description": "This is the tentative proposal submission for the Geant4-FastSim - Building an ML pipeline for fast shower simulation. This proposal discusses a detailed approach to implementing the Kubeflow End to End Pipeline for the existing framework. The proposal consists of a synopsis discussing how the proposed approach was thought and what were the key insights that motivated the steps mentioned. It further discusses the task that will be encountered throughout the Project timeline and the Major Deliverables that would mark the completion of the project.\nThe Major tasks have been visited in detail in the proposal and these are used to determine the objectives for the weekly progress timeline in the project. The proposal tries to cover up all the adopted methodology's significant detail and come up with the most ideal way to tackle each and every task.\n\nDeliverables:\n\nEnd to End Deployed ML Pipeline on Kubeflow\nMeta Logger component throughout the pipeline to store and analyze the results\nExperimenting with existing and new algorithmic approaches that might improve the performance, throughput of the modeling pipeline\nCreation of a connector component for the transition from training phase into the inference phase.\nIterative development of the Pipeline into a generalized workflow that is capable of scaling to newer versions of data as well support the integration of newer algorithmic experimentation\nWell documented results and analysis for motivating additional focus on the pipeline as well motivating the use of this pipeline for different projects\n\\",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/49Vg7Sho/",
          "proposal_id": null,
          "short_description": "This is the tentative proposal submission for the Geant4-FastSim - Building an ML pipeline for fast shower simulation. This proposal discusses a...",
          "slug": "project-proposal-geant4-fastsim-ml-pipeline-optimization-using-kubeflow",
          "status": "completed",
          "student_name": "Guneet Singh Kohli",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui"
          ],
          "title": "Project Proposal Geant4-FastSim - ML pipeline optimization using Kubeflow"
        },
        {
          "code_url": "https://hepsoftwarefoundation.org/gsoc/blogs/2022/blog_PODIO_SoumilBaldota.html",
          "description": "Currently, two major languages are used in high-energy physics (HEP): C++ for numerically intensive code, where execution speed is critical, and Python for interactivity and simplicity of development (frequently used as 'glue' between high-performance code modules). Julia has recently sparked increased attention as a potential language for HEP. This could provide Python's convenient features while maintaining C++'s ideal computational efficiency.\n\nIn order to continue this investigation, this project will interface the data model library PODIO with Julia. This will allow you to read existing data files into a Julia program.\n\nThis project seeks to use the same YAML-syntax to auto-generate Julia code for the end user to be utilized in HEP, as well as to do performance testing to compare the language interfaces for C++ and Julia.\n\nGeneral Plan\n\nPODIO can already :\n\nRead YAML files and validate and parse them to extract necessary information like data members, relations and vector members of components and form MemberVariable objects from that information, forming a object dictionary to be used by the jinja2 template engine (using templates for C++) to generate C++ code.\n\nOur Plan:\n\nBuild a prototype without code generation to test whether the current info passed to the jinja2 template Engine by the ClassGenerator class is sufficient, Accordingly adding the pre-processing logic required to the ClassGenerator class create new templates and dictionaries for jinja2 to generate Julia code. Running tests on the Julia code and refactoring generator code. Benchmarking and Documentation.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/WGGtU8Pp/",
          "proposal_id": null,
          "short_description": "Currently, two major languages are used in high-energy physics (HEP): C++ for numerically intensive code, where execution speed is critical, and...",
          "slug": "interfacing-podio-to-julia",
          "status": "completed",
          "student_name": "Soumil Baldota",
          "student_profile": null,
          "tags": [
            "python",
            "ml",
            "ai",
            "ui"
          ],
          "title": "Interfacing PODIO to Julia"
        },
        {
          "code_url": "https://gist.github.com/Harshil-Jani/c6778653973ccabcb397c170e56f6ddc#file-gsoc-report-md",
          "description": "The project aims at monitoring performance and improving the data visuals used to represent the performance report for Geant4 which is a toolkit for the simulation of passage of particles through matter. Performance reports are handled by using profiling tools such as perf and data visuals are generated by d3.js. Extending the performance report is also a part of this project.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/ZypcqQVM/",
          "proposal_id": null,
          "short_description": "The project aims at monitoring performance and improving the data visuals used to represent the performance report for Geant4 which is a toolkit for...",
          "slug": "geant4-performance-data-visualization-using-d3js",
          "status": "completed",
          "student_name": "Harshil Jani",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Geant4 - Performance Data Visualization using d3.js"
        },
        {
          "code_url": "https://docs.google.com/document/d/1dk3XECWpW_gXezTgUQ4cJDTM2-UKltWLwQCVxLt8iWk/edit?usp=sharing",
          "description": "ROOT is a data analysis framework designed to handle large amounts of data with high performance.\r\nThis proposal aims at optimizing the performance of ROOT by reducing unnecessary symbol lookup across the very large set of C++ modules.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/LcJL17UZ/",
          "proposal_id": null,
          "short_description": "ROOT is a data analysis framework designed to handle large amounts of data with high performance. This proposal aims at optimizing the performance of...",
          "slug": "optimize-root-use-of-modules-for-large-codebases",
          "status": "completed",
          "student_name": "Jun Zhang",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Optimize ROOT use of modules for large codebases"
        },
        {
          "code_url": "https://github.com/alexverus/hsf.github.io/blob/master/_gsocblogs/2022/blog_Belle2_ArulPrakash.md",
          "description": "The international Belle II collaboration handles huge amounts of data from the Belle II detector, relying on custom software for simulation, reconstruction,\nvisualization, and analysis. This software, developed over the past decade by hundreds of physicists, is constantly evolving to optimize the algorithms for better physics performance and resource efficiency. \n\nGiven the size, diversity and dynamic nature of the development team, quality assurance of the software is a big challenge. Belle II Software has had several monitoring schemes implemented so far, along with a high-level validation tool. This tool runs on existing large datasets producing distribution plots of quantities relevant for physics analyses and displaying the results on a webpage. This project aims to improve this validation tool. Bookkeeping of known issues detected by the tool, along with improved logging and integration with the issue tracking system can help fast track root cause analyses. The plots generated can also display more relevant information, and point toward the datasets used to encourage the reuse of common artefacts. \n\nThe plan is to enhance the user experience of the Belle II software validation tool and make workflows more efficient with appropriate technical solutions. We will start with integrating the issue tracking system with the validation tool, to better track the history of degradations. The plotting module will be overhauled to include relevant software information, as well as hyperlinks to make them self-sufficient. Enhancements to the logging module to have better logs attached directly to the issues from the validation tool to fast track issue debugging. The common artefacts will be pooled together in a manner allowing for easy reference from plots, logs and issues and also to prevent regeneration overheads. All the changes will be integrated with the production code, after rigorous testing, along with detailed documentation.",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2022_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/C7QoeYiS/",
          "proposal_id": null,
          "short_description": "The international Belle II collaboration handles huge amounts of data from the Belle II detector, relying on custom software for simulation,...",
          "slug": "advanced-belle-ii-software-validation",
          "status": "completed",
          "student_name": "arul",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "Advanced Belle II Software Validation"
        },
        {
          "code_url": "https://bit.ly/nimish-gsoc-blog-rucio",
          "description": "Rucio has proven its potential to be used for providing functionality to scientific collaborations to organize, manage, monitor, and access their distributed data and dataflows across heterogeneous infrastructures. What it needs is a revamped user-friendly UI. This will not only encourage existing users to get a feel of how Rucio continues to grow and reach new milestones, but also increase the adoption  of the Rucio WebUI in general. The desired outcomes of my stint would not only involve a complete revamp of the existing UI by building a UI library of our own but also presenting users with a new & intuitive dashboard, keeping the core functionality of Rucio in mind. Rucio also supports multiple types of users and their specific workflows.\n\nThe first task would involve migrating the WEBUI to a pure REST’ful architecture would require identifying and implementing new REST endpoints on the Rucio Server and developing a dynamic cross-platform ReactJS application capable of consuming the REST API directly. The second task is to improve the overall user experience for different ‌users. The new dashboard would allow users to get a quick overview of relevant activity and provide quick access to frequently used functionalities.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/1ps6wuOz/",
          "proposal_id": null,
          "short_description": "Rucio has proven its potential to be used for providing functionality to scientific collaborations to organize, manage, monitor, and access their...",
          "slug": "rucio-webui-revamp",
          "status": "completed",
          "student_name": "Nimish B",
          "student_profile": null,
          "tags": [
            "react",
            "web",
            "api",
            "ui"
          ],
          "title": "Rucio WebUI Revamp"
        },
        {
          "code_url": "https://gist.github.com/Harshalzzzzzzz/b593db68f8d9e79407746f0c463206e5",
          "description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework. The pythonization work proposed here is vital to facilitate the transition from C++ to python to write analysis code. The goal of this project is to enhance the Python interface to make it more “pythonic”, i.e. easier to use. This project aims to simplify complex workflows and enhancement of the python interface, greatly reducing the amount of code that has to be written, including pythonizations for TMVA GUI and Hist functions and converters for PyROOT NumPy arrays to convert RTensor from and to PyROOT NumPy arrays.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2022_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/luKbKGoR/",
          "proposal_id": null,
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework. The...",
          "slug": "tmva-developments-improve-python-interface-for-tmva",
          "status": "completed",
          "student_name": "Harshal Shende",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "TMVA Developments - Improve Python interface for TMVA"
        },
        {
          "code_url": "https://hepsoftwarefoundation.org/gsoc/blogs/2022/blog_ATHENA_UjwalKundur.html",
          "description": "The goal of this project is to re-implement the current clustering algorithms using SYCL for higher-throughput processing and handling future upgrades to the EIC ECAL pixel detector resolution. SYCL enables GPGPU (General Purpose GPU) compute for hardware accelerated processing of ECAL data while enabling future-proofing of newer hardware backends.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/ShD0pC22/",
          "proposal_id": null,
          "short_description": "The goal of this project is to re-implement the current clustering algorithms using SYCL for higher-throughput processing and handling future...",
          "slug": "electromagnetic-cluster-finding-on-gpus",
          "status": "completed",
          "student_name": "Ujwal Kundur",
          "student_profile": null,
          "tags": [
            "backend"
          ],
          "title": "Electromagnetic Cluster Finding on GPUs"
        },
        {
          "code_url": "https://github.com/DamianArado/GSoC-2022-Phoenix/blob/main/SUMMARY.md",
          "description": "This project aims to introduce a new testing strategy so that we encourage Behaviour-driven Development here, at Phoenix — the official web event display of the ATLAS experiment.\n\nThe whole testing setup of Phoenix needs a complete revamp. Currently, Phoenix as a project comprises 3 packages: phoenix-event-display, phoenix-ui-components, and phoenix-app. \n\nAnd, my goal is to set new benchmarks and coverage thresholds for these 3 packages that will essentially, enforce and encourage testing for the whole of the project with Behavior-driven Development as its sole motivation so that potential issues that may occur in a critical environment can be identified much before they happen. This can effectively help us to troubleshoot problems and bugs so that the changes that will be made in the future to the codebase can be deemed reliable.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/s6n3c0Zc/",
          "proposal_id": null,
          "short_description": "This project aims to introduce a new testing strategy so that we encourage Behaviour-driven Development here, at Phoenix — the official web event...",
          "slug": "revamped-testing-infrastructure-for-phoenix",
          "status": "completed",
          "student_name": "Mohammad Humayun Khan",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "Revamped Testing Infrastructure for Phoenix"
        },
        {
          "code_url": "https://github.com/MadAnalysis/madanalysis5/pull/125",
          "description": "The purpose of this project is to enable multi-weight calculations in the MadAnalysis 5 platform. The current implementation only uses the nominal weight. The secondary goal is to evaluate ideas for parallelization of compute heavy components of the project for long term contribution beyond GSoC. \n\nTo preserve dependencies and avoid conflicts, I believe that a refactoring of the Weight class object is the ideal approach, functionality to combine weights shall be implemented as methods within the new Weight class, histogramming and cut-flow classes will then be modified to take advantage of the new Weight class object.",
          "difficulty": null,
          "id": "proj_cern-hsf_2022_021",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/Nd2nB4BJ/",
          "proposal_id": null,
          "short_description": "The purpose of this project is to enable multi-weight calculations in the MadAnalysis 5 platform. The current implementation only uses the nominal...",
          "slug": "madanalysis-5-integration-of-theoretical-uncertainty-calculation-with-multi-weight-integration",
          "status": "completed",
          "student_name": "Kyle Fan",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "MadAnalysis 5 - Integration of theoretical uncertainty calculation with multi-weight integration"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2022/organizations/cern-hsf/"
    },
    "year_2023": {
      "num_projects": 16,
      "projects": [
        {
          "code_url": "https://maximusron.github.io/2022-09-23/GSoC-23-CERN",
          "description": "Numba is a JIT compiler that translates a subset of Python and NumPy code into fast machine code. Cppyy is an automatic, run-time, Python-C++ bindings generator, for calling C++ from Python and Python from C++.\nCppyy has to pay a time penalty each time it needs to switch between languages which can multiply into large slowdowns when using loops with cppyy objects. This is where Numba can help. Since Numba compiles the code in loops into machine code it only has to cross the language barrier once and the loops thus run faster.\nInitial support for Cppyy objects in Numba enabled the use of builtin types and classes, but some essential C++ features, such as references and STL classes, are not yet supported.\n\n\nThe project aims to add support for C++ reference types in Numba through Cppyy and improve the existing numba extension implementation to provide general support for C++ templates. This added support will allow cppyy users to define a wider array of standard and templated functions that can leverage reference types to the C++ code defined in python.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/PFtlRi3L",
          "proposal_id": "QWfrZSAS",
          "short_description": "Numba is a JIT compiler that translates a subset of Python and NumPy code into fast machine code. Cppyy is an automatic, run-time, Python-C++...",
          "slug": "extend-the-cppyy-support-in-numba",
          "status": "completed",
          "student_name": "Aaron Jomy",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "Extend the Cppyy support in Numba"
        },
        {
          "code_url": "https://zenodo.org/records/10073216",
          "description": "Development and Deployment of a Lossy Compression Tool 'Baler' that uses deep autoencoders to flexibly compress scientific data. Use Baler to compress data for LHC experiments like ATLAS and improve existing Baler model to be more robust and accurate while reconstructing data. Document and Benchmark Baler's performance for real-world physics experiment and explore it's capabilities on non-scientific data.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/gnKb1DHZ",
          "proposal_id": "MrXJnVPd",
          "short_description": "Development and Deployment of a Lossy Compression Tool 'Baler' that uses deep autoencoders to flexibly compress scientific data. Use Baler to...",
          "slug": "deep-autoencoders-for-scientific-data-compression",
          "status": "completed",
          "student_name": "Aman Singh Thakur",
          "student_profile": null,
          "tags": [],
          "title": "Deep Autoencoders for Scientific Data Compression"
        },
        {
          "code_url": "https://gitlab.com/hepcedar/lhapdf/-/merge_requests?scope=all&state=all&author_username=ArjunBoi",
          "description": "The LHAPDF C++ library plays a crucial role in the Large Hadron Collider (LHC) programme by providing it with Parton Distribution Function (PDF) data for both experimental and theoretical calculations. The library's reliability and stability are essential to ensure that the information supplied to the LHC programme is accurate.\n\nTo ensure that all aspects of the LHAPDF calculations are thoroughly tested, this project aims to implement continuous integration (CI) tests, in GitLab and Cron. These tests will be scalable and will run on a schedule to guarantee the stability of LHAPDF-established behaviours to a high degree of precision. Data from these tests will then be fed into a dashboard on the LHAPDF website, which will work to provide the developers with a bird’s-eye view of the library’s status and performance. This testing and visualisation is aimed at providing the necessary confidence when making new releases and analysing current metrics.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/v2AxsuIT",
          "proposal_id": "Q3VVKUTq",
          "short_description": "The LHAPDF C++ library plays a crucial role in the Large Hadron Collider (LHC) programme by providing it with Parton Distribution Function (PDF) data...",
          "slug": "mcnetlhapdf-test-suite-and-coverage-for-parton-density-calculations",
          "status": "completed",
          "student_name": "Arjun  Taneja",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "MCnet/LHAPDF - Test Suite and Coverage for Parton Density Calculations"
        },
        {
          "code_url": "https://gitlab.cern.ch/cguan/ml4fastsim/-/tree/autoregressive-dev?ref_type=heads",
          "description": "Calorimeter is one of the most important components of the Large Hadron Collider (LHC) experiments, detecting energy loss of particles after collision. However, current Geant4 simulation of showering process in the calorimeter is inherently slow for large amounts of events, especially after High Luminosity Upgrading in the future. Therefore, generative models and other machine learning techniques can be used to accelerate the Geant4 simulation. This project aims to improve the performances of the current VQ-VAE Transformer for particle showering and explore architectures beyond (VQ-)VAE and/or Transformer, including new position embedding, hierarchical attention, an-isotropic attention, Fourier Transform linear mixer, and etc. We expect to provide deliverables including notes of extended numeric experiment and integrating new models into the project code.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/mQ2n3AcZ",
          "proposal_id": "CHegyXTu",
          "short_description": "Calorimeter is one of the most important components of the Large Hadron Collider (LHC) experiments, detecting energy loss of particles after...",
          "slug": "geant4-fastsim-transformer-based-architecture-for-fast-shower-simulation",
          "status": "completed",
          "student_name": "Chenguang Guan",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Geant4-FastSim: Transformer-based architecture for fast shower simulation"
        },
        {
          "code_url": "https://docs.google.com/presentation/d/1l9cGNBloVSmt0u_Cyl1EBOu72orrBEdg/edit?usp=sharing&ouid=109152010533861457157&rtpof=true&sd=true",
          "description": "I would like to apply for the Improve automatic differentiation of object-oriented paradigms using Clad project. \nThis project is mainly focused on supporting object-oriented programming features in clad, including several milestones such as differentiation of constructors, differentiation of operator overloads, reference class members, and custom derivatives for object-oriented constructs. It's valuable as no other AD system has done this before, and thus is experimental.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/AFXlNwSz",
          "proposal_id": "sKFwbkVc",
          "short_description": "I would like to apply for the Improve automatic differentiation of object-oriented paradigms using Clad project. This project is mainly focused on...",
          "slug": "improve-automatic-differentiation-of-object-oriented-paradigms-using-clad",
          "status": "completed",
          "student_name": "Daemond Zhang",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Improve automatic differentiation of object-oriented paradigms using Clad"
        },
        {
          "code_url": "https://daniel-sparemblek.github.io/gsoc-2023/",
          "description": "Reva is the core of the CERNBox service. It is a middleware framework,\nproviding interoperability between storage and application providers using a\nuniversal set of APIs. \n\nCurrently, Reva’s notifications system is under development. In the next\nfew weeks, a small set of actions in the notifications system will be launched,\nincluding\n• Notifying a user you shared a file/folder with them via email.\n• Notifying a user you uploaded a file to a folder they shared with you.\n\nI propose to improve this notifications center with several additions.\nThe notifications would have a new settings option in which the user can\nprimarily choose which MIME type and possibly file type he wants to receive\nnotifications for.\n\nThis is to be done with additions to the database schema, possibly designing\na completely new schema. Also, there is major work to be done in the back-end\nsystem in order to fully support this feature, and of course lastly, in the front\nend - there needs to be work done on creating the settings window and parsing\nthe user choices for the setting they want to choose.\n\nAfter this, another improvement can be done to the notifications center\nwhich is the ability to get metrics on certain data - in particular how many\nfiles/folders of a certain type or in general have been shared to this or by this\nuser. There is mostly work on the front end to be done here designing the\npossible options in the settings to display this metric. But also work on accessing\nthe database to get these metrics displayed to the user.\n\nDeliverables:\nThe extension of the simple notifications into a notifications center in the\nUI, including settings for filtering the MIME type of the file\n• Documentation on the implementation, reusability, extensibility, and testability\nof the system, alongside a guide on how to use the UI.\n• This includes extensions of the current back-end architecture as well as\nextensions in the front-end UI.\n• A richer API system for notifications that include metrics analysis",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/zrPEE0Cg",
          "proposal_id": "0kkyYbLp",
          "short_description": "Reva is the core of the CERNBox service. It is a middleware framework, providing interoperability between storage and application providers using a...",
          "slug": "extend-and-improve-the-cernbox-notifications-platform",
          "status": "completed",
          "student_name": "daniel-rey",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "database",
            "ui"
          ],
          "title": "Extend and improve the CERNBox Notifications platform"
        },
        {
          "code_url": "https://gist.github.com/HieuLCM/c69359336d4fe6138cecd872bd5f5540",
          "description": "The proposed project aims to improve the accessibility and user experience of Belle II by developing a web application for event display. Currently, the event display for Belle II relies on ROOT TEve, which requires the installation of the full Belle II software on the local machine, making it difficult for users to access the information they need. To resolve this issue, the proposed solution is to develop a web application that utilizes the Phoenix-based event display for Belle II, enabling users to view the detector geometry, recorded signals, and reconstructed objects on a web browser. The project's deliverables include the development of a working web application, thorough testing, and documentation for both users and developers. The successful implementation of this project is expected to enhance the usability and accessibility of Belle II results, thus contributing to the advancement of particle physics research.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/2WYpamvj",
          "proposal_id": "QPjetVjH",
          "short_description": "The proposed project aims to improve the accessibility and user experience of Belle II by developing a web application for event display. Currently,...",
          "slug": "belle-ii-event-display-with-phoenix",
          "status": "completed",
          "student_name": "Hieu Le Cong Minh",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "Belle II Event Display with Phoenix"
        },
        {
          "code_url": "https://gist.github.com/jnpark3/f30662b6948cd7e1a346da76c8116daa",
          "description": "Graph neural networks (GNNs) have shown great potential in solving the complex problem of charged particle tracking, which is an essential task in high-energy physics. Charged particle tracking entails reconstructing the trajectory of charged particles produced after particle collisions, and the scalability of GNNs could enable this task to be performed with high efficiency. The first step in applying GNNs to particle detector data is to construct edges between hits, which then enables a GNN to operate on the graph and generate predicted trajectories. The current process for graph construction involves constructing all possible edges first and then removing edges that violate manually set geometric constraints. This project aims to improve the accuracy and efficiency of this graph construction step by creating a point cloud network (PCN) with PyTorch Geometric to perform this task. PCNs are a recently developed class of neural networks that operate directly on point cloud data, and they could be used to perform graph construction without manually selected geometric constraints and with fewer edge constructions in total. This project also involves optimizing hyperparameters for the PCN model architecture and the training process by using Ray tune and Optuna.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2023_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/bL22MDFU",
          "proposal_id": "4ET7OVxE",
          "short_description": "Graph neural networks (GNNs) have shown great potential in solving the complex problem of charged particle tracking, which is an essential task in...",
          "slug": "graph-construction-for-charged-particle-tracking-using-point-cloud-networks",
          "status": "completed",
          "student_name": "Jian Park",
          "student_profile": null,
          "tags": [
            "ai",
            "cloud"
          ],
          "title": "Graph Construction for Charged Particle Tracking using Point Cloud Networks"
        },
        {
          "code_url": null,
          "description": "The Large Hadron Collider (LHC) experiments generate massive datasets composed of billions of proton-proton collisions. The analysis of this data requires high-throughput scientific computing that relies on efficient software algorithms. In this project, I aim to investigate whether small efficiency improvements in the LHC software can have a large energetic impact, given the sheer volume of data involved. Additionally, I aim to explore the impact of different computing architectures and job submission systems on energy efficiency. To achieve these goals, I will use metrics from the Green Software Foundation and other resources to estimate energy efficiency. I will then evaluate whether to make small changes to the code to improve efficiency and evaluate the potential savings. I will also test the software on different platforms and job submission systems. My expected results include a summary of metrics for software energy consumption, visualisation of test results, and identification of possible improvements to software algorithms. The project will provide valuable insights into the energy efficiency of scientific software, with potential applications beyond the LHC experiments.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/Nks9akq7",
          "proposal_id": "Om5oyoKX",
          "short_description": "The Large Hadron Collider (LHC) experiments generate massive datasets composed of billions of proton-proton collisions. The analysis of this data...",
          "slug": "estimating-the-energy-cost-of-scientific-software",
          "status": null,
          "student_name": "Manas Pratim Biswas",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Estimating the energy cost of scientific software"
        },
        {
          "code_url": "https://didithilmy.github.io/gsoc2023",
          "description": "CERN uses a service called Service for Web Analysis (SWAN) to perform analyses on scientific data, which is built on top of Jupyter notebook. Currently, the way a notebook kernel connects to a Spark cluster is through SWAN’s own open-source SparkConnector extension. Due to a current Spark limitation, it is not possible for multiple notebooks to share the same set of Spark resources. In addition, the process of spawning the Spark resources could take a while, and may add an inconvenience.\n\nCurrently, an effort is underway to employ a client-server architecture known as Spark Connect. This would allow multiple notebooks to connect to a previously instantiated Spark session and submit computations to it.\n\nTo make allocating Spark resources easier for users, this project proposes the development of a JupyterLab extension. The extension shall have a friendly interface that would allow users to instantiate one or more Spark sessions–in which the notebook will be able to connect, configure proper credentials and authentication, and make the connection accessible to the notebook code. The connection and session will be persistent across multiple kernels and restarts.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/2Z7UhtwJ",
          "proposal_id": "k7MQ6aFC",
          "short_description": "CERN uses a service called Service for Web Analysis (SWAN) to perform analyses on scientific data, which is built on top of Jupyter notebook....",
          "slug": "leverage-spark-connect-for-interactive-data-analysis-in-jupyter-notebooks",
          "status": "completed",
          "student_name": "Muhammad Aditya Hilmy",
          "student_profile": null,
          "tags": [
            "web",
            "ui"
          ],
          "title": "Leverage Spark Connect for interactive data analysis in Jupyter Notebooks"
        },
        {
          "code_url": "https://gist.github.com/Neel-Shah-29/b2c22b07025c72496fe3dbe1fb0b1cbb",
          "description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in many particle physics data analysis and applications. Currently, we are developing a fast inference system in TMVA, called SOFIE, that takes takes ONNX model as input and produces compilation-ready standalone C++ scripts as output. These scripts will then provide users an easy way to deploy their deep learning models in their physics software and analysis frameworks.\r\n\r\nThis project will focus on development of some missing deep learning operations which will allow to build more complex networks within TMVA for parsing the Transformer based models and Graph Net Models in SOFIE. Specifically, we propose to implement the inference functionality of some ONNX operators in the code generation format. The student can choose to build this based on existing implementations in TMVA or other existing machine learning software tools or build their own from scratch. The expected result is a working implementation of modular operators classes that implement the operators as defined by the ONNX standards in the code generation format. The project requires also to write the corresponding unit tests need to validate the written code.",
          "difficulty": "beginner",
          "id": "proj_cern-hsf_2023_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/xoPLgxys",
          "proposal_id": "LqBKqknP",
          "short_description": "Toolkit for Multivariate Analysis (TMVA) is a multi-purpose machine learning toolkit integrated into the ROOT scientific software framework, used in...",
          "slug": "root-tmva-sofie-developments-inference-code-generation-for-deep-learning-models",
          "status": "completed",
          "student_name": "Neel Shah",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "ROOT - TMVA SOFIE Developments - Inference Code Generation for Deep Learning models"
        },
        {
          "code_url": null,
          "description": "xeus-clang-REPL is a C++ kernel for Jupyter notebooks using clang-REPL as its C++ Interpreter. Cppyy is an automatic, run-time, Python-C++ bindings generator, for calling C++ from Python and Python from C++.\n\nAllowing C++ and Python to talk between themselves in a Jupyter notebook will allow users to switch between Python and C++ at will. This means that data analysts can set up their analysis in Python while running the actual analysis in C++. Thus reducing the time to write and debug their analysis pipeline.\n\nInitial support of cross talk between the two kernels has been implemented but this only supports passing primitive data types. This project aims to use Cppyy to extend this to support classes and functions.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/GGwZBrTf",
          "proposal_id": "jUqhIN5M",
          "short_description": "xeus-clang-REPL is a C++ kernel for Jupyter notebooks using clang-REPL as its C++ Interpreter. Cppyy is an automatic, run-time, Python-C++ bindings...",
          "slug": "enable-cross-talk-between-python-and-c-kernels-in-xeus-clang-repl-by-using-cppyy",
          "status": null,
          "student_name": "Smit Shah",
          "student_profile": null,
          "tags": [
            "python",
            "ai"
          ],
          "title": "Enable cross-talk between Python and C++ kernels in xeus-clang-REPL by using Cppyy"
        },
        {
          "code_url": "https://github.com/Somya-Bansal159/GSoC-2023",
          "description": "This project aims to improve the scale and navigation inside the Phoenix application which is basically an Angular-based web application written in the ThreeJS library to visualize high-energy physics events in 3D. The application renders a detector that detects the collisions of particles that have been accelerated up to speeds comparable to that of light, along with the event data that consists of tracks, jets, missing energy, etc. after the collision.\n\nThe problem statement is to improve the visualization of the event, for example, by developing a customizable and interactive cartesian grid functionality, so as to better give the idea of the locations and distances of the particles to the researchers. Also, the user should be able to view the dimensions of the individual detector parts. Apart from this, the users should be able to navigate to the individual detector parts. My plan for the first problem is to introduce a 3D cartesian grid with multiple equally spaced XY, YZ, and ZX planes, as well as, to be able to display the 3D coordinates of a point on clicking. For this, I will make use of the ThreeJS raycaster intersects. Also, I am planning to render the Euclidean distance between any two points clicked. To view the dimensions of the detector, I am planning to render the dimensions of the bounding box of the component. The solution for the second problem would be to list the detector parts and let the user select any one of them, after which it will be highlighted using a ThreeJS outline pass.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/lZhAZStY",
          "proposal_id": "bks08EF4",
          "short_description": "This project aims to improve the scale and navigation inside the Phoenix application which is basically an Angular-based web application written in...",
          "slug": "improving-the-sense-of-scale-and-navigation-in-high-energy-physics-event-visualization",
          "status": "completed",
          "student_name": "Somya Bansal",
          "student_profile": null,
          "tags": [
            "angular",
            "web",
            "ai"
          ],
          "title": "Improving the sense of scale and navigation in high energy physics event visualization"
        },
        {
          "code_url": "https://gist.github.com/Sumalyo/1fbb17299c3dd3195cde10786409dd27",
          "description": "This project aims to add support for real-time lossless compression libraries and algorithms in the data acquisition system that is being used for the FASER experiment, to reduce storage costs.  This would be done after thorough exploration and experimentation, identifying suitable compression libraries which can be integrated seamlessly into the system. Detailed documentation of the entire process and support for the decompression of compressed data (for physics analysis) will also be provided.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/6JxnmPQ0",
          "proposal_id": "J1dPoByw",
          "short_description": "This project aims to add support for real-time lossless compression libraries and algorithms in the data acquisition system that is being used for...",
          "slug": "real-time-lossless-data-compression-for-the-faser-experiment",
          "status": "completed",
          "student_name": "Sumalyo Datta",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui"
          ],
          "title": "Real-time lossless data compression for the FASER experiment"
        },
        {
          "code_url": "https://gist.github.com/vaithak/82125fa9618c81741dcecb88f0e76d4b",
          "description": "In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Automatic differentiation is an alternative technique to Symbolic differentiation and Numerical differentiation (the method of finite differences). Clad is based on Clang which provides the necessary facilities for code transformation. The AD library is able to differentiate non-trivial functions, to find a partial derivative for trivial cases and has good unit test coverage.\r\n\r\nVector mode support will facilitate the computation of gradients using the forward mode AD in a single pass and thus without explicitly performing differentiation n times for n function arguments. The major benefit of using vector mode is that computationally expensive operations do not need to be recomputed n times for n function arguments.",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/Ro5V6AT1",
          "proposal_id": "UoBkLELx",
          "short_description": "In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function...",
          "slug": "implement-vector-mode-in-forward-mode-automatic-differentiation-in-clad",
          "status": "completed",
          "student_name": "Vaibhav Thakkar",
          "student_profile": null,
          "tags": [],
          "title": "Implement vector mode in forward mode automatic differentiation in Clad"
        },
        {
          "code_url": "https://yashnator.github.io/gsoc-submission",
          "description": "RooFit uses RooWorkSpace to manage its models. Currently, the syntax to interact with RooWorkspace is completely different from Python syntax, which makes the code less intuitive & harder to manage. The project aims at making the interaction with RooWorkspace more Pythonic. \n\nPythonization RooWorkspace is done in two main steps:\n1.) Writing a C++ plugin for the RooWorkspace factory\n2.) Writing Pythonizations \nThis is followed by making unit tests, writing documentation & writing tutorial for the new Pythonic interaction.\n\nThe deliverables for this project are:\n\n● Develop a Pythonic alternative to RooWorkspace::factory()\n● Pythonization of special operations of RooWorkspace along with support for creating Histfactory p.d.f.s using the JSON I/O\n● Write unit tests to check the functionality of Pythonic code\n● Make tutorials to demonstrate the usage of new functions in Python & improve documentation for HistFactory",
          "difficulty": null,
          "id": "proj_cern-hsf_2023_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/eBsTbcaN",
          "proposal_id": "cLD9QT1I",
          "short_description": "RooFit uses RooWorkSpace to manage its models. Currently, the syntax to interact with RooWorkspace is completely different from Python syntax, which...",
          "slug": "roofit-pythonic-interaction-with-the-rooworkspace",
          "status": "completed",
          "student_name": "Yash Solanki",
          "student_profile": null,
          "tags": [
            "python",
            "ai",
            "ui"
          ],
          "title": "RooFit - Pythonic interaction with the RooWorkspace"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2023/organizations/cern-hsf"
    },
    "year_2024": {
      "num_projects": 17,
      "projects": [
        {
          "code_url": "https://compiler-research.org/blogs/gsoc24_tharun_anandh_final_blog/",
          "description": "Xeus-cpp, a Jupyter kernel for C++, is built upon the native implementation of the Jupyter protocol, xeus. This setup empowers users to interactively write and execute C++ code, providing immediate visibility into the results.With its REPL (read-eval-print-loop) functionality, users can rapidly prototype and iterate without the need to compile and run separate C++ programs. Additionally, this integration facilitates seamless interaction between C++ and Python within the same Jupyter environment.\n\nWith the ever growing popularity of large language models, this project aims to integrate a large language model with the xeus-cpp Jupyter kernel. This integration will enable users to interactively generate and execute code in C++ leveraging the assistance of the language model.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/MpQ83ckH/",
          "proposal_id": null,
          "short_description": "Xeus-cpp, a Jupyter kernel for C++, is built upon the native implementation of the Jupyter protocol, xeus. This setup empowers users to interactively...",
          "slug": "integrate-a-large-language-model-with-the-xeus-cpp-jupyter-kernel",
          "status": "completed",
          "student_name": "Tharun A",
          "student_profile": null,
          "tags": [
            "python",
            "api",
            "ml",
            "ai",
            "ui"
          ],
          "title": "Integrate a Large Language Model with the xeus-cpp Jupyter kernel"
        },
        {
          "code_url": "https://gist.github.com/gojakuch/cb6df9940192abc23fdaf5f3f56aff86",
          "description": "The goal is to implement the differentiation of the Kokkos framework including the support of Kokkos functors, lambdas, methods such as parallel_for, parallel_reduce, and deep_copy, as well as the general support for Kokkos view data structures. The set-off points for the project should be the existing \"Kokkos-aware Clad\" PR and the test cases I have developed.\r\n\r\nThe additional aim of the project is to implement a generic approach to support any C++ library (starting with Kokkos) in such a way that the core of Clad is invariant to the internals of the library, but any Clad user can add it in a pluggable format for individual use cases.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/9hnfCx7g/",
          "proposal_id": null,
          "short_description": "The goal is to implement the differentiation of the Kokkos framework including the support of Kokkos functors, lambdas, methods such as parallel_for,...",
          "slug": "implement-differentiating-of-the-kokkos-framework-in-clad",
          "status": "completed",
          "student_name": "atell krasnopolsky",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Implement Differentiating of the Kokkos Framework in Clad"
        },
        {
          "code_url": "https://gist.github.com/UncleWeeds/5fd3ac678405d4f1b1b6cb61f3d0939b",
          "description": "This proposal aims to enhance the MCnet projects, Rivet and YODA, by automating the generation and deployment of Docker images across multiple architectures, thus ensuring broad accessibility and ease of use. For Rivet, we focus on Docker automation and web documentation migration into CI builds for seamless new website deployments. YODA enhancements will concentrate on bolstering unit testing and coverage analysis frameworks through the integration of gcov for comprehensive coverage insights, utilizing lcov for accessible reporting, and developing web-based visualizations for coverage reports. This initiative promises to significantly elevate the clarity, reliability, and functionality of both projects, making them more robust and user-friendly for the high-energy physics research community.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/2ftQZYBL/",
          "proposal_id": null,
          "short_description": "This proposal aims to enhance the MCnet projects, Rivet and YODA, by automating the generation and deployment of Docker images across multiple...",
          "slug": "mcnetrivetyoda-ci-based-multi-arch-docker-and-code-coverage",
          "status": "completed",
          "student_name": "Abhiramsai",
          "student_profile": null,
          "tags": [
            "web",
            "ml",
            "ai",
            "docker",
            "ui"
          ],
          "title": "MCnet/Rivet+YODA CI-based Multi-Arch Docker and Code Coverage"
        },
        {
          "code_url": "https://gitlab.cern.ch/mpiorczy/diffusion4fastsim",
          "description": "Fast and accurate simulation of high-energy physics experiments is crucial for advancing our understanding of fundamental particles and forces in nature. However, the traditional method of obtaining such simulations using Monte Carlo methods is computationally expensive. Recently, machine learning techniques such as generative modeling have been proposed as an alternative approach to providing simulation results. In particular, diffusion models have emerged as highly accurate, but the standard formulation of the diffusion process suffers from slow inference speeds. This project aims to accelerate the inference of diffusion models by exploring recently proposed techniques in the field, such as distillation, continuous-time diffusion formulations, or more efficient architectures. The methods developed in this project will be integrated into the Geant4FasSim repository, enhancing its capabilities for fast and accurate simulation of high-energy physics experiments.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/xq4BGRsM/",
          "proposal_id": null,
          "short_description": "Fast and accurate simulation of high-energy physics experiments is crucial for advancing our understanding of fundamental particles and forces in...",
          "slug": "geant4-fastsim-fast-inference-of-diffusion-models",
          "status": "completed",
          "student_name": "Mikołaj Piórczyński",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Geant4-FastSim - Fast inference of Diffusion models"
        },
        {
          "code_url": "https://github.com/extint/gsoc24-blog/blob/main/Final-Evaluation-Gsoc.md",
          "description": "SOFIE aims to streamline the integration of deep learning models into the ROOT scientific software framework through efficient inference code generation. By implementing missing ONNX operators, particularly those used in transformer models, we will enhance TMVA's capabilities for building complex neural networks. Our project will deliver modular operator classes adhering to ONNX standards, accompanied by thorough unit testing, enabling seamless deployment of deep learning models in physics software and analysis frameworks.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2024_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/6DDnly0z/",
          "proposal_id": null,
          "short_description": "SOFIE aims to streamline the integration of deep learning models into the ROOT scientific software framework through efficient inference code...",
          "slug": "sofie-developments-inference-code-generation-for-deep-learning-models",
          "status": "completed",
          "student_name": "Vedant Mehra",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui"
          ],
          "title": "SOFIE Developments - Inference Code Generation for Deep Learning models"
        },
        {
          "code_url": "https://gist.github.com/imorlxs/d6a09ca535a94df296547f8a52c51b5d",
          "description": "BioDynaMo, a powerful agent-based simulation platform, utilizes ROOT for functionalities like statistical analysis, random number generation, and IO. However, BioDynaMo's reflection system, which relies on efficient access to class information, can be optimized for improved performance. This project proposes upgrading to C++ modules within the ROOT framework to achieve this goal.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/SSN3Mol1/",
          "proposal_id": null,
          "short_description": "BioDynaMo, a powerful agent-based simulation platform, utilizes ROOT for functionalities like statistical analysis, random number generation, and IO....",
          "slug": "improving-performance-of-biodynamo-using-root-c-modules",
          "status": "completed",
          "student_name": "Isaac Morales",
          "student_profile": null,
          "tags": [],
          "title": "Improving performance of BioDynaMo using ROOT C++ Modules"
        },
        {
          "code_url": "https://gist.github.com/brauliorivas/f1a4cb5dc84ee63182b8525ae0f581c5",
          "description": "EDM4hep offers a powerful set of objects to describe event data from simulation to analysis. At any step of this chain, one can inspect the event data containing various collections of the objects, e.g. calorimeter clusters, tracker hits, reconstructed particles, etc. At the moment dmX offers only a limited possibility to visualize a tree of MonteCarlo particles. In this project, dmX is extended to visualize other types of collections and relationships between them as a graph to gain more insights about the collisions useful for further development in the Accelerators world.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/9rDo0T9A/",
          "proposal_id": null,
          "short_description": "EDM4hep offers a powerful set of objects to describe event data from simulation to analysis. At any step of this chain, one can inspect the event...",
          "slug": "any-collection-in-data-model-explorer",
          "status": "completed",
          "student_name": "Braulio Rivas",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Any Collection in Data Model Explorer"
        },
        {
          "code_url": "https://github.com/pavlo-svirin/root/tree/superbuilds",
          "description": "ROOT is a framework for data processing, born at CERN, at the heart of the research on high-energy, molecular and laser physics, as well as in  astronomy.\r\nROOT has plenty of built-in components. Because of a variety of  features, ROOT is a very large software which takes a very long time to compile. Currently ROOT compiles all of the components available within the source code distribution. \r\nThe goal of the project is to speed up the compilation process by letting users specify which components of the ROOT will be needed. This can be done by converting ROOT’s CMake configuration into a set of “CMake External Projects” (superbuilds).  Superbuilds can remove all of this cruft from the project’s source repository, and enable you to more directly use the upstream project’s build system as an independently built software component. It is basically a simple package manager that you make consistently work across your target build platforms, and if your target platforms have a common package manager you might consider using that instead. \r\nAlso, during the configuration process it could be possible to identify if ROOT is already installed in the destination folder, the components which are already installed and offer an option to skip their compilation and use these already installed components for current compilation.\r\nWe are not going to change the build system of ROOT, but to optimize it and offer users an option to select only  the parts of the ROOT to  be built. Partial  builds for ROOT can allow the creation of “edition” builds if necessary.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/7qwRtT1i/",
          "proposal_id": null,
          "short_description": "ROOT is a framework for data processing, born at CERN, at the heart of the research on high-energy, molecular and laser physics, as well as in...",
          "slug": "superbuilds-for-root",
          "status": "completed",
          "student_name": "psvirin",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Superbuilds for ROOT"
        },
        {
          "code_url": "https://yashnator.github.io/rootio-jl",
          "description": "ROOT is CERN's data analysis framework, which is widely used for HEP applications. Currently, the UnROOT package supports reading the ROOT files in Julia. However, there is no support for writing into ROOT files, meaning users have to write to the ROOT files independently. This can be a pretty tedious task involving references and pointers, which is not user-friendly.\n\nThe project aims to provide a write interface to the Julia community, allowing the ROOT users to experience the performance benefits of Julia and Julia users to use ROOT for HEP & data analysis without worrying about I/O.\n\nThe project goal will be achieved by providing a baseline Julia package and four feature packages allowing users to write any data structure to the ROOT file. The interface will be safe, user-friendly and have a Julia-like interface.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/Qy0aJ5SQ/",
          "proposal_id": null,
          "short_description": "ROOT is CERN's data analysis framework, which is widely used for HEP applications. Currently, the UnROOT package supports reading the ROOT files in...",
          "slug": "julia-interoperating-with-hep-c-libraries",
          "status": "completed",
          "student_name": "Yash Solanki",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Julia interoperating with HEP C++ libraries"
        },
        {
          "code_url": "https://gist.github.com/yrs147/bded65888902e45246ba34d9acda553a",
          "description": "The project aims to enhance CERN Storage's backup orchestration system, CBACK, by introducing a Scheduler Agent to dynamically manage job agent creation based on service demand. Leveraging database triggers for real-time responsiveness, the Scheduler Agent will analyze changes in the internal database and spawn agents as needed, boosting scalability. Moreover, modifications will enable containerization of existing agents, facilitating seamless integration with Kubernetes-based environments. Additionally, enhancements will enforce scope limitations within file mounts using cephx keys to strengthen security and control over data access. Comprehensive documentation of all changes will ensure smooth adoption and maintenance.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/uO9QhnX7/",
          "proposal_id": null,
          "short_description": "The project aims to enhance CERN Storage's backup orchestration system, CBACK, by introducing a Scheduler Agent to dynamically manage job agent...",
          "slug": "containerization-of-cback-backup-system",
          "status": "completed",
          "student_name": "YashrajSingh",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "database",
            "kubernetes"
          ],
          "title": "Containerization of CBACK backup system"
        },
        {
          "code_url": "https://gist.github.com/MihailMihov/c5d4eddd2d14f5ec7b61abe2d66c9866",
          "description": "Clad is a Clang library which allows automatic differentiation (AD) of functions. Clang provides\r\nthe necessary facilities for code transformation. The AD library can differentiate non-trivial\r\nfunctions, to find a partial derivative for trivial cases. Newer C++ versions provide the constexpr and consteval specifiers, but currently Clad does not abide by them. The aim of this proposal is to ensure that Clad generated derivative functions follow the semantics of the original functions. Adding support will likely require a new interface for CladFunction's and changes in clad::differentiate and clad::gradient.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/AmHhalqN/",
          "proposal_id": null,
          "short_description": "Clad is a Clang library which allows automatic differentiation (AD) of functions. Clang provides the necessary facilities for code transformation....",
          "slug": "add-support-for-consteval-and-constexpr-functions-in-clad",
          "status": "completed",
          "student_name": "Mihail Mihov",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Add support for consteval and constexpr functions in clad"
        },
        {
          "code_url": "https://gist.github.com/YBelikov/1052b42dd9b4244e7b8a6f6dd67545be",
          "description": "The project focuses on integration of FUSE-T kextless module for CVMFS on macOS. This initiative is motivated by the computational demands of particle physics, especially those associated with the Large Hadron Collider, which necessitates efficient and reliable software deployment across a massive, globally distributed computing grid. CVMFS (CernVM File System) plays a pivotal role in this process, serving as an optimized, POSIX-compliant read-only file system designed for user space via FUSE modules, thus facilitating software distribution.\nI propose replacing the current MacFUSE module as kernel extensions facing deprecation by Apple, with FUSE-T, a user-space module claimed to be a drop-in replacement. This transition addresses the issues with kernel extensions (kexts) and aligns with Apple's shift towards prohibition of support for them.  Such approach potentially seems to be the most user-friendly in terms of security.\nThe project involves various stages, including adapting CVMFS to use FUSE-T, benchmarking file operations, and investigating libfuse's new features like FUSE_CAP_SPLICE_MOVE. \nDeliverables include the successful integration of FUSE-T into CVMFS, benchmark reports on file operations, and an evaluation of libfuse updates.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/QgW8LR3a/",
          "proposal_id": null,
          "short_description": "The project focuses on integration of FUSE-T kextless module for CVMFS on macOS. This initiative is motivated by the computational demands of...",
          "slug": "cvmfs-benchmarking-of-new-fuse-features",
          "status": "completed",
          "student_name": "Yuriy Belikov",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "CVMFS: Benchmarking of new FUSE features"
        },
        {
          "code_url": "https://github.com/Ish-D/AtlEventProcess",
          "description": "The goal of this project is to study the performance and effectiveness of various compression algorithms, specifically on ATLAS RAW data. The ATLAS experiment produces extremely large amounts of data, and it is only expected to increase with future planned upgrades within the LHC. Prior studies into compression of the data has shown that due to the highly redundant nature of the generated data, lossless data compression algorithms are extremely effective in reducing the binary size of ATLAS data. Here, we would like to find an algorithm that has a good balance of compression time, and compressed binary size.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/2kr4E3Fh/",
          "proposal_id": null,
          "short_description": "The goal of this project is to study the performance and effectiveness of various compression algorithms, specifically on ATLAS RAW data. The ATLAS...",
          "slug": "lossless-compression-of-raw-data-for-the-atlas-experiment-at-cern",
          "status": "completed",
          "student_name": "Ishan Darji",
          "student_profile": null,
          "tags": [],
          "title": "Lossless compression of raw data for the ATLAS experiment at CERN"
        },
        {
          "code_url": "https://gist.github.com/kchristin22/13119e994fb9bcae7f431b891c812de0#file-gsoc-cuda-kernels-autodiff-christina-koutsou-md",
          "description": "Nowadays, the rise of AI has shed light into the power of GPUs. The notion of General Purpose GPU Programming is becoming more and more popular and it seems that the scientific community is increasingly favoring it over CPU Programming. Consequently, implementation of mathematics and operations needed for such projects are getting adjusted to GPU’s architecture. \nAutomatic differentiation is a notable concept in this context, finding applications across diverse domains from ML to Finance to Physics. Clad is a clang plugin for automatic differentiation that performs source-to-source transformation and produces a function capable of computing the derivatives of a given function at compile time. This project aims to widen Clad’s use range and audience by enabling the reverse-mode automatic differentiation of CUDA kernels.\nThe total goal of the project is to support the differentiation of CUDA kernels that may also include typical CUDA built-in objects (e.g. threadIdx, blockDim etc.), which are employed to prevent race conditions, using Clad. These produced kernels will compute the derivative of an argument specified by the user as the output based on an input parameter of their choosing. In addition, the user must be able to call these kernels with a custom grid configuration.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/6ek5ahwH/",
          "proposal_id": null,
          "short_description": "Nowadays, the rise of AI has shed light into the power of GPUs. The notion of General Purpose GPU Programming is becoming more and more popular and...",
          "slug": "reverse-mode-automatic-differentiation-of-gpu-kernels-using-clad",
          "status": "completed",
          "student_name": "Christina Koutsou",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui"
          ],
          "title": "Reverse-mode automatic differentiation of GPU kernels using Clad"
        },
        {
          "code_url": "https://github.com/ganga-devs/ganga-ai",
          "description": "Ganga is a tool for composing, running, and tracking computing jobs across a variety of backends and application types. Ganga primarily runs as a command-line tool and has an IPython-like prompt. My proposal lays out a plan to add a large language model to the Ganga prompt. It discusses options like cloud serving, local serving, and the challenges that come with each. It also addresses whether a rag system would make sense for the project at this juncture. Finally, I suggest a timeline breakup where I first focus on implementing a local solution followed by a cloud one.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/0E5hVUFg/",
          "proposal_id": null,
          "short_description": "Ganga is a tool for composing, running, and tracking computing jobs across a variety of backends and application types. Ganga primarily runs as a...",
          "slug": "incorporating-a-large-language-model-in-ganga-to-assist-users",
          "status": "completed",
          "student_name": "Aryabhatta Dey",
          "student_profile": null,
          "tags": [
            "python",
            "cloud",
            "backend"
          ],
          "title": "Incorporating a Large Language Model in Ganga to assist users"
        },
        {
          "code_url": "https://github.com/doorkn-b/GSoC-24-PDF-Dashboard",
          "description": "At the Large Hadron Collider (LHC), protons collide at the highest energies achieved by humanity, unravelling the particles within. To decode these collisions, scientists rely on parton density functions (PDFs), which shed light on the proton's internal makeup. \n\nThese PDFs are called millions of times in the creation of every simulated event dataset at the LHC, highlighting the necessity for improved visualization tools. Given that PDFs play a critical role yet introduce significant uncertainty in LHC research, there's a need for tools that offer clearer and more precise analysis. \n\nWe propose to develop a web dashboard and a live plotting tool focused on vizualizing PDFs. This setup will facilitate the comparison of various PDF fits and their uncertainty margins, which are crucial for the accurate interpretation of LHC results. \nOur motivation is driven by the dual role of PDFs: as indispensable tools for simulation in LHC research and as major sources of uncertainty. \n\nEnhancing visualization capabilities will allow physicists to deeply explore the differences between PDF fits and their uncertainty-variations, aiding in more accurate interpretations of LHC data.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/h0TLIWfe/",
          "proposal_id": null,
          "short_description": "At the Large Hadron Collider (LHC), protons collide at the highest energies achieved by humanity, unravelling the particles within. To decode these...",
          "slug": "mcnetlhapdf-online-dashboard-and-data-visualisation-for-parton-density-functions",
          "status": "completed",
          "student_name": "Arnab Mukherjee",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "(MCnet/LHAPDF) Online dashboard and data-visualisation for parton density functions"
        },
        {
          "code_url": "https://gist.github.com/guiyrt/1402fcf43c0d55baeb3d4671004a47a0",
          "description": "The calorimeter, a key detector in Large Hadron Collider (LHC) experiments, measures the energy of particles interacting with detector materials. Particles emerging from collisions create cascades of secondary particles, or showers. Describing these processes requires precise simulation methods like the Geant4 toolkit. Machine learning (ML) techniques, such as generative modeling, offer a promising alternative. Recently, the Fast Calorimeter Simulation Challenge (CaloChallenge) spurred the development and evaluation of different models. Also, in High-Energy Physics (HEP), there has been an increasing interest in using Julia as a language for software development, for combining the ease of programming of interactive languages, e.g. Python, with the speed of compiled languages, e.g. C++. This project targets to assess Julia's machine learning ecosystem's maturity, in terms of availability and robustness of libraries and tools for deep learning. To this end, a selection of models from the CaloChallenge will be chosen for implementation in Julia, as well as the required functionality to enable training and evaluation of the networks. Ultimately, a comprehensive analysis will be conducted between Python/C++ and Julia implementations, considering training time, inference time and evaluation metrics from CaloChallenge.",
          "difficulty": null,
          "id": "proj_cern-hsf_2024_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/2ivdEHkk/",
          "proposal_id": null,
          "short_description": "The calorimeter, a key detector in Large Hadron Collider (LHC) experiments, measures the energy of particles interacting with detector materials....",
          "slug": "machine-learning-in-julia-for-calorimeter-showers",
          "status": "completed",
          "student_name": "Daniel Regado",
          "student_profile": null,
          "tags": [
            "python",
            "ml",
            "ai",
            "ui"
          ],
          "title": "Machine Learning in Julia for Calorimeter Showers"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2024/organizations/cern-hsf/"
    },
    "year_2025": {
      "num_projects": 26,
      "projects": [
        {
          "code_url": null,
          "description": "This project proposes to enhance Clad, a Clang-based automatic differentiation (AD) tool, with support for NVIDIA's Thrust library. By enabling differentiation of Thrust's GPU-parallel algorithms, Clad users will gain the ability to automatically generate gradients for CUDA-accelerated code in scientific computing and machine learning applications. The implementation will include extending Clad's source-to-source transformation engine to recognize Thrust primitives (e.g., transform, reduce), implement custom derivatives, and validate performance through real-world use cases. This work will bridge the gap between high-performance GPU computing and AD, potentially accelerating gradient-based optimization tasks by orders of magnitude.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/pKQZ4KcQ",
          "proposal_id": "0N7LLnk6",
          "short_description": "This project proposes to enhance Clad, a Clang-based automatic differentiation (AD) tool, with support for NVIDIA's Thrust library. By enabling...",
          "slug": "support-usage-of-thrust-api-in-clad",
          "status": "in-progress",
          "student_name": "Abdelrhman Elrawy",
          "student_profile": null,
          "tags": [
            "api",
            "ai"
          ],
          "title": "Support usage of Thrust API in Clad"
        },
        {
          "code_url": null,
          "description": "This proposal outlines integrating debugging into the xeus-cpp kernel for Jupyter using LLDB and its Debug Adapter Protocol (lldb-dap). Modeled after xeus-python, it leverages LLDB’s Clang and JIT debugging support to enable breakpoints, variable inspection, and step-through execution. The modular design ensures compatibility with Jupyter’s frontend, enhancing interactive C++ development in notebooks.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/uEB2as1n",
          "proposal_id": "ITylGX94",
          "short_description": "This proposal outlines integrating debugging into the xeus-cpp kernel for Jupyter using LLDB and its Debug Adapter Protocol (lldb-dap). Modeled after...",
          "slug": "implementing-debugging-support-for-xeus-cpp",
          "status": "in-progress",
          "student_name": "Abhinav B Kumar",
          "student_profile": null,
          "tags": [
            "python",
            "frontend"
          ],
          "title": "Implementing Debugging Support for xeus-cpp"
        },
        {
          "code_url": null,
          "description": "The project aims to enhance the CICADA anomaly detection system in the Compact Muon Solenoid (CMS) Level-1 Trigger (L1T) by integrating highly granular quantization (HGQ) techniques, which allow automatic, per-weight and per-bias quantization, providing finer control over model precision and resource usage. CICADA currently uses a distilled neural network model with layer-wise quantization to meet the strict sub-500ns latency required for the CMS L1T. This project involves developing a HGQ version of the CICADA distilled model and evaluating its performance and resource consumption when deployed on a Field Programmable Gate Array (FPGA), while contributing to the open-source CICADA, HGQ2 and hls4ml frameworks as needed. Notably, the HGQ2 library is also compatible with JAX, opening the door for future integration with modern, hardware-friendly ML ecosystems. These contributions will benefit the broader fast machine learning community focused on deploying ML models in low-latency, resource-constrained environments.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/b1JQ9zmB",
          "proposal_id": "ZtMEPDmX",
          "short_description": "The project aims to enhance the CICADA anomaly detection system in the Compact Muon Solenoid (CMS) Level-1 Trigger (L1T) by integrating highly...",
          "slug": "highly-granular-quantization-for-cicada",
          "status": "in-progress",
          "student_name": "Abhishikth Mallampalli",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui"
          ],
          "title": "Highly Granular Quantization for CICADA"
        },
        {
          "code_url": null,
          "description": "Automatic Differentiation (AD) is a computational technique that enables efficient and precise evaluation of derivatives for functions expressed in code. Clad is a Clang-based automatic differentiation tool that transforms C++ source code to compute derivatives efficiently. A crucial component for AD in Clad is the tape, a stack-like data structure that stores intermediate values for reverse mode AD. This project aims to optimize and generalize the Clad tape to improve its efficiency, introduce multilayer storage, enhance thread safety, and enable CPU-GPU transfer.",
          "difficulty": "medium",
          "id": "proj_cern-hsf_2025_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/PI3FsJoG",
          "proposal_id": "EVnl2iGV",
          "short_description": "Automatic Differentiation (AD) is a computational technique that enables efficient and precise evaluation of derivatives for functions expressed in...",
          "slug": "implement-and-improve-an-efficient-layered-tape-with-prefetching-capabilities",
          "status": "in-progress",
          "student_name": "Aditi Milind Joshi",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Implement and improve an efficient, layered tape with prefetching capabilities"
        },
        {
          "code_url": null,
          "description": "This project will improve how we store and analyze DNA sequence data by using ROOT's newer RNTuple format. We'll work in two main steps: First, we'll set up the current system and make it faster by improving search methods, processing data in batches, and using memory more efficiently. Then, we'll build a complete new system using RNTuple, which stores data in columns rather than rows. This includes creating tools to convert between common DNA data formats and RNTuple, splitting files in smart ways, and building fast search tools. Our goal is to make DNA data analysis much faster and able to handle the growing amount of sequence data that scientists are generating. This could become a new standard for DNA data storage that works better than current methods.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/DlfupPVR",
          "proposal_id": "g6ad9glW",
          "short_description": "This project will improve how we store and analyze DNA sequence data by using ROOT's newer RNTuple format. We'll work in two main steps: First, we'll...",
          "slug": "using-root-in-the-field-of-genome-sequencing",
          "status": "in-progress",
          "student_name": "Aditya18",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Using ROOT in the field of genome sequencing"
        },
        {
          "code_url": null,
          "description": "At the end of Moore’s law, building domain-specific computer architectures is considered the next step in improving program efficiency. Hls4ml is an open-source project that translates machine learning (ML) models to high-level synthesis (HLS) code for deployment on hardware accelerators. The idea stems from the high-energy physics community at CERN. Google XLS (Accelerated Hardware Synthesis) is a novel framework that implements an HLS toolchain to produce synthesizable code for FPGA and ASIC applications. Thus, XLS can be integrated as one of the backends in hls4ml for transforming ML models into synthesizable code.\n\nThe goal of this project is to integrate an XLS-based backend for the hls4ml framework. In doing so, hls4ml benefits from improved vendor compatibility and portability while also offering the potential to increase hardware efficiency. During development, we will benchmark performance and resource utilization against the existing backends (e.g., AMD Vivado, Intel Quartus). Furthermore, XLS provides a domain-specific language (DSL) called DSLX and an optimized compiler that can reduce compilation time.\n\nThe expected results are an XLS backend prototype and benchmarks performed on various metrics for the synthesized code. Everything will be documented to facilitate use and future developments.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_006",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/Mv9bV0rM",
          "proposal_id": "sU1tNGVJ",
          "short_description": "At the end of Moore’s law, building domain-specific computer architectures is considered the next step in improving program efficiency. Hls4ml is an...",
          "slug": "integrating-support-for-google-xls-in-hls4ml",
          "status": "in-progress",
          "student_name": "Andrei Girjoaba",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "ui",
            "backend"
          ],
          "title": "Integrating Support for Google XLS in hls4ml"
        },
        {
          "code_url": null,
          "description": "The Large Hadron Collider (LHC) hosts high-energy physics (HEP) experiments, such as ATLAS, which generate over 1 petabyte of data every second. However, due to bandwidth and storage limitations, only a fraction of the produced data can be stored. To ensure that CERN’s scientists can benefit from having as much data as possible, lossy compression techniques can be considered as a possible solution to CERN’s high data volume challenge. A promising contender in data compression techniques is “BALER,” a machine-learning-based compression tool under development by the particle physics divisions of the University of Manchester. Through the utilization of autoencoder neural networks, significant levels of compression while maintaining high accuracy have been achieved for a variety of datasets, including highly-dimensional LHC datasets and 2D images.\n\nThrough Google Summer of Code 2025, I plan to collaborate with HSF, Manchester, and BALER to enhance their current compression model, tailoring it for physics-specific experimental data, enabling high-fidelity reconstruction for scientific investigation and analysis. Specifically, I plan to implement a physics-informed autoencoder model and evaluate its performance on open LHC datasets. Leveraging my past experience with Tensorized Physics-Informed Neural Networks, I aim to integrate physical laws directly into the autoencoder architecture and loss function, ensuring that compressed representations maintain physical validity. Furthermore, comparing reconstructed data against the original collected data will highlight outliers and physical abnormalities. Finally, I plan to document all experiments and evaluate the physics-informed autoencoder against BALER’s current performance when compressing real-world physics experiment data. With aid from my mentors, I hope to deploy this specialized version of BALER’s tool for CERN and other groups who could benefit from powerful physics-aware data compression.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_007",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/8p9T1Cnr",
          "proposal_id": "agAa2xpV",
          "short_description": "The Large Hadron Collider (LHC) hosts high-energy physics (HEP) experiments, such as ATLAS, which generate over 1 petabyte of data every second....",
          "slug": "physics-constrained-autoencoders-intelligent-compression-in-high-energy-physics",
          "status": "in-progress",
          "student_name": "Carter Capetz",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Physics-Constrained Autoencoders: Intelligent Compression in High Energy Physics"
        },
        {
          "code_url": null,
          "description": "Physicists at the LHC need to perform ML inference on massive amounts of data. Currently, the bookkeeping of ML model files is an unsolved problem. The goal of this project is to evaluate the CernVM File System (CVMFS) as a platform to store, organize, and distribute model files. The two primary issues to tackle are latency and infrastructure. \n\nWe know there is a latency with using CVMFS versus local storage, so I will rigorously benchmark the performance of CVMFS and determine ways to minimize this overhead. For infrastructure, I will test integrating different services with CVMFS such as Cern Document Server (CDS) or Kubeflow's KServe. \n\nThe final deliverables will include extensive documentation on benchmarks and best-practices for using CVMFS, and an overall evaluation of CVMFS and the possible integrations like KServe. An example deployment of ML models from the ML4EP project using CVMFS will also be demonstrated.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_008",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/xAitvXdz",
          "proposal_id": "6Xlmaf5u",
          "short_description": "Physicists at the LHC need to perform ML inference on massive amounts of data. Currently, the bookkeeping of ML model files is an unsolved problem....",
          "slug": "evaluating-cvmfs-for-machine-learning-model-distribution",
          "status": "in-progress",
          "student_name": "jasonwu224",
          "student_profile": null,
          "tags": [
            "ml"
          ],
          "title": "Evaluating CVMFS for Machine Learning Model Distribution"
        },
        {
          "code_url": null,
          "description": "This project aims to extend Clad, a Clang-based automatic differentiation tool for C++, to support OpenMP programs. By enabling Clad to parse and differentiate OpenMP directives such as parallel for, reduction, and atomic, we will allow gradient computation in multi-threaded environments. The implementation will include enhancements to Clad’s AST parsing, variable scope analysis, and differentiation logic for both forward and reverse modes. Deliverables include robust OpenMP AD support, comprehensive tests, and user documentation.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_009",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/XboSFIbL",
          "proposal_id": "X01O6Wg2",
          "short_description": "This project aims to extend Clad, a Clang-based automatic differentiation tool for C++, to support OpenMP programs. By enabling Clad to parse and...",
          "slug": "enable-automatic-differentiation-of-openmp-programs-with-clad",
          "status": "in-progress",
          "student_name": "Jiayang Li",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Enable automatic differentiation of OpenMP programs with Clad"
        },
        {
          "code_url": null,
          "description": "This project aims to enhance the BEAD framework for anomaly detection in high-energy physics by developing a new multi-stage architecture that combines a contrastive variational autoencoder (VAE) with a self-supervised transformer and multi-task fine-tuning layers. The key goal is to learn generator-invariant representations from 4-momentum vectors of background events by pretraining the VAE using generator labels (e.g., Pythia, Herwig, Sherpa). The latent variables produced are then masked and enriched with high-level physics features (e.g., MET, HT, Meff) and passed through a transformer trained to reconstruct them, encouraging deeper physics representation learning.\n\nThe enhanced latent space is decoded using fine-tuning layers that perform multiple tasks simultaneously: reconstructing event-level features, predicting jet properties, and regressing unseen quantities. This structure encourages collaboration between the VAE and transformer components, increasing robustness against spurious correlations and improving anomaly detection performance.\n\nDeliverables will include:\n- A modular implementation of the full architecture within BEAD\n- Integration with tools like Weights & Biases for experiment tracking\n- Docker support for reproducibility\n- Evaluation benchmarks on synthetic datasets\n- Final documentation and presentation of findings at the CERN ML Forum",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_010",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/6RCE7sFt",
          "proposal_id": "nwNyBlkV",
          "short_description": "This project aims to enhance the BEAD framework for anomaly detection in high-energy physics by developing a new multi-stage architecture that...",
          "slug": "background-enrichment-augmented-anomaly-detection-bead-contrastive-vae-transformer-architecture",
          "status": "in-progress",
          "student_name": "JohnKala",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "docker"
          ],
          "title": "Background Enrichment augmented Anomaly Detection (BEAD)– Contrastive VAE & Transformer Architecture"
        },
        {
          "code_url": null,
          "description": "The Large Hadron Collider (LHC) particle track reconstruction faces increasing computational demands, driving exploration into novel methods like Quantum Computing QC. However, evaluating these requires assessing factors such as performance and sustainability. This project aims at developing a modular Python benchmarking framework to systematically compare classical algorithms e.g., Kalman Filter-based, GNN-based against quantum approaches e.g., QAOA, VQC simulated via PennyLane for tracking tasks using TrackML dataset. The framework will evaluate algorithms based on physics performance efficiency, purity, computational cost runtime, memory, quantum resource requirements qubits, depth, and sustainability indicators simulated/projected energy consumption. The key deliverable is the framework itself, providing the community with a tool for quantitative comparison, understanding performance/sustainability trade-offs, and informing future directions in deploying potentially sustainable computing solutions for High Energy Physics.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_011",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/kBQyVgrm",
          "proposal_id": "ptspTebl",
          "short_description": "The Large Hadron Collider (LHC) particle track reconstruction faces increasing computational demands, driving exploration into novel methods like...",
          "slug": "benchmarking-sustainability-of-classical-quantum-algorithms-for-particle-trajectory-reconstruction",
          "status": "in-progress",
          "student_name": "KaranSingh",
          "student_profile": null,
          "tags": [
            "python",
            "ml",
            "ai",
            "ui"
          ],
          "title": "Benchmarking Sustainability of Classical & Quantum Algorithms for particle trajectory reconstruction"
        },
        {
          "code_url": null,
          "description": "This project aims to introduce a structured, automated deprecation system for the Ganga project. Ganga currently lacks a formal process for phasing out legacy APIs and outdated code, resulting in accumulated technical debt and maintenance difficulties. The proposed solution includes decorator-based deprecation utilities (leveraging PEP 702), CI workflows to track expiry deadlines, and tools to detect regressions using deprecated features in pull requests. Deliverables also include test coverage and tooling for maintainers to enforce clean removals. This will improve maintainability, reduce clutter, and enhance developer experience across the project.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_012",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/hIrlU5CX",
          "proposal_id": "G1JjxL6e",
          "short_description": "This project aims to introduce a structured, automated deprecation system for the Ganga project. Ganga currently lacks a formal process for phasing...",
          "slug": "implementing-a-deprecation-system-for-ganga",
          "status": "in-progress",
          "student_name": "Kossi GLOKPOR",
          "student_profile": null,
          "tags": [
            "api",
            "ai"
          ],
          "title": "Implementing a deprecation system for Ganga"
        },
        {
          "code_url": null,
          "description": "This project addresses the lack of RNTuple support in JSROOT, a critical gap hindering browser-based visualization and analysis of high-energy physics (HEP) data. \n\nMy Solution:\nBy developing a JavaScript parser for RNTuple’s binary format, extending JSROOT’s interface to mirror its `TSelector` API for TTrees, and integrating visualization tools (histograms, scatter plots), researchers will gain seamless access to RNTuple data directly in web browsers. \n\nKey deliverables include:  \n\n1. Validated Parser: Tested against ROOT-generated datasets (via C++ macros) for accuracy.  \n\n2. JSROOT Integration: A `TRootupleSelector` interface enabling reuse of existing visualization modules.  \n\n3. Interactive Visualizations: Histograms, scatter plots, and UI features (tooltips, zoom) for real-time data exploration.  \n\n4. Optimized Performance: Stress-tested with 1M+ entries, cross-browser compatibility (Chrome, Firefox, Safari, Edge).  \n\nThis work will bridge the gap between RNTuple and browser-based HEP analysis, enhancing accessibility for global researchers.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_013",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/JJDvSbP6",
          "proposal_id": "MzazMujD",
          "short_description": "This project addresses the lack of RNTuple support in JSROOT, a critical gap hindering browser-based visualization and analysis of high-energy...",
          "slug": "rntuple-in-jsroot",
          "status": "in-progress",
          "student_name": "Kriti Mahajan",
          "student_profile": null,
          "tags": [
            "java",
            "javascript",
            "web",
            "api",
            "ml"
          ],
          "title": "RNTuple in JSROOT"
        },
        {
          "code_url": null,
          "description": "The CernVM File System - CVMFS distributes LHC experiment software and conditions data to the LHC computing infrastructure. The client application is a read-only filesystem in userspace - a FUSE module, and may be sensitive to the different versions of linux kernel. Currently the integration test suite of CVMFS is run on different linux distributions and platforms and has coverage over a handful of kernel versions. The goal of this project is to increase this coverage and allow systematic testing for specific kernel versions and features.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_014",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/oh10qRfo",
          "proposal_id": "7DTXdxrr",
          "short_description": "The CernVM File System - CVMFS distributes LHC experiment software and conditions data to the LHC computing infrastructure. The client application is...",
          "slug": "extending-support-on-custom-kernels-with-virtme-ng",
          "status": "in-progress",
          "student_name": "Madlani Shivam",
          "student_profile": null,
          "tags": [
            "ui",
            "ux"
          ],
          "title": "Extending support on custom kernels with virtme-ng"
        },
        {
          "code_url": null,
          "description": "GPUs and CUDA have revolutionized computing beyond graphics rendering, becoming essential in various fields due to their parallel processing capabilities. Clad, a Clang plugin, provides automatic differentiation for C++ functions by modifying Abstract-Syntax-Tree using LLVM compiler features. While Clad supports reverse-mode differentiation of CUDA kernels, data-race conditions are very frequent and considerably slow down the execution. Thread Safety Analysis helps detect data races to optimize performance by potentially reducing atomic operations in generated code.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_015",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/XSdlsEEe",
          "proposal_id": "YrIukEYZ",
          "short_description": "GPUs and CUDA have revolutionized computing beyond graphics rendering, becoming essential in various fields due to their parallel processing...",
          "slug": "implement-activity-analysis-for-reverse-mode-differentiation-of-cuda-gpu-kernels",
          "status": "in-progress",
          "student_name": "Maksym Andriichuk",
          "student_profile": null,
          "tags": [],
          "title": "Implement activity analysis for reverse-mode differentiation of (CUDA) GPU kernels"
        },
        {
          "code_url": null,
          "description": "As a final-year Software Engineering student at Kyiv Polytechnic Institute, I propose transforming Rucio WebUI into a more powerful, intuitive, and sustainable platform. Drawing from my previous contributions and technical expertise, I've developed a comprehensive strategy that addresses current limitations while setting the foundation for future growth.\n\nMy plan focuses on migrating to modern technologies (Next.js 15, React 19, TailwindCSS 4.x), implementing server-side rendering for improved performance, enhancing the authentication system with NextAuth.js and RBAC, and fixing critical bugs like DID slash-handling issues and non-functional replicas loading. I'll also optimize performance through advanced streaming mechanisms for large data volumes, improve architecture with a transition to NX monorepo, enhance entity management functionalities, and implement comprehensive UI/UX refinements.\n\nWith substantial experience in the Rucio WebUI codebase from my previous internship and 30+ contributions to the project, I've demonstrated my commitment and technical ability to execute this comprehensive modernization plan. My current work with Next.js and NX at a startup provides me with cutting-edge expertise that I can directly apply to enhance the project's development and implementation.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2025_016",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/IzibloUu",
          "proposal_id": "GFlBDJTV",
          "short_description": "As a final-year Software Engineering student at Kyiv Polytechnic Institute, I propose transforming Rucio WebUI into a more powerful, intuitive, and...",
          "slug": "rucio-webui-revamp",
          "status": "in-progress",
          "student_name": "MytsV",
          "student_profile": null,
          "tags": [
            "react",
            "web",
            "ai",
            "ui",
            "ux"
          ],
          "title": "Rucio WebUI Revamp"
        },
        {
          "code_url": null,
          "description": "The nopayloaddb tool works as an implementation of the conditions database reference for the HSF. The project is based upon Nginx, Django, and the PostgreSQL database. The tool also includes RESTful APIs and Helm charts to provide easier development and integration. However, the tool lacks centralized logging to effectively monitor and debug issues. \n\nThis project aims to implement intelligent logging software that can enhance observability and performance. The solution will aggregate logs from multiple sources into a centralized logging system. It will ensure streamlined monitoring and use machine learning to analyze logs for potential issues, inefficiencies, or failures.\n\nBy detecting anomalies, the system will help address problems before they escalate and will provide insights for optimal parameter adjustments during high request loads. Comprehensive documentation and an integration guide will be provided for long-term maintainability. In conclusion, this solution will enhance the reliability, efficiency, and scalability of the nopayloaddb tool.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_017",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/bD4yyXz6",
          "proposal_id": "jE3QNFwX",
          "short_description": "The nopayloaddb tool works as an implementation of the conditions database reference for the HSF. The project is based upon Nginx, Django, and the...",
          "slug": "intelligent-log-analysis-for-the-hsf-conditions-database",
          "status": "in-progress",
          "student_name": "Osama Tahir",
          "student_profile": null,
          "tags": [
            "api",
            "ml",
            "ai",
            "database",
            "ui"
          ],
          "title": "Intelligent Log Analysis for the HSF Conditions Database"
        },
        {
          "code_url": null,
          "description": "The project addresses a limitation in Clad, a Clang plugin for C++ automatic differentiation, which currently cannot fully support non-trivially copyable types, restricting its use in Object-Oriented Programming. I am going to enhance Clad's reverse mode differentiation by modifying its forward sweep functions to store intermediate values effectively and improving the To-Be-Recorded analysis to handle nested function calls and pointer operations. Deliverables include support for non-copyable types, enhanced TBR analysis, and optimized memory usage. These improvements will significantly extend Clad's applicability in modern C++ codebases using object-oriented design patterns.",
          "difficulty": "medium",
          "id": "proj_cern-hsf_2025_018",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/78v5Tl8E",
          "proposal_id": "qbJAi82F",
          "short_description": "The project addresses a limitation in Clad, a Clang plugin for C++ automatic differentiation, which currently cannot fully support non-trivially...",
          "slug": "improve-automatic-differentiation-of-object-oriented-paradigms-using-clad",
          "status": "in-progress",
          "student_name": "Petro Zarytski",
          "student_profile": null,
          "tags": [],
          "title": "Improve automatic differentiation of object-oriented paradigms using Clad"
        },
        {
          "code_url": null,
          "description": "CLAD is a compiler plugin that enables automatic differentiation of C++ functions. It supports differentiation of user-defined structs, however it struggles with complex structures/classes. It  does not account for functions with side effects either. Though this could be aided with the use of custom derivatives, it would still be useful to have built-in support of some of those complex structures and functions with side effects. One of the most useful ones is std::thread, which is the thread class from STL. Utilizing parallelism is very important for modern numerical methods and, as such, having support of std::thread and other STL concurrency primitives would be a useful feature for CLAD.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2025_019",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/2CTNxjSY",
          "proposal_id": "vPtaBm11",
          "short_description": "CLAD is a compiler plugin that enables automatic differentiation of C++ functions. It supports differentiation of user-defined structs, however it...",
          "slug": "integration-of-stl-concurrency-primitives-into-clad",
          "status": "in-progress",
          "student_name": "pmozil",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Integration of STL concurrency primitives into CLAD"
        },
        {
          "code_url": null,
          "description": "The SOFIE (System for Optimized Fast Inference Code Emit) project is an initiative within the TMVA (Toolkit for Multivariate Data Analysis) framework in ROOT, which aims to enhance the efficiency and speed of inference on Machine Learning Models. SOFIE converts ML models trained in different frameworks such as ONNX, PyTorch, and Tensorflow, converting them into an Intermediate Representation (IR). This IR allows SOFIE to generate optimized C++ functions for fast and effective inference of neural networks and subsequently convert them into C++ header files, which can be used in plug-and-go style for inference. \nFor GSoC 2025, this project aims to enhance the Keras parser to support models trained in the latest TensorFlow v2.18.0, which introduces NumPy 2.0 compatibility. As well as Integrate JAX/FLAX support, enabling SOFIE to generate C++ inference functions for models developed using JAX/FLAX",
          "difficulty": "medium",
          "id": "proj_cern-hsf_2025_020",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/uAjGYhgX",
          "proposal_id": "cdBp3ctQ",
          "short_description": "The SOFIE (System for Optimized Fast Inference Code Emit) project is an initiative within the TMVA (Toolkit for Multivariate Data Analysis) framework...",
          "slug": "tmva-sofie-enhancing-keras-parser-and-jaxflax-integration",
          "status": "in-progress",
          "student_name": "Prasanna Kasar",
          "student_profile": null,
          "tags": [
            "ml",
            "ai"
          ],
          "title": "TMVA SOFIE - Enhancing Keras Parser and JAX/FLAX Integration"
        },
        {
          "code_url": null,
          "description": "Training Large Language Models (LLMs) is computationally expensive, often bottlenecked by the performance limitations of Python-based frameworks. This project addresses this challenge by enhancing LLM training efficiency within a C++ environment through the integration of Clad, a Clang/LLVM compiler plugin for automatic differentiation (AD). We will deveop a custom C++ tensor library specifically designed for optimal interaction with Clad. The core objective is to replace traditional runtime or manual gradient computations with Clad's efficient compile-time differentiation for key LLM operations within a GPT-2 training pipeline. This involves investigating effective strategies to bridge Clad's static analysis with dynamic neural network computations, benchmarking the resulting performance gains in speed and memory usage against a non-Clad baseline, and leveraging OpenMP for further parallelization.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_021",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/TqLF4fm0",
          "proposal_id": "45EdWDLv",
          "short_description": "Training Large Language Models (LLMs) is computationally expensive, often bottlenecked by the performance limitations of Python-based frameworks....",
          "slug": "enhancing-llm-training-efficiency-with-clad-for-automatic-differentiation-in-c",
          "status": "in-progress",
          "student_name": "Rohan Timmaraju",
          "student_profile": null,
          "tags": [
            "python",
            "ai"
          ],
          "title": "Enhancing LLM Training Efficiency with Clad for Automatic Differentiation in C++"
        },
        {
          "code_url": null,
          "description": "ROOT's TMVA( Toolkit for Multi-Variate Analysis ) has SOFIE (System for Optimized Fast Inference code Emit) which  offers a parser capable of converting ML models trained in Keras, PyTorch, or ONNX format into its own Intermediate Representation, and generates C++ functions that can be easily invoked for fast inference of trained neural networks. It is currently implemented for CPU inference along with a SYCL implementation.\n \n This project aims to explore different GPU stacks (such as CUDA, ROCm, ALPAKA) and implement GPU-based inference functionalities in SOFIE. Although there exist CPU implementations, working with HEP applications, need for GPU inferences become important. \n\nWe will be writing optimized GPU Kernels for various operators such as RELU, GEMM, CONV2D, etc. that SOFIE already has for CPU implementation. \n\nTesting and experimenting which tech stack (such as CUDA, ROCm, ALPAKA) will be best aligned with SOFIE is our first goal. After getting to know about the pros and cons we can decide which one aligns with our goal.\n\nOur next aim would be to integrate all operators for GPU implementation in a incremental fashion ensuring each operator gets enough testing for edge cases.\n\nOnce we have done basic implementation, we move on to optimizing thread-block config by using tools like Nsight for maximum performance.\n\nOnce we have implemented our operators for the GPU stack, there is a need for performance benchmarking against popular frameworks like Pytorch/Tensorflow.\n\nLast part of the project would be extensive testing and benchmarking memory usage and execution time.\n\nI would end the project with proper documentation on how exactly to implement and scopes for further improvements.",
          "difficulty": "medium",
          "id": "proj_cern-hsf_2025_022",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/WbEjJL5j",
          "proposal_id": "EOzEmlCA",
          "short_description": "ROOT's TMVA( Toolkit for Multi-Variate Analysis ) has SOFIE (System for Optimized Fast Inference code Emit) which offers a parser capable of...",
          "slug": "tmva-sofie-gpu-support-for-machine-learning-inference",
          "status": "in-progress",
          "student_name": "S.Akash",
          "student_profile": null,
          "tags": [
            "ml",
            "ai"
          ],
          "title": "TMVA SOFIE - GPU Support for Machine Learning Inference"
        },
        {
          "code_url": null,
          "description": "The project addresses the rising energy consumption of ML workloads in high-energy physics (HEP) at CERN. While models like Baler and ATLAS top taggers are essential for real-time data processing at the LHC, they impose significant carbon and energy costs. This proposal introduces GreenML@CERN, a holistic framework for optimizing the energy footprint of scientific ML workflows. The solution combines cross-architecture energy profiling (GPUs, CPUs, TPUs), scheduler efficiency analysis (HTCondor, Kubernetes), algorithmic optimizations (quantization, pruning, dynamic training), and an automated Jupyter-based energy profiler with a real-time dashboard.\n\nDeliverables include energy-optimized versions of CERN ML models, benchmarking datasets, a profiling toolkit, and visual dashboards. The project extends CERN's ongoing GreenDIGIT sustainability efforts with developer-focused ML tools to help the community balance performance with energy awareness—contributing directly to CERN’s 2030 carbon neutrality goals.",
          "difficulty": null,
          "id": "proj_cern-hsf_2025_023",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/r6G5rINJ",
          "proposal_id": "tw7fG9a6",
          "short_description": "The project addresses the rising energy consumption of ML workloads in high-energy physics (HEP) at CERN. While models like Baler and ATLAS top...",
          "slug": "greenmlcern-a-comprehensive-framework-for-energy-efficient-scientific-machine-learning",
          "status": "in-progress",
          "student_name": "Sakshi Kumar",
          "student_profile": null,
          "tags": [
            "ml",
            "ai",
            "kubernetes"
          ],
          "title": "GreenML@CERN – A Comprehensive Framework for Energy-Efficient Scientific Machine Learning"
        },
        {
          "code_url": null,
          "description": "This project aims to develop an agent-based simulation of CAR-T cell therapy using the BioDynaMo platform to better understand and optimize immunotherapy treatments. \nUsing BioDynaMo, this project will model key components of CAR-T therapy, including T-cell migration, tumor cell engagement, and the influence of microenvironmental factors like hypoxia. The simulation will incorporate dynamic agent behaviors for both CAR-T and tumor cells, simulate various tumor types (e.g., solid tumors, leukemia), and progressively integrate biological complexities such as chemotaxis, apoptosis, and immune evasion mechanisms.\n\nDeliverables include:\n- A complete and well-documented BioDynaMo-based simulation modeling CAR-T cell therapy.\n- Custom analysis scripts to visualize tumor shrinkage and evaluate CAR-T cell performance.\n- Performance comparisons across multiple therapeutic strategies through benchmarking.\n- A scientific-style report detailing results, insights, and potential implications.\n\nBy combining computational modeling with biological insights, this project will support researchers in designing better CAR-T strategies and understanding treatment dynamics under diverse conditions.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2025_024",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/VycIbWat",
          "proposal_id": "FPJrumUj",
          "short_description": "This project aims to develop an agent-based simulation of CAR-T cell therapy using the BioDynaMo platform to better understand and optimize...",
          "slug": "agent-based-simulation-of-car-t-cell-therapy-using-biodynamo",
          "status": "in-progress",
          "student_name": "Salvador de la Torre Gonzalez",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Agent-Based Simulation of CAR-T Cell Therapy Using BioDynaMo"
        },
        {
          "code_url": null,
          "description": "Summary:\nThis project addresses a critical computational bottleneck in high-energy physics: the simulation of particle showers in calorimeters, which consumes up to 90% of computing resources using traditional Monte Carlo methods. Modern calorimeters generate sparse, high-dimensional data that challenges machine learning approaches. I'm developing a physics-informed clustering framework that intelligently reduces this dimensionality while preserving essential physics information through two complementary algorithms: a Modified DBSCAN with CLUE elements and a Graph-Based approach that leverages simulation history. Both utilize physics-informed distance metrics incorporating spatial, energy, and time information and employ a cell-aware strategy that clusters dense regions aggressively when within detector cells while preserving structure between them.\n\nDeliverables:\nThe deliverables include an end-to-end pipeline that transforms detailed Geant4 shower data into optimized point cloud representations, two physics-preserving clustering algorithms, and an evaluation framework that quantifies how well these representations maintain critical shower properties while significantly reducing data complexity. This optimized representation will enable efficient training of generative models for fast calorimeter simulation, addressing a key challenge in high-luminosity experiments.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2025_025",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/C9arRGA2",
          "proposal_id": "tTAqtHnt",
          "short_description": "Summary: This project addresses a critical computational bottleneck in high-energy physics: the simulation of particle showers in calorimeters, which...",
          "slug": "data-representation-optimisation-for-generative-model-based-fast-calorimeter-shower-simulation",
          "status": "in-progress",
          "student_name": "Tarun_Tej_Nandi",
          "student_profile": null,
          "tags": [
            "ai",
            "cloud"
          ],
          "title": "Data Representation Optimisation for Generative Model-based Fast Calorimeter Shower Simulation"
        },
        {
          "code_url": null,
          "description": "In high-energy physics experiments such as those conducted at CERN's ATLAS project, the volume of data generated is immense, reaching up to 60 million MB per second. While lossless compression is already employed to manage this data, lossy compression—specifically of floating-point precision—offers more aggressive reductions, potentially decreasing file sizes by over 30%. However, this comes at the cost of irreversibly discarding information, raising the challenge of how to recover or approximate full-precision data for downstream analysis.\n\nThis project proposes a novel approach using deep probabilistic models to reconstruct high-precision floating-point data from aggressively compressed representations, a problem coined as precision upsampling. The goal is to explore and compare the capabilities of three classes of generative models: autoencoders, diffusion models, and normalizing flows. Each model type offers distinct advantages: autoencoders are well-studied in neural compression, diffusion models are robust to noise and excel in reconstructing multi-scale structures, and normalizing flows offer exact likelihood estimation and invertible mappings that align well with the structured nature of physical data.\n\nDeliverables include a literature review on neural precision upsampling methods, a working baseline model adapted to CERN's PHYSLITE floating-point compressed data, and a comparative study of advanced probabilistic models—including variational autoencoders, diffusion models, and normalizing flows. Evaluation metrics will also be tailored to the data and implemented. Final outputs will include reproducible code, performance benchmarks, a written technical report, and optionally a publication or integration into the ATLAS codebase.",
          "difficulty": "advanced",
          "id": "proj_cern-hsf_2025_026",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/W5UjfwMc",
          "proposal_id": "JzLqI8jH",
          "short_description": "In high-energy physics experiments such as those conducted at CERN's ATLAS project, the volume of data generated is immense, reaching up to 60...",
          "slug": "neural-decompression-for-high-energy-physics",
          "status": "in-progress",
          "student_name": "Yolanne",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Neural (De)compression for High Energy Physics"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2025/organizations/cern-hsf"
    }
  },
  "first_time": false,
  "contact": {
    "email": "hsf-gsoc-admin@googlegroups.com",
    "guide_url": "https://hepsoftwarefoundation.org/activities/gsoc.html#for-students",
    "ideas_url": "https://hepsoftwarefoundation.org/gsoc/2025/summary.html",
    "irc_channel": "https://gitter.im/HSF/HSF-GSoC?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge",
    "mailing_list": "https://hepsoftwarefoundation.org/activities/gsoc.html"
  },
  "social": {
    "blog": null,
    "discord": null,
    "facebook": null,
    "github": null,
    "gitlab": null,
    "instagram": null,
    "linkedin": null,
    "mastodon": null,
    "medium": null,
    "reddit": null,
    "slack": null,
    "stackoverflow": null,
    "twitch": null,
    "twitter": "https://twitter.com/hepsoftfound",
    "youtube": null
  },
  "meta": {
    "version": 1,
    "generated_at": "2026-01-25T15:28:52.978Z"
  }
}