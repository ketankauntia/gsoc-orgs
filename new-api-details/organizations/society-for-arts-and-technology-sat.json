{
  "id": "692251e753dd9d7326d33eb2",
  "slug": "society-for-arts-and-technology-sat",
  "name": "Society for Arts and Technology (SAT)",
  "category": "Media",
  "description": "Technological tools for creativity and industry",
  "image_url": "https://summerofcode.withgoogle.com/media/org/society-for-arts-and-technology-sat/pkgrduycyoh0wxmf-360.png",
  "img_r2_url": "https://pub-268c3a1efc8b4f8a99115507a760ca14.r2.dev/society-for-arts-and-technology-sat.webp",
  "logo_r2_url": null,
  "url": "https://sat.qc.ca",
  "active_years": [
    2022,
    2023,
    2025
  ],
  "first_year": 2022,
  "last_year": 2025,
  "is_currently_active": true,
  "technologies": [
    "python",
    "javascript",
    "opengl",
    "c++",
    "supercollider",
    "qt",
    "esp32"
  ],
  "topics": [
    "web",
    "graphics",
    "cloud",
    "multimedia",
    "accessibility",
    "computer vision",
    "Telepresence",
    "digital arts"
  ],
  "total_projects": 12,
  "stats": {
    "avg_projects_per_appeared_year": 4,
    "projects_by_year": {
      "year_2016": null,
      "year_2017": null,
      "year_2018": null,
      "year_2019": null,
      "year_2020": null,
      "year_2021": null,
      "year_2022": 3,
      "year_2023": 5,
      "year_2024": null,
      "year_2025": 4
    },
    "students_by_year": {
      "year_2016": null,
      "year_2017": null,
      "year_2018": null,
      "year_2019": null,
      "year_2020": null,
      "year_2021": null,
      "year_2022": 3,
      "year_2023": 5,
      "year_2024": null,
      "year_2025": 4
    },
    "total_students": 12
  },
  "years": {
    "year_2016": null,
    "year_2017": null,
    "year_2018": null,
    "year_2019": null,
    "year_2020": null,
    "year_2021": null,
    "year_2022": {
      "num_projects": 3,
      "projects": [
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2022-contributions/matthewwiese-satellite-face-tracking/work-product-matthewwiese/",
          "description": "The \"metaverse\" has seen exceptional interest in both tech and the wider market. However, metaverse software currently on the market, from Second Life to Horizon Worlds, either limit input to traditional input devices like mice and keyboards, or require expensive gear in the form of VR headsets and specialized controllers. In this project we aim at integrating interaction techniques that rely on human facial and body gestures, similar to these employed in high-fidelity telepresence systems like Google’s Starline, that we will apply to a hybrid presence system and with low-cost hardware to enable accessibility and affordability. This GSoC project aims to perform much needed research into using face tracking as a viable means to improve the accessibility of 3D virtual spaces, much like those found in the metaverse. The result benefits all users, whether or not they have specific needs for accessibility: improved interaction vectors and empathy channels liberate all users. Face tracking software built upon open source technologies will be integrated with Satellite/Mozilla Hubs to provide this crucial feature, enabling a practical testbed for future innovation in hybrid telepresence interaction.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2022_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/ve25fc1w/",
          "proposal_id": null,
          "short_description": "The \"metaverse\" has seen exceptional interest in both tech and the wider market. However, metaverse software currently on the market, from Second...",
          "slug": "face-tracking-to-improve-accessibility-and-interaction-in-the-metaverse-with-satellite",
          "status": "completed",
          "student_name": "Matt Wiese",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Face Tracking to Improve Accessibility and Interaction in the Metaverse with Satellite"
        },
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2022-contributions/vanshitaverma-add-support-of-unix-permission-in-the-shmdata-library/work-product-vanshitaverma/",
          "description": "The Shmdata library provides a layer to share streams of framed data between processes via shared memory. It supports any kind of data stream: it has been used with multichannel audio, video frames, 3D models, OSC messages, and various others types of data. Shmdata is server-less, but requires applications to link data streams using socket path (e.g. \"/tmp/my-shmdata-stream\"). Shmdata is very fast and allows processes to access data streams without the need for extra copies.\n\nHowever, permissions are not supported, and any stream shared on the system with Shmdata is accessible system-wide. The project will enable the possibility for a Shmdata user to configure streams permission using shared memory right setting at the API level.\n\nWhen sharing data between processes via shared memory, not all components of the data need to be visible to everyone. Different groups might need to access different data, furthermore, some data might only be needed to be visible to the owner of the files. In such cases, the permissions that would have to be implemented are combinations of 'read', 'write' and 'execute' permissions that would be given to the owner, group or common user(i.e. everyone else).\n\nPermissions would be 'none', 'execute only', 'write only', 'write and execute', 'read only', 'read and execute', 'read and write', 'read, write and execute' with corresponding values of 0-7 respectively.\n\nDELIVERABLES\n\n* Providing an option for users to restrict or openly share data streams via these file/ directory access modes: read, write and execute/traverse.\n\n* Changing data access permissions.\n\n* Documentation for the improvements made.\n\n* Incorporation of ACL to assist with Unix file permissions. (optional/if time allows).",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2022_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/xNd20Xxl/",
          "proposal_id": null,
          "short_description": "The Shmdata library provides a layer to share streams of framed data between processes via shared memory. It supports any kind of data stream: it has...",
          "slug": "add-support-of-unix-permission-in-the-shmdata-library",
          "status": "completed",
          "student_name": "Vanshita Verma",
          "student_profile": null,
          "tags": [
            "api",
            "ui"
          ],
          "title": "Add support of Unix permission in the Shmdata library"
        },
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2022-contributions/tiger-yash-webui-for-data-mapping-and-software-control/work-product-tiger-yash/",
          "description": "Open Source tools, such as those developed by SAT, have been widely used for a variety of applications by academics and independent users. The tools benefit users a lot and make their work easier, more efficient, and less stressful, but most of them need lengthy setups and installations, which are not very user-friendly. \n\nThis GSoC project attempts to integrate tools like SATIE and LivePose into a bespoke WebUI and make them accessible. The UI will be responsible for software configuration, deploying an  audio synthesizer on the back-end, etc. \n\nThe WebUI will be capable of deploying the tools (start/stop LivePose and SATIE, control Jack Audio parameters, and request reboot/service restart) and\nbasically remotely launching (and possibly integrating and mapping) SAT tools in embedded systems based on the Raspberry Pi generic computer running Linux. \n\nAny program or product's goal is to provide users with a rich and interactive experience; similarly, our WebUI will serve as an entry point to those tools with simple customizations that will make users' life much easier.\n\n\nDeliverables:\n 1. Create the Server ( Embedded system ) backend with the tools set up and operational.\n2. Create a backend for the WebUI.\n3. Connect both the backends with a suitable bridge.\n4. Develop the Frontend UI for launching and configuring the tools.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2022_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2022/projects/fhWh7het/",
          "proposal_id": null,
          "short_description": "Open Source tools, such as those developed by SAT, have been widely used for a variety of applications by academics and independent users. The tools...",
          "slug": "create-a-webui-for-data-mapping-and-software-control-in-embedded-systems",
          "status": "completed",
          "student_name": "Yash_Raj",
          "student_profile": null,
          "tags": [
            "web",
            "ui",
            "ux",
            "frontend",
            "backend"
          ],
          "title": "Create a WebUI for data mapping and software control in embedded systems"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2022/organizations/society-for-arts-and-technology-sat/"
    },
    "year_2023": {
      "num_projects": 5,
      "projects": [
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-contributions/caciliefanny/motion-capture-in-webxr/",
          "description": "Live performances and concerts promote a sense of uniqueness. It happens for two main reasons: the relationship between the artist and the public; and the relationship between the audience itself. It is a space in time where artists respond to the public as much as the public responds to the artists. Also, every person in the audience is surrounded by other people having similar feelings and sensations. \n\nSearching to create related energy and experience via live performances in Extended Reality, this proposal aims to develop a platform prototype to generate a fluid XR experience for users by reliving live performances through motion capture. The key technologies for this goal are: MediaPipe Pose, open source software for pose detection in a live streaming context, and WebXR for the development and hosting of virtual reality and augmented reality experiences on the web.\n\nLive performances have no analog due to their unique energy and feeling of shared experience. XR's experience with concerts and live performances does not necessarily wants to exist as a copy of performances in real-life, but as a proposal of different interactions. Therefore Extended Reality can create its paradigm in art and technology.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2023_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/ZjzzYxSd",
          "proposal_id": "4XGjx0h4",
          "short_description": "Live performances and concerts promote a sense of uniqueness. It happens for two main reasons: the relationship between the artist and the public;...",
          "slug": "motion-capture-in-webxr-a-search-for-the-uniqueness-in-replaying-live-performances",
          "status": "completed",
          "student_name": "Fanny",
          "student_profile": null,
          "tags": [
            "web",
            "api",
            "ai",
            "ui"
          ],
          "title": "Motion Capture in WebXR: a search for the uniqueness in replaying live performances"
        },
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-contributions/kuk1song-development-of-generalizable-satie-mappers/work-product-kuk1song/",
          "description": "SATIE(Spatial Audio Toolkit for Immersive Environments) can be used for audio spatialization. It is an audio spatialization engine programmed in SuperCollider that can render dense audio scenes to large multi-channel loudspeaker systems in real time. This lower-level audio rendering process maintains a dynamic DSP graph created and controlled via OSC messages from the external process.\n\n\nSATIE can be used not only for sound object control (location, propagation, etc.) but it can also be used for Haptic Floor dedicated to large immersive spaces. In this project, however, we focus on a function of SATIE is that implementing geometry understanding at a spatializer level. In this case, we usually apply the current version of SATIE mappers plugin to perform this function. A mapper plugin does not process audio, it can be used for provide for non-generic control and geometric computation of lower-level spatialization parameters. However, it still has limitations that need to be programmed for each use case. In order to perfect it and improve its efficiency, we need to design and implement the generalizable SATIE mappers that could get all loaded spatializers and set them at a relative distance concerning each other. In other words, it is necessary to make each different spatializer in different positions recognize the relative position of each other. The proposed mappers will be designed to overcome these limitations and will be tested with different spatializer configurations. This project will provide SATIE with a powerful and flexible tool that will enable users to create rich and diverse sound spatial effects in different scenarios.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2023_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/JoEdoUzG",
          "proposal_id": "1SiBxJo7",
          "short_description": "SATIE(Spatial Audio Toolkit for Immersive Environments) can be used for audio spatialization. It is an audio spatialization engine programmed in...",
          "slug": "development-of-generalizable-satie-mappers-that-optimized-audio-spatialization-flexibility",
          "status": "completed",
          "student_name": "Haokun Song",
          "student_profile": null,
          "tags": [
            "ios",
            "ai"
          ],
          "title": "Development of Generalizable SATIE Mappers that Optimized Audio Spatialization Flexibility"
        },
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-contributions/maxw3llgm/work-product-maxw3llgm-audio-to-haptic-interaction-design-with-feelix-supporting-torquetuner/",
          "description": "Haptics, as a multidisciplinary field, has applications within the Medical, Consumer and Entertainment domains, to name a few, and despite their widespread presence in our phones to provide the user with a vibrotactile feedback response (a subset of haptics), haptics have very minimally been explored by artists as a tool for authoring immersive arts. The proposed project aims to create low-cost affordable haptic devices with one or mutliple degrees of freedom and authoring tools for artists to implement into their artwork immersive haptics and interactive audio. In the near future, artists can then use the proposed toolkit to benefit from Haptic floors being deployed into immersive art spaces like the Dome at SAT.\nThe authoring tools Feelix, a ”haptic authoring tool developed to support the design and integration of force feedback and shape change in user interfaces”, and ForceHost, a toolchain that uses the functional sound synthesis and processing programming language FAUST that compiles firmware for audio-haptic applications, will be extended for use with the DeformableHapticSurfaces, a work-in-progress and open-source toolkit for interactive multi-linear DoF deformable surfaces, to create and demo the proposed immersive haptic and audio interaction toolkit. Support for audio input will also be added into Feelix potentially with ForceHost as an input modality so that ForceHost developed tools can interface their audio with the Haptic floor.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2023_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/YmhosRcH",
          "proposal_id": "P8OFraFy",
          "short_description": "Haptics, as a multidisciplinary field, has applications within the Medical, Consumer and Entertainment domains, to name a few, and despite their...",
          "slug": "audio-to-haptic-interaction-design-with-forcehost-and-feelix-supporting-deformablehapticsurfaces",
          "status": "completed",
          "student_name": "Maxwell Gentili-Morin",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Audio to Haptic interaction design with ForceHost and Feelix supporting DeformableHapticSurfaces"
        },
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-contributions/pedroansa1/work-product-pedroansa1-building-immersive-learning-experiences/",
          "description": "The proposed project aims to support teaching in hybrid telepresence settings by developing and embedding examplar explorable explanations into the Satellite hub, an immersive 3D social web environment developed at the Society for Arts and Technology (SAT) using Mozilla Hubs. Explorable explanations are interactive documents that use visualizations, animations, and simulations to help learners understand complex concepts. The project will integrate explorable explanations in Mozilla Hubs used for Satellite by adapting Spoke to defining dynamic assets with interactivity. This integration will allow for the creation of immersive and interactive educational environments that cater to diverse abilities in learning and allow learners to play with parameters to deepen their understanding of the concepts.\nThe project seeks to create synergy by interconnecting various cultural and artistic contents in the Satellite hub and embedding explorable explanations to enhance the learning experience. The explorable explanations will be designed to cater to a broad audience and facilitate access to learning.\nThe project will involve prototyping examplar explorable explanations and embedding them in Satellite / Mozilla Hubs, leveraging the capabilities of the platform to provide an immersive learning experience. The aim is to enhance the teaching of digital arts concepts in hybrid telepresence campus settings and promote interactivity and innovation in teaching, leveraging emerging technologies to enhance the learning experience. Ultimately, the project will contribute to promoting interactivity and innovation in teaching, leveraging emerging technologies to enhance the learning experience.",
          "difficulty": "advanced",
          "id": "proj_society-for-arts-and_2023_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/ZJTheXqH",
          "proposal_id": "SJZUE3X5",
          "short_description": "The proposed project aims to support teaching in hybrid telepresence settings by developing and embedding examplar explorable explanations into the...",
          "slug": "building-immersive-learning-experiences",
          "status": "completed",
          "student_name": "Pedro Andrade",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "Building Immersive Learning Experiences"
        },
        {
          "code_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-contributions/knockerpulsar/replace-opengl-by-a-multi-api-rendering-library/",
          "description": "The Splash projection mapping software allows for controlling multiple video projectors together to build a single projection. It is able to adapt to virtually any real geometry, as long as the surface is diffuse. Its rendering engine is built using the OpenGL API. It currently runs on platforms capable of running Linux and handling an OpenGL 4.5\ncontext, and has been tested successfully on x86_64 (with NVIDIA, AMD and Intel graphic cards) and aarch64 (with NVIDIA graphic cards).\n\nHowever to be able to: a) optimize it further and b) support more platforms (for example Raspberry Pi), it would be interesting to support more graphics API. To do this it is envisioned to replace direct use of OpenGL with an intermediate, multi-API rendering library.\n\n\n\nWork Done: 1) Investigating available rendering libraries and APIs supported by the Raspberry Pi 4. 2) Modifying Splash's OpenGL 4.5 code to work with OpenGL ES and thus the Raspberry Pi 4. 3) Merging the old OpenGL and the new OpenGL ES codepaths to allow one binary to run both. 4). Pulling out graphics code such that application classes are only responsible for application logic, they just use the provided graphics API specific implementation. 5) Rudamentary performance profiling. 6) Bug fixes to application and graphics code.",
          "difficulty": "medium",
          "id": "proj_society-for-arts-and_2023_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/UlGCj7qL",
          "proposal_id": "T4SU35pF",
          "short_description": "The Splash projection mapping software allows for controlling multiple video projectors together to build a single projection. It is able to adapt to...",
          "slug": "replace-opengl-by-a-multi-api-rendering-library-in-splash",
          "status": "completed",
          "student_name": "Tarek Yasser",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "ui",
            "ux"
          ],
          "title": "Replace OpenGL by a multi-API rendering library in Splash."
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2023/organizations/society-for-arts-and-technology-sat"
    },
    "year_2024": null,
    "year_2025": {
      "num_projects": 4,
      "projects": [
        {
          "code_url": null,
          "description": "This project comprises two parts: prototyping a real-time Python pipeline for object detection on point cloud data, and integrating it into LivePose.  \r\n\r\nThe goal is to build a functional pipeline that reads streaming point cloud data from various sources (e.g., Orbbec Femto Megas, Intel RealSense, stereo cameras), runs deep learning inference using models like PointNet++, applies filters for smoothing and tracking, and streams the results via OSC. During prototyping, the system will be evaluated across multiple models, input types, and hardware configurations to assess real-time performance and reliability. Once validated, the pipeline will be integrated into LivePose, leveraging its modular infrastructure for input handling, filtering, and output.  \r\n\r\nBy the end of the project, LivePose will support end-to-end processing of streaming 3D point cloud data in at least one working configuration, with clear documentation and performance benchmarks to guide future development and extension.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2025_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/miLarupg",
          "proposal_id": "kBjp18hw",
          "short_description": "This project comprises two parts: prototyping a real-time Python pipeline for object detection on point cloud data, and integrating it into LivePose....",
          "slug": "object-detection-on-point-cloud-streams",
          "status": "in-progress",
          "student_name": "Eddie Liu",
          "student_profile": null,
          "tags": [
            "python",
            "cloud",
            "ui"
          ],
          "title": "Object Detection on Point Cloud Streams"
        },
        {
          "code_url": null,
          "description": "This project aims to enhance fulldome content rendering in ossia score by integrating advanced ray-tracing render pipelines. \nTraditional fulldome rendering requires multiple render passes, which leads to high computational overhead. By leveraging ray-tracing techniques on modern graphics APIs (e.g. DirectX, Vulkan), this project will enable single-pass fulldome rendering, significantly improving efficiency and render quality. \n  \n\nThe project will explore and conduct comparative analysis on various rendering pipelines and graphics APIs, including HDR, meshlet-based, volumetric, and SDF-based techniques, with a focus on their suitability and performance for fulldome rendering within ossia score. \nFinal Deliverables: \n[1] Integration of the ray-tracing fulldome rendering pipeline into ossia score for better performance and quality. \n[2] Systematic comparison and analysis of different ray-tracing fulldome rendering pipelines. \n[3] Open-source release of the codebase and benchmarking results of Deliverable [2]. \n[4] Documentation of rendering pipeline design, challenges faced, and solutions implemented. \n  \n\nMentors: Jean-Michaël Celerier and Manuel Bolduc",
          "difficulty": "advanced",
          "id": "proj_society-for-arts-and_2025_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/YLjs5vlA",
          "proposal_id": "0Ygnd4lW",
          "short_description": "This project aims to enhance fulldome content rendering in ossia score by integrating advanced ray-tracing render pipelines. Traditional fulldome...",
          "slug": "exploring-advanced-rendering-pipelines-for-fulldome-content-with-ray-tracing-in-ossia-score",
          "status": "in-progress",
          "student_name": "Lejie LIU",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "ui"
          ],
          "title": "Exploring Advanced Rendering Pipelines for Fulldome Content with Ray-tracing in ossia score"
        },
        {
          "code_url": null,
          "description": "This project addresses the limited accessibility of the powerful Puara-gestures library for creating high-level gestural descriptors within modern audiovisual workflows. The goal is to bridge this gap by porting Puara-gestures to the Avendish framework, enabling its integration with the visual programming environment ossia score.\r\n\r\n This will be achieved by wrapping each Puara-gestures class in Avendish to create standardized objects that can be compiled into native ossia score processes. \r\nThese objects will allow artists and designers to seamlessly incorporate sophisticated gestural analysis into interactive multimedia environments using ossia score's visual interface and timeline-based orchestration. Key deliverables include Avendish wrappers for various Puara-gestures descriptors (motion, orientation, etc.), corresponding native ossia score objects with user-friendly controls, composite objects for complex gesture recognition, comprehensive documentation and examples, and performance optimizations.\r\n Optionally, the project will explore the creation of VST/LV2 plugins for wider compatibility with Digital Audio Workstations. This integration will democratize access to advanced gestural control, benefiting artists, performers, instrument designers, and the academic research community.",
          "difficulty": "advanced",
          "id": "proj_society-for-arts-and_2025_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/skzm18oM",
          "proposal_id": "TMfBCGBn",
          "short_description": "This project addresses the limited accessibility of the powerful Puara-gestures library for creating high-level gestural descriptors within modern...",
          "slug": "porting-puara-gestures-to-avendish-and-creating-ossia-score-objects",
          "status": "in-progress",
          "student_name": "yashtiwari9182",
          "student_profile": null,
          "tags": [
            "ml"
          ],
          "title": "Porting Puara-gestures to Avendish and creating ossia score objects"
        },
        {
          "code_url": null,
          "description": "Managing large libraries of media files efficiently is challenging, especially when dealing with diverse formats, extracting metadata, and performing fast searches. This project aims to develop a lightweight CLI-based Library\nContent Manager that automatically scans directories, extracts metadata using FFmpeg/TagLib, and organises media\nfiles in an SQLite database for efficient querying.\n\nTo ensure speed, this tool will monitor directories in real-time (using inotify/ReadDirectoryChangesW), and to encourage personalisation, custom user-defined tagging, and fast search/filtering via structured queries will be implemented. Users can interact through a Command Line Interface or an IPC-based API, enabling integration with\nexternal tools. A dedicated plug-in will be developed to allow a seamless and intuitive integration with ossia score.\n\nPerformance is crucial and thus multithreading, batch processing, and optimised indexing strategies will be implemented to ensure responsiveness even when handling large libraries.\n\nExisting solutions are either GUI-heavy or lack structured querying but this tool offers the best of both worlds.\nTherefore, this tool will provide a robust and extensible solution for organisation of media files, making it a great\nhelp for artists, developers and creative professionals.",
          "difficulty": null,
          "id": "proj_society-for-arts-and_2025_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/LDlr5P8f",
          "proposal_id": "VeyR3KSV",
          "short_description": "Managing large libraries of media files efficiently is challenging, especially when dealing with diverse formats, extracting metadata, and performing...",
          "slug": "a-responsive-and-lightweight-cli-for-managing-media-libraries",
          "status": "in-progress",
          "student_name": "zepredos",
          "student_profile": null,
          "tags": [
            "api",
            "ml",
            "ai",
            "database",
            "ui"
          ],
          "title": "A Responsive And Lightweight CLI For Managing Media Libraries"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2025/organizations/society-for-arts-and-technology-sat"
    }
  },
  "first_time": false,
  "contact": {
    "email": null,
    "guide_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2025-news/2025-share/",
    "ideas_url": "https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/categories/ideas/",
    "irc_channel": "https://app.element.io/#/room/#sat-gsoc:matrix.org",
    "mailing_list": "https://gitlab.com/sat-mtl/collaborations/google-summer-of-code/-/issues"
  },
  "social": {
    "blog": null,
    "discord": null,
    "facebook": null,
    "github": null,
    "gitlab": null,
    "instagram": null,
    "linkedin": null,
    "mastodon": null,
    "medium": null,
    "reddit": null,
    "slack": null,
    "stackoverflow": null,
    "twitch": null,
    "twitter": "https://twitter.com/SATmontreal",
    "youtube": null
  },
  "meta": {
    "version": 1,
    "generated_at": "2026-01-25T15:28:54.222Z"
  }
}