{
  "id": "692251d853dd9d7326d33dea",
  "slug": "jax-and-keras",
  "name": "JAX and Keras",
  "category": "Artificial Intelligence",
  "description": "Python libraries for large-scale machine learning",
  "image_url": "https://summerofcode.withgoogle.com/media/org/jax-and-keras/7czut6i8nmxbyzkp-360.png",
  "img_r2_url": "https://pub-268c3a1efc8b4f8a99115507a760ca14.r2.dev/jax-and-keras.webp",
  "logo_r2_url": null,
  "url": "https://docs.jax.dev/",
  "active_years": [
    2025
  ],
  "first_year": 2025,
  "last_year": 2025,
  "is_currently_active": true,
  "technologies": [
    "python",
    "machine learning",
    "ai"
  ],
  "topics": [
    "machine learning",
    "Jax",
    "keras",
    "PYTHON LIBRARY"
  ],
  "total_projects": 4,
  "stats": {
    "avg_projects_per_appeared_year": 4,
    "projects_by_year": {
      "year_2016": null,
      "year_2017": null,
      "year_2018": null,
      "year_2019": null,
      "year_2020": null,
      "year_2021": null,
      "year_2022": null,
      "year_2023": null,
      "year_2024": null,
      "year_2025": 4
    },
    "students_by_year": {
      "year_2016": null,
      "year_2017": null,
      "year_2018": null,
      "year_2019": null,
      "year_2020": null,
      "year_2021": null,
      "year_2022": null,
      "year_2023": null,
      "year_2024": null,
      "year_2025": 4
    },
    "total_students": 4
  },
  "years": {
    "year_2016": null,
    "year_2017": null,
    "year_2018": null,
    "year_2019": null,
    "year_2020": null,
    "year_2021": null,
    "year_2022": null,
    "year_2023": null,
    "year_2024": null,
    "year_2025": {
      "num_projects": 4,
      "projects": [
        {
          "code_url": null,
          "description": "Large Language Model (LLM) serving frameworks like vLLM must deliver high throughput and low latency to justify the substantial cost of GPU and TPU deployments. This project targets systematic benchmarking, profiling, and optimization of vLLM, one of the most widely adopted open-source LLM serving systems. The goal is to push its performance limits under realistic workloads and contribute meaningful improvements back to the community.",
          "difficulty": null,
          "id": "proj_jax-and-keras_2025_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/hn9YIJn3",
          "proposal_id": "kOadfIEt",
          "short_description": "Large Language Model (LLM) serving frameworks like vLLM must deliver high throughput and low latency to justify the substantial cost of GPU and TPU...",
          "slug": "benchmarking-and-improving-large-language-model-serving-performance",
          "status": "in-progress",
          "student_name": "Bob Chen",
          "student_profile": null,
          "tags": [],
          "title": "Benchmarking and Improving Large Language Model Serving Performance"
        },
        {
          "code_url": null,
          "description": "VB-LoRA (Vector Bank Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that extends LoRA (Low-Rank Adaptation) by introducing a shared vector bank and a differentiable top-k selection mechanism. This allows dense layers to adapt to new tasks by training a small set of parameters while keeping the original weights frozen. The goal of this project is to implement VB-LoRA in Keras, integrating it into layers—Dense, EinsumDense, BaseConv, Embedding, and Attention —and extending it to the backbones of model architectures in Keras Hub. The implementation will involve initializing VB-LoRA parameters, creating a vector bank and logits, computing low-rank matrices, and ensuring seamless integration with comprehensive testing and documentation.",
          "difficulty": null,
          "id": "proj_jax-and-keras_2025_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/vcABnzOo",
          "proposal_id": "TX7HLzOK",
          "short_description": "VB-LoRA (Vector Bank Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that extends LoRA (Low-Rank Adaptation) by introducing a...",
          "slug": "integrating-vblora-into-keras",
          "status": "in-progress",
          "student_name": "Dhiraj BM",
          "student_profile": null,
          "tags": [
            "ml",
            "ai"
          ],
          "title": "Integrating VBLoRA into Keras"
        },
        {
          "code_url": null,
          "description": "As interest in large language models continues to grow, many open-source implementations exist across various frameworks—but accessible, modular, and well-documented implementations in Flax, built on JAX, remain limited. This lack of resources makes it more difficult for developers and researchers to understand, experiment with, and extend LLMs in the JAX ecosystem.\nThis project addresses that gap by implementing one or more open LLM architectures in Flax. The goal is to provide reference models that are easy to read, modify, and build upon. The project will also include a comprehensive test suite to ensure reliability, and an interactive educational notebook that explains the architecture, shows how to load pretrained weights, and demonstrates how to run inference.",
          "difficulty": "beginner",
          "id": "proj_jax-and-keras_2025_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/GKcZodMM",
          "proposal_id": "lAqe8CeL",
          "short_description": "As interest in large language models continues to grow, many open-source implementations exist across various frameworks—but accessible, modular, and...",
          "slug": "implement-open-llm-models-with-jax-and-flax",
          "status": "in-progress",
          "student_name": "Megan Andrews",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Implement open LLM models with JAX and Flax"
        },
        {
          "code_url": null,
          "description": "Current Frontier LLMs are really bad at assisting with writing JAX code due to the sparse resources spread across multiple different forums and platforms which are often inacessible/unscrapeable (such as discord) by conventional search engine indexers or web scrapers. Thus they usually do not make it into the final pretraining datasets, hurting frontier models' performance on those frameworks.\n\nI want to fine-tune OSS models on JAX code, where the data is manually scraped/collected by hand from diverse sources like YT talks, GitHub discussions, Books & Papers and even XLA HLO dumps to enhance the models' understanding of the compiler internals and improve debugging capabilities as well as better guide the developers when dealing with the internals.\n\nAnother key focus would be incorporating commonly-used popular libraries like Equinox, Optax and Scalax as well because current SOTA LLMs are not familiar with these (relatively) niche but handy libraries and thus prove to be less helpful and prone to hallucinating components of the ecosystem that don't exist.\n\nThis pretrained model will be available externally through an API or a CLI interface like Aider/Claude Code styled wherein it'd be accessible to everyone regardless of their development environment or IDE/Editor.\n\nI would also experiment with new ideas such as integrating type-annotations, Array and PyTree shape annotations and packaging other auxiliary information embedded in the AST to provide further context to the model apart from the raw code itself.",
          "difficulty": null,
          "id": "proj_jax-and-keras_2025_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/HdO43aXo",
          "proposal_id": "75vXYPfZ",
          "short_description": "Current Frontier LLMs are really bad at assisting with writing JAX code due to the sparse resources spread across multiple different forums and...",
          "slug": "pomni-jax-code-assistant",
          "status": "in-progress",
          "student_name": "neel04",
          "student_profile": null,
          "tags": [
            "web",
            "api",
            "ai",
            "ui",
            "ux"
          ],
          "title": "\"Pomni\" - JAX Code Assistant"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2025/organizations/jax-and-keras"
    }
  },
  "first_time": true,
  "contact": {
    "email": null,
    "guide_url": "https://forms.gle/yJaBBLJfqTJfWfyt6",
    "ideas_url": "https://docs.google.com/document/d/16uAZEldrXUBHjrszSO22Hr81OmVSjzVXza0szZl6Fxw/edit?usp=sharing",
    "irc_channel": null,
    "mailing_list": "jax-keras-gsoc-2025@google.com"
  },
  "social": {
    "blog": "https://deepmind.google/discover/blog/using-jax-to-accelerate-our-research/",
    "discord": null,
    "facebook": null,
    "github": null,
    "gitlab": null,
    "instagram": null,
    "linkedin": null,
    "mastodon": null,
    "medium": null,
    "reddit": null,
    "slack": null,
    "stackoverflow": null,
    "twitch": null,
    "twitter": null,
    "youtube": null
  },
  "meta": {
    "version": 1,
    "generated_at": "2026-01-25T15:28:53.283Z"
  }
}