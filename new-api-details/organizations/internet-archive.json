{
  "id": "692251d753dd9d7326d33de5",
  "slug": "internet-archive",
  "name": "Internet Archive",
  "category": "Science and medicine",
  "description": "Universal Access to All Knowledge",
  "image_url": "https://summerofcode.withgoogle.com/media/org/internet-archive/uzbgzbb9tvp81c2i.png",
  "img_r2_url": "https://pub-268c3a1efc8b4f8a99115507a760ca14.r2.dev/internet-archive.webp",
  "logo_r2_url": null,
  "url": "http://archive.org",
  "active_years": [
    2017,
    2018,
    2019,
    2020,
    2021,
    2023,
    2024,
    2025
  ],
  "first_year": 2017,
  "last_year": 2025,
  "is_currently_active": true,
  "technologies": [
    "python",
    "javascript",
    "pthon",
    "node.js",
    "php",
    "elasticsearch",
    "golang",
    "hadoop",
    "go"
  ],
  "topics": [
    "web archiving",
    "non-profit",
    "web extensions",
    "voice apps",
    "library",
    "archive",
    "web archives",
    "books",
    "web archving",
    "archiving",
    "media"
  ],
  "total_projects": 26,
  "stats": {
    "avg_projects_per_appeared_year": 3.25,
    "projects_by_year": {
      "year_2016": null,
      "year_2017": 2,
      "year_2018": 5,
      "year_2019": 2,
      "year_2020": 3,
      "year_2021": 2,
      "year_2022": null,
      "year_2023": 5,
      "year_2024": 2,
      "year_2025": 5
    },
    "students_by_year": {
      "year_2016": null,
      "year_2017": 2,
      "year_2018": 5,
      "year_2019": 2,
      "year_2020": 3,
      "year_2021": 2,
      "year_2022": null,
      "year_2023": 5,
      "year_2024": 2,
      "year_2025": 5
    },
    "total_students": 26
  },
  "years": {
    "year_2016": null,
    "year_2017": {
      "num_projects": 2,
      "projects": [
        {
          "code_url": "https://github.com/abhidas17695/GSoC-project",
          "description": "<p>The 5 advanced features added to the Chrome extension of the Wayback Machine are focused on users who wish to research the history of the web .</p>\n<ol>\n<li>The timestamp feature lets a user see the timestamp of external page elements in a playback page.</li>\n<li>The radial tree feature lets a user visualize the hierarchy of pages in a website as a radial tree also known as a sequences sunburst diagram.</li>\n<li>The 'get books' feature lets a user navigate to a book on Internet Archive by analyzing a Wikipedia page for book titles. </li>\n<li>The ORCID feature displays the ORCID accounts of people whose names match a name given as input.</li>\n<li>The robust links feature is useful for web developers who wish to fight the phenomenon of link rot. This feature extends the Memento Project by Los Alamos National Laboratory and Old Dominion University.</li>\n</ol>\n",
          "difficulty": "advanced",
          "id": "proj_internet-archive_2017_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6684594220826624/",
          "proposal_id": null,
          "short_description": "The 5 advanced features added to the Chrome extension of the Wayback Machine are focused on users who wish to research the history of the web .\n\nThe...",
          "slug": "addition-of-advanced-features-to-the-existing-google-chrome-extension-of-the-wayback-machine",
          "status": "completed",
          "student_name": "abhidas17695",
          "student_profile": null,
          "tags": [
            "web"
          ],
          "title": "Addition of advanced features to the existing Google Chrome extension of the Wayback Machine"
        },
        {
          "code_url": "https://github.com/rakesh-chinta/GSOC-Project-Internet-Archive",
          "description": "<p>This project focuses on \"Building a Chrome extension for the Way Back Machine, Adding new feature for the way back machine's chrome extension.\"</p>\n<p>The original chrome extension was released during January 2017 where the unofficial and pre-release version was developed and released as an open source contribution at mid-2016.\nThis is an extended update for the January 2017 chrome extension of the way back machine.\nThis project has already several repositories in GitHub, where previous selected projects for building a chrome extension for the way back machine alongside with integrations, are available at the link below:</p>\n<p><a href=\"http://www.github.com/rakesh-chinta/WayBackMachineChromeExt/\" target=\"_blank\">http://www.github.com/rakesh-chinta/WayBackMachineChromeExt/</a></p>\n<p>The project's Goals and criteria for success:</p>\n<ul>\n<li>Integration with the Wayback Machine’s Site Search</li>\n<li>Support for social sharing, including Twitter and Facebook</li>\n<li>Display Tweets about a URL for a given date range</li>\n<li>Provide user with visual feedback based on if we have archived a URL or not</li>\n<li>Provide user with a one-click Summary view of a given site (Alexa Rank Whois, etc.)</li>\n<li>Automatically archive URLs that are not in the Wayback Machine</li>\n<li>Quick access to an overview of all the Archives for a specific URL</li>\n</ul>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2017_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2017/projects/6528240499818496/",
          "proposal_id": null,
          "short_description": "This project focuses on \"Building a Chrome extension for the Way Back Machine, Adding new feature for the way back machine's chrome extension.\"\nThe...",
          "slug": "internet-archive-gsoc-project-proposal",
          "status": "completed",
          "student_name": "Rakesh N Chinta",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "Internet Archive GSOC Project Proposal"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2017/organizations/5373878247358464/"
    },
    "year_2018": {
      "num_projects": 5,
      "projects": [
        {
          "code_url": "https://verseczy.github.io",
          "description": "<p>Since it contains more than 600 billion archives today, these huge amounts of webpages need to be analyzed. Produce the reports about hosts and domains of the archives, and helping to inform web archiving efforts. In addition, this analysis will combine with third party data in order to improve this archiving program.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2018_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6433015437721600/",
          "proposal_id": null,
          "short_description": "Since it contains more than 600 billion archives today, these huge amounts of webpages need to be analyzed. Produce the reports about hosts and...",
          "slug": "create-a-map-of-the-web",
          "status": "completed",
          "student_name": "Zhengyue Cheng",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "Create a “Map” of the Web"
        },
        {
          "code_url": "https://docs.google.com/spreadsheets/d/1-Ujft-n7l4dV0Z8TXJfqgVkTqjB_1jm-RmB0W_xr9LQ/edit?usp=sharing",
          "description": "<p>I would like to help expand the current Google Home Actions for the Internet Archive. I believe that the introduction of voice prompts would make many of the Internet Archive’s large data sets more accessible and easy to access. As a digital library, I think it’s only fitting that the Internet Archive have it’s very own digital librarian. With some help from the Google Home, this could make for an awesome interactive experience.</p>\n",
          "difficulty": "beginner",
          "id": "proj_internet-archive_2018_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/5684192989937664/",
          "proposal_id": null,
          "short_description": "I would like to help expand the current Google Home Actions for the Internet Archive. I believe that the introduction of voice prompts would make...",
          "slug": "proposal-for-internet-archive",
          "status": "completed",
          "student_name": "Dave Barry",
          "student_profile": null,
          "tags": [],
          "title": "Proposal for Internet Archive"
        },
        {
          "code_url": "https://github.com/ftsalamp/gsoc2018-InternetArchive",
          "description": "<p>This proposal will present the project goal of idea 5, which is to integrate the web-monitoring software into the Wayback Machine and help to further advance it, and how this can be achieved. More specifically, the following solutions are suggested as part of the implementation plan: making the login screen optional, querying the CDX server and not the web-monitoring-db, implementing a mechanism for making changes to the UI easily, improving the code so it is able to handle a lot more data, using The Wayback Machine's calendar to present a webpage capture's dates, and keeping only the required project components. In addition, this proposal contains a link to a server hosting my implementation of some of the proposed solutions. Apart from that, a timeline that presents a rough plan of how the project would be implemented is included. Last but not least, biographical information is provided.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2018_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/4727113080897536/",
          "proposal_id": null,
          "short_description": "This proposal will present the project goal of idea 5, which is to integrate the web-monitoring software into the Wayback Machine and help to further...",
          "slug": "idea-5-integrate-the-scanner-software-into-the-wayback-machine",
          "status": "completed",
          "student_name": "Fotios Tsalampounis",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "Idea 5 - Integrate the “Scanner” software into the Wayback Machine"
        },
        {
          "code_url": "http://blog.openlibrary.org/2018/08/12/google-summer-of-code-2018/",
          "description": "<p>Currently Open Library has over 1.75M Members who borrow millions of books every year. This Project looks to develop Open Library further by improving it with the addition of new options to the newly added feature of Public Reading Logs and creating a new UI for merging Workflows. Furthermore, adding more books to Open Library via ONIX and the Internet Archive Wishlist and creating a standard bot to import new book metadata records from 3rd party partners (like BetterWorldBooks APIs, Onix Feeds, MARCs). The Project proposes to document and establish a pipeline to help other librarians to add books much easily in the furture. The project also focuses on improving System Reliability by shifting from a Vagrant based system to Docker and Ansible.</p>\n<p>Currently the Project focuses on three main categories:</p>\n<ol>\n<li><strong>Ingestion</strong> - Using the Internet Archive Wishlist and ONIX Records to import new books into Open Library</li>\n<li><strong>Devops</strong> - Using Docker and Ansible for better System Reliability</li>\n<li><strong>Features</strong>  - Adding Features to Open Library for Merging Works and Public Reading Log</li>\n</ol>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2018_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/4762123708137472/",
          "proposal_id": null,
          "short_description": "Currently Open Library has over 1.75M Members who borrow millions of books every year. This Project looks to develop Open Library further by...",
          "slug": "building-the-worlds-open-library-together",
          "status": "completed",
          "student_name": "salman-bhai",
          "student_profile": null,
          "tags": [
            "api",
            "ai",
            "docker",
            "ui"
          ],
          "title": "Building the World’s Open Library, Together"
        },
        {
          "code_url": "https://github.com/anishsarangi/GSOC-Project",
          "description": "<p>The Wayback Machine chrome extension helps to make the web more reliable by detecting dead web pages and offering to replay archived versions of them. When previously valid URLs don’t respond, but instead return a result code of 404, it can be seen through wayback machine. Some of the features, where I would like to work on are -</p>\n<ol>\n<li>Fixing the existing bugs</li>\n<li>Enabling the extension to automatically archive a page if not previously archived</li>\n<li>Redesigning About page .</li>\n<li>Adding Context Feature with user’s control\n  (a). Users can control which how they want to see the context.\n  (b). Users can select if they want the context to auto-update while visiting any new URL.\n  (c). Users can select which context screens they want to see.They can select individual context screen also.</li>\n<li>Adding Wayback Machine Overview, annotations of the current URL and Domain as a Context Screen.</li>\n<li>Adding SimilarWeb Overview of the Current URL as a Context Screen .</li>\n<li>Adding tag-cloud of a host as a Context Screen.</li>\n<li>Adding Single Window Context and getting all the Context Screen datas as JSON(Alexa, Whois, Twitter, Wayback Machine Overview, Annotations and Tagcloud).</li>\n<li>Adding Books Feature</li>\n</ol>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2018_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2018/projects/6554366517444608/",
          "proposal_id": null,
          "short_description": "The Wayback Machine chrome extension helps to make the web more reliable by detecting dead web pages and offering to replay archived versions of...",
          "slug": "continue-development-of-the-chrome-extension-wayback-machine",
          "status": "completed",
          "student_name": "Anish Kumar Sarangi",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "cloud"
          ],
          "title": "Continue development of the Chrome extension “Wayback Machine”"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2018/organizations/6298343651147776/"
    },
    "year_2019": {
      "num_projects": 2,
      "projects": [
        {
          "code_url": "https://github.com/koderjoker/gsoc-internetarchive-2019",
          "description": "<p>The Internet Archive is a non-profit library committed to Universal Access to Knowledge. In its 23 years of operation, the Internet Archive and its community have archived millions of web pages, books, texts, audio tracks, videos, images, and software. These items are made freely available to the public to consume and repurpose through the Internet Archive’s flagship website, Archive.org.</p>\n<p>However, in the midst of the Archive's archiving efforts, it is imperative that users are able to easily navigate through the website's content and find what they are searching for, regardless of the device used or disabilities that they may have to contend with.</p>\n<p>Thus, the following project aims at improving Archive.org's navigation by researching, prototyping and developing relevant components at IAUX, the Internet Archive’s monorepo for Archive.org UX development and prototyping.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2019_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6347257455050752/",
          "proposal_id": null,
          "short_description": "The Internet Archive is a non-profit library committed to Universal Access to Knowledge. In its 23 years of operation, the Internet Archive and its...",
          "slug": "improving-site-navigation-for-archiveorg",
          "status": "completed",
          "student_name": "Kanchan Joshi",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ux"
          ],
          "title": "Improving site navigation for Archive.org"
        },
        {
          "code_url": "https://lsingh123.github.io/gsoc19/",
          "description": "<p>The Wayback Machine archives billions of webpages, with vast numbers being added to the collection every day, including news sources. Though the crawling operations are quite successful, there is still work to be done to improve the quality of webpages archived by checking for “broken” or “bad” pages. I propose developing a methodology to assess the quality of news sources, both in terms of bias/factual reporting and in terms of technical viability such as geoblocking, paywall blocks, and CAPTCHAs. I will then use this methodology to create a tool that can automatically assess quality and take an appropriate action, perhaps preserving an error message and reason for failure in the Wayback Machine. This tool will ideally be integrated with the Wayback Machine’s web crawlers. I will also use my findings to develop recommendations for circumvention tactics, such as paying for subscriptions to important news sources or negotiating agreements with CAPTCHA services to allow the Internet Archive, as a public good, to bypass the CAPTCHA. The goal of the project is to develop a system to think about the quality of archived webpages and then create tools to automate quality assessment.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2019_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2019/projects/6251189589508096/",
          "proposal_id": null,
          "short_description": "The Wayback Machine archives billions of webpages, with vast numbers being added to the collection every day, including news sources. Though the...",
          "slug": "good-archiving-improving-quality-and-efficiency-of-news-archiving",
          "status": "completed",
          "student_name": "Lavanya Singh",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "“Good” Archiving: Improving Quality and Efficiency of News Archiving"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2019/organizations/5931882041049088/"
    },
    "year_2020": {
      "num_projects": 3,
      "projects": [
        {
          "code_url": "https://blog.openlibrary.org/2020/08/29/giacomo-cignoni-my-internship-at-the-internet-archive/",
          "description": "<p>The scope of this project is to improve  the BookReader framework in two ways.:</p>\n<ul>\n<li>The first one is implementing a series of comprehensive end to end tests for the framework.  These tests will be made using JS-based test applications which will be chosen based on a written evaluation of the capabilities and functionalities.</li>\n<li>The second part consists in improving BookReader in order to make the framework more easily implementable outside archive.org. Currently BookReader is very book-centered, but it may find uses for other sources such as newspapers and magazines. The current number of easy-to-change parameters available in the BookReader demos and templates is quite small, and doesn’t allow great customization. This part of the project consists in improving and expanding the parameter system (included in the E2E testing). This allows for a more widespread use of the framework even for not-so-expert users and for other sources such as newspapers and magazines.</li>\n</ul>\n",
          "difficulty": "beginner",
          "id": "proj_internet-archive_2020_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5458130244206592/",
          "proposal_id": null,
          "short_description": "The scope of this project is to improve  the BookReader framework in two ways.:\n\nThe first one is implementing a series of comprehensive end to end...",
          "slug": "bookreader-e2e-testing-and-improvements",
          "status": "completed",
          "student_name": "Giacomo Cignoni",
          "student_profile": null,
          "tags": [
            "ai",
            "ui"
          ],
          "title": "BookReader E2E testing and improvements"
        },
        {
          "code_url": "https://blog.openlibrary.org/2020/08/29/google-summer-of-code-2020-book-lovers-adoption/",
          "description": "<p>OpenLibrary.org is the world’s best-kept library secret: Let’s make it easier for book lovers to discover and get started with Open Library.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2020_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/5124430243561472/",
          "proposal_id": null,
          "short_description": "OpenLibrary.org is the world’s best-kept library secret: Let’s make it easier for book lovers to discover and get started with Open Library.",
          "slug": "adoption-by-book-lovers",
          "status": "completed",
          "student_name": "Tabish Shaikh",
          "student_profile": null,
          "tags": [],
          "title": "Adoption by Book Lover’s"
        },
        {
          "code_url": "https://github.com/tikhsuP/GSoC-2020-Project",
          "description": "<p>The Wayback Machine Extension is a powerful tool that collects and stores information about the search queries you make and the web pages you view to provide you with information about those pages and to help the Wayback Machine archive the Web. Driven by a powerful idea and an enthusiastic team, this extension has successfully been archiving the internet since 2017.\nThe Wayback Machine Extension plays a significant role in providing people a way to archive and preserve the publicly available web pages and mapping the evolution of the World Wide Web.</p>\n<h4>This Project focused on Improving the Wayback Machine Extension by:</h4>\n<ol>\n<li>Enhancing the existing Search Box feature.</li>\n<li>Adding 'Fact Check' feature</li>\n<li>Adding 'Bulk Save' feature</li>\n<li>Re-defining the display of existing Contexts</li>\n<li>More enhancements:<ul>\n<li>Solving the existing issues to ensure full functionality</li>\n<li>Code optimization and improvement</li>\n<li>UI improvements</li>\n<li>Searching, inspecting and fixing bugs</li>\n<li>Interacting with users about issues and opportunities to make the app better</li>\n<li>Writing Tests</li>\n<li>Adding support for Edge</li>\n<li>Making the Internet a BETTER place to surf.</li>\n</ul>\n</li>\n</ol>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2020_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2020/projects/6018075498905600/",
          "proposal_id": null,
          "short_description": "The Wayback Machine Extension is a powerful tool that collects and stores information about the search queries you make and the web pages you view to...",
          "slug": "continue-development-of-the-chrome-extension-wayback-machine",
          "status": "completed",
          "student_name": "Pushkit Kapoor",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "ui"
          ],
          "title": "Continue development of the Chrome extension \"Wayback Machine\""
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2020/organizations/5652935767228416/"
    },
    "year_2021": {
      "num_projects": 2,
      "projects": [
        {
          "code_url": "https://blog.openlibrary.org/2021/08/23/gsoc-2021-making-books-lendable/",
          "description": "<p>To create an open, community-powered Book Genome Project which enables deeper, faster and more holistic understanding of a book's unique characteristics.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2021_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/6029482359521280/",
          "proposal_id": null,
          "short_description": "To create an open, community-powered Book Genome Project which enables deeper, faster and more holistic understanding of a book's unique...",
          "slug": "open-book-genome-project",
          "status": "completed",
          "student_name": "Nolan Windham",
          "student_profile": null,
          "tags": [],
          "title": "Open Book Genome Project"
        },
        {
          "code_url": "https://github.com/users/graceCXY/projects/1",
          "description": "<p>Algorithmic approach to automate the process of adding hyperlinks of archived books to Wikipedia pages in an attempt to drastically simplify and alter the form of manual labor, if not completely reduce it.</p>\n",
          "difficulty": null,
          "id": "proj_internet-archive_2021_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2021/projects/5138192222650368/",
          "proposal_id": null,
          "short_description": "Algorithmic approach to automate the process of adding hyperlinks of archived books to Wikipedia pages in an attempt to drastically simplify and...",
          "slug": "turn-all-references-blue",
          "status": "completed",
          "student_name": "Xin Yue Chen",
          "student_profile": null,
          "tags": [],
          "title": "Turn All References Blue"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2021/organizations/6193723385315328/"
    },
    "year_2022": null,
    "year_2023": {
      "num_projects": 5,
      "projects": [
        {
          "code_url": "https://archive.org/details/tarb-gsoc-2023-content-drift",
          "description": "The \"Turn All References Blue\" (TARB) project at the Internet Archive aims to tackle the issue of content drift in Wikipedia hyperlinks. Content drift refers to the phenomenon where the content behind a URL changes over time, leading to a discrepancy between the original cited information and the current content. The project leverages machine learning models like BERT and BART to identify and flag such instances of content drift. It then suggests replacements for the content drifted URLs with stable references from the Wayback Machine, ensuring the reliability and integrity of Wikipedia citations.",
          "difficulty": null,
          "id": "proj_internet-archive_2023_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/7YkLVuE5",
          "proposal_id": "hjBth6Mf",
          "short_description": "The \"Turn All References Blue\" (TARB) project at the Internet Archive aims to tackle the issue of content drift in Wikipedia hyperlinks. Content...",
          "slug": "turn-all-references-blue-tarb",
          "status": "completed",
          "student_name": "darahaas15",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Turn All References Blue (TARB)"
        },
        {
          "code_url": "https://blog.openlibrary.org/2023/08/25/google-summer-of-code-2023-supercharging-subject-pages",
          "description": "The objective of this proposal is to enhance the user experiences for readers of Open Library through a two-prong approach. \n\nThe first prong, Improving Site Performance, addresses the poor site performance that negatively affects our patrons’ experience. This prong takes a metric-based approach to identify the key bottlenecks of performance using various performance measurement tools. This metric-based approach allows us to identify the areas for performance optimisation and quantify the effects of our changes. \n\nThe second prong, Implementing Subject Tags, seeks to offer a more resourceful alternative to the Subject tags that are currently implemented for books in Open Library. The current Subject tags, which are implemented as a list of strings, are limited in extensibility and scalability, and are not an efficient tool in categorising and providing information about books. Subject Tags will be implemented as first-class objects, allowing for metadata to be included within tags. The metadata will provide relationships and hierarchy between subjects, internationalisation (i18n) information, additional description about tags and many other extensible information fields. Upon the success of this implementation, its utility will catalyse improvement within numerous avenues in the Open Library system which includes the search engine, Subject page interface, content advisory and many more. \n\nThese two prongs work together to improve the overall patron experience, making the site more enjoyable to use and enriching the information about books.",
          "difficulty": null,
          "id": "proj_internet-archive_2023_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/9Tzc7AuO",
          "proposal_id": "5Ac4H080",
          "short_description": "The objective of this proposal is to enhance the user experiences for readers of Open Library through a two-prong approach. The first prong,...",
          "slug": "improving-site-performance-and-implementing-subject-tags",
          "status": "completed",
          "student_name": "Jayden Teoh Jing Xiang",
          "student_profile": null,
          "tags": [],
          "title": "Improving Site Performance and Implementing Subject Tags"
        },
        {
          "code_url": "https://github.com/krish-910/GSoC_IA",
          "description": "Problem to Solve :\n\nBuild a system which performs the following:\nEnables full-text search of documents; filtered by agency, date, views\nRecommends similar documents based on semantic similarity of document contents\nExtracts significant document features such as topics, persons, locations, etc.\nClusters and sorts documents by artifact topic (i.e. aerospace, environmental, nuclear, civil rights, etc.)\nIncluding: Natural language summaries for each artifact in the collection.\n\nSolution:\nTo build the requested system, we will need several components and technologies. Here are the basic steps to achieve the functionality we require:\n\n    Data ingestion: Loading the documents from a data source and extract the relevant information such as the document ID, title, content, agency, date, and views.\n\n    Full-text search: Indexing the documents in a search engine such as Elasticsearch and enable full-text search with filters on agency, date, and views.\n\n    Semantic similarity: Using a natural language processing model such as Doc2Vec to calculate the semantic similarity between documents and recommend similar documents.\n\n    Document feature extraction: Using a named entity recognition model such as Spacy to extract significant document features such as topics, persons, locations, etc.\n\n    Clustering and sorting: Using a clustering algorithm such as KMeans to group the documents into clusters based on artifact topic and sort them accordingly.\n\n    Natural language summaries: we can use a summarization algorithm such as Gensim to generate natural language summaries for each document in the collection.",
          "difficulty": null,
          "id": "proj_internet-archive_2023_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/MMFObnCX",
          "proposal_id": "H8hGggUS",
          "short_description": "Problem to Solve : Build a system which performs the following: Enables full-text search of documents; filtered by agency, date, views Recommends...",
          "slug": "idea-1-improving-democracys-library",
          "status": "completed",
          "student_name": "Krish Shewani",
          "student_profile": null,
          "tags": [
            "ui"
          ],
          "title": "Idea 1 : Improving Democracy’s Library"
        },
        {
          "code_url": "https://archive.org/details/tarb-gsoc-2023-soft-404",
          "description": "Have you ever encountered a frustrating error when browsing a webpage, or perhaps landed on a page expecting to find information about the first computer, only to discover something completely unrelated?\n\n\n\n\n\nWhen we request a webpage, the server communicates its status through an HTTP response code. If the page is broken or doesn't exist, the server responds with a 404 error code or some other error code in the 4xx and 5xx range, indicating an issue, and 200 OK (or some other status codes in the 2xx range) if everything is good. However, a common challenge arises when the server returns a 200 OK in every case, making it difficult for computers to differentiate between a healthy page and a problematic one. If these problematic pages go unnoticed and are not replaced with their healthy counterparts over time, they contribute to a pervasive issue known as \"link rot,\" leading to the gradual loss of valuable internet information.\n\n\n\nThis work addresses such issues by performing content analysis of webpages to identify potential soft-404s that would otherwise go undetected due to the wrong HTTP status code they return. A prior version of such soft-404 pages is then looked up in the Wayback Machine to replace the link in the Wikipedia article with, if present.",
          "difficulty": null,
          "id": "proj_internet-archive_2023_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/2g9hvKN4",
          "proposal_id": "aONFAZZd",
          "short_description": "Have you ever encountered a frustrating error when browsing a webpage, or perhaps landed on a page expecting to find information about the first...",
          "slug": "wayback-machine-turn-all-references-blue-tarb",
          "status": "completed",
          "student_name": "Pranjal20",
          "student_profile": null,
          "tags": [
            "web"
          ],
          "title": "Wayback Machine - Turn All References Blue (TARB)"
        },
        {
          "code_url": "https://git.archive.org/soumya/gsoc_ia_1/-/blob/main/finevadoc.md",
          "description": "The objective of this project is to create a document retrieval and recommendation system powered by NLP, with an emphasis on allowing effective token extraction from massive amounts of text data. In addition to automatically identifying and retrieving related or quoted documents, the system will enable users to look for and retrieve pertinent documents based on author, date, and other metadata. To do this, we will create a set of APIs and ingestion protocols that allow text data to be parsed and tagged while utilising ensemble learning, W2V, and LSA/LDA algorithms to produce tailored suggestions based on user preferences. Additionally, we'll put in place a user-friendly front-end interface that allows API-based queries, extensive unit testing, and optimisation.\n\nA completely working document retrieval and recommendation system, as well as a strong set of APIs and ingestion protocols for processing sizable amounts of text data, will be included in our deliverables. In-depth documentation outlining the system's design, implementation, and usage policies will also be made available. Our overall goal is to offer organisations looking to extract valuable insights from their text data a creative and practical solution that will allow for improved business outcomes and more informed decision-making.",
          "difficulty": null,
          "id": "proj_internet-archive_2023_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2023/projects/pKvq93oa",
          "proposal_id": "klVQhQsy",
          "short_description": "The objective of this project is to create a document retrieval and recommendation system powered by NLP, with an emphasis on allowing effective...",
          "slug": "democratizing-government-documents-a-comprehensive-system-for-access-discovery-and-analysis",
          "status": "completed",
          "student_name": "SoumyaGupta",
          "student_profile": null,
          "tags": [
            "api",
            "ai"
          ],
          "title": "Democratizing Government Documents: A Comprehensive System for Access, Discovery, and Analysis"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2023/organizations/internet-archive"
    },
    "year_2024": {
      "num_projects": 2,
      "projects": [
        {
          "code_url": "https://docs.google.com/document/d/1fGMYRArEm2YLD4eQb2mvjHXJbNkq9Suycid_Tge1wHo/edit?usp=sharing",
          "description": "he AI-Enhanced Historical Knowledge Graph (AI-HKG) project proposes the development of an advanced, AI-powered knowledge graph that interlinks historical data sourced from the Internet Archive and the Wayback Machine. \nThis project aims to construct a searchable, interactive platform that visualizes the interconnectedness of web pages, academic papers, books, and multimedia content based on context, content, and temporal relevance. Through the innovative use of NLP and knowledge graph technology, AI-HKG aspires to revolutionize access to historical information, enabling a deeper understanding of the evolution of knowledge over time.",
          "difficulty": "advanced",
          "id": "proj_internet-archive_2024_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/4HEo3IFh/",
          "proposal_id": null,
          "short_description": "he AI-Enhanced Historical Knowledge Graph (AI-HKG) project proposes the development of an advanced, AI-powered knowledge graph that interlinks...",
          "slug": "ai-enhanced-historical-knowledge-graph",
          "status": "completed",
          "student_name": "Bhaskar Joshi",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "AI-Enhanced Historical Knowledge Graph"
        },
        {
          "code_url": "https://gist.github.com/haruki25/80881c64437de3e9c866ed75f5cad7f6",
          "description": "This proposal outlines a project to develop an AI-powered tool, named Wayback Machine CoPilot, aimed at enhancing the functionality of the Wayback Machine. Leveraging Python and AI technologies, the project will integrate with Wayback Machine APIs to access historical versions of web pages and provide intelligent temporal analysis, summarization, classification, and change detection. The proposed tool will empower users to explore and understand the evolution of web content over time, making archived web pages more accessible and informative. With a strong background in software engineering and experience in application development, the applicant is well-equipped to undertake this project and contribute meaningfully to the Wayback Machine community.",
          "difficulty": null,
          "id": "proj_internet-archive_2024_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/archive/2024/projects/GaKaHxiA/",
          "proposal_id": null,
          "short_description": "This proposal outlines a project to develop an AI-powered tool, named Wayback Machine CoPilot, aimed at enhancing the functionality of the Wayback...",
          "slug": "enhancing-the-wayback-machine-ai-driven-analysis-and-exploration-of-web-archives",
          "status": "completed",
          "student_name": "Piyush Soni",
          "student_profile": null,
          "tags": [
            "python",
            "web",
            "api",
            "ai",
            "ui"
          ],
          "title": "Enhancing the Wayback Machine: AI-driven Analysis and Exploration of Web Archives"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/archive/2024/organizations/internet-archive/"
    },
    "year_2025": {
      "num_projects": 5,
      "projects": [
        {
          "code_url": null,
          "description": "Despite capturing millions of web pages daily, The Wayback Ma\nchine may fail to archive many important resources before they dis\nappear. This proposal outlines a comprehensive technical pipeline to\noptimize web archiving resources through intelligent URL ingestion,\nquality assessment, and prioritization. I propose a machine learning-\ndriven pipeline that integrates diverse signal sources to identify at-risk,\nhigh-value web content while filtering low-quality material. The im\nplementation includes a unified data collection framework, real-time\nquality scoring system, dynamic priority queue, and continuous eval\nuation mechanisms. This project, therefore, aims to significantly im\nprove the Internet Archive’s ability to preserve historically significant\nweb content before it potentially vanishes.",
          "difficulty": null,
          "id": "proj_internet-archive_2025_001",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/MFyduPDw",
          "proposal_id": "nHvQJuVP",
          "short_description": "Despite capturing millions of web pages daily, The Wayback Ma chine may fail to archive many important resources before they dis appear. This...",
          "slug": "what-are-we-not-archiving-a-strategic-resource-allocation-proposal-for-the-wayback-machine",
          "status": "in-progress",
          "student_name": "Akshith Garapati",
          "student_profile": null,
          "tags": [
            "web",
            "ai"
          ],
          "title": "What Are We Not Archiving? A Strategic Resource Allocation Proposal for the Wayback Machine"
        },
        {
          "code_url": null,
          "description": "Since the rise of social media two decades ago, researchers have been eager to characterize its impact on society, particularly in the area of information diffusion. Yet, one major gap remains: its reach via television remains unmeasured. As a result, studying the mediated influence of social media has been an ongoing challenge for Social, Behavioral, & Economic (SBE) researchers. \n\nInternet Archive’s TV News Archive provides access to over 2.6 million U.S. news broadcasts dating back to 2009. We will use these TV news broadcasts to train object detection and image classification models. “SM LogoTrack” will focus on detecting social media platform logos, while “SM PostTrack” will detect social media post screenshots. \n\nBy developing robust visual detection models - \"SM LogoTrack\" and \"SM PostTrack\" our project will offer researchers and journalists new tools to systematically trace the flow of information from social media to television news. More importantly, this work directly contributes to the mission of the TV News Archive by enhancing the discoverability and analytical value of its vast video collections.",
          "difficulty": null,
          "id": "proj_internet-archive_2025_002",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/j0CKIRCi",
          "proposal_id": "kb7R8CCT",
          "short_description": "Since the rise of social media two decades ago, researchers have been eager to characterize its impact on society, particularly in the area of...",
          "slug": "detecting-social-media-content-in-tv-news-logo-and-post-screenshot-detection-models",
          "status": "in-progress",
          "student_name": "Himarsha",
          "student_profile": null,
          "tags": [
            "ai"
          ],
          "title": "Detecting Social Media Content in TV News: Logo and Post Screenshot Detection Models"
        },
        {
          "code_url": null,
          "description": "Libraries globally face challenges transitioning to digital lending due to restrictive publisher licensing and expensive, rigid vendor systems that limit control over digital collections. My project, Lenny, is an open-source, self-hosted lending system for Open Library, designed to empower libraries by enabling ownership of digital books and reducing reliance on costly licenses. Inspired by the Internet Archive and Koha, Lenny offers a fast, secure, and scalable solution for libraries in regions like the US, UK, Germany, and India.\n          Lenny will feature a FastAPI-based API for borrowing, returning, and waitlist management, supporting Readable, Borrowable, and Print-Disabled books. It includes LCP DRM and Geo-IP compliance for security, a PostgreSQL database with triggers for loan expiration, and an OPDS feed for integration with Open Library and Thorium. A React-based dashboard will let librarians set rules like loan durations, while Docker with Alpine OS and Nginx ensures lightweight deployment. Lenny will provide libraries a cost-effective, flexible tool to manage digital collections, enhancing independence and access for patrons worldwide. will have a playground which will let users experience lenny system in action.",
          "difficulty": null,
          "id": "proj_internet-archive_2025_003",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/WcVnJVdw",
          "proposal_id": "jzurv9kh",
          "short_description": "Libraries globally face challenges transitioning to digital lending due to restrictive publisher licensing and expensive, rigid vendor systems that...",
          "slug": "lenny-open-source-lending-system-for-libraries",
          "status": "in-progress",
          "student_name": "Roni Bhakta",
          "student_profile": null,
          "tags": [
            "react",
            "api",
            "ai",
            "database",
            "docker"
          ],
          "title": "Lenny: Open Source Lending System for Libraries"
        },
        {
          "code_url": null,
          "description": "I would like to work with the Open Library team to create a plugin within the Internet Archive’s book reader app that would enable users to locally translate books and integrate the translated text to allow text-to-speech functionality for different languages. Using JavaScript and the Bergamot Translation model, this feature will parse the transparent text layer within the IA's book reader, feed the input into the translation model, and output the translated text onto the app. \r\n\r\nUsing a simplified calculation of books available for lending within the Open Library catalog, this project would expand the 3 million works available in English to at least 18 million titles across the 6 different languages supported by the Bergamot project, directly contributing to the Open Library's 2025 goal of offering more with less.\r\n\r\nThe primary deliverables for this project can be broken down into two major parts: implementing a translation plugin that allows users to select the source + target translation languages and intercepting the existing text-to-speech plugin to read the translated output from the Bergamot translation model. A future high-level deliverable for this project would allow volunteers to contribute their own translations for texts, which could allow higher quality translations to be provided to fellow patrons.",
          "difficulty": null,
          "id": "proj_internet-archive_2025_004",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/HZv00PgI",
          "proposal_id": "yyIpdtwP",
          "short_description": "I would like to work with the Open Library team to create a plugin within the Internet Archive’s book reader app that would enable users to locally...",
          "slug": "gsoc-2025-real-time-in-browser-translation-and-text-to-speech",
          "status": "in-progress",
          "student_name": "Sandy Chu",
          "student_profile": null,
          "tags": [
            "java",
            "javascript",
            "ai"
          ],
          "title": "GSoC 2025 - Real time In Browser Translation and Text-To-Speech"
        },
        {
          "code_url": null,
          "description": "Enhance Zeno by implementing headless browsing via Rod for improved archiving of dynamic websites, developing a precise CSS parser for accurate URL extraction from both inline and separate CSS files, addressing potential disk exhaustion when downloading large files, creating a dummy test site for better local testing, enabling HQ to control Zeno clients remotely via WebSocket, and addressing existing GitHub issues to improve overall stability and functionality.\n\nDeliverables:\n- A Zeno version with implemented headless browsing, configurable for URL/domain matching patterns\n- A CSS extractor capable of extracting URLs from inline and separate CSS files.\n- A Zeno version implementing the --max-content-length and --min-space-urgent parameters.\n- A dummy test site backend with various test endpoints.\n- A Zeno version that can be controlled by HQ via WebSocket.",
          "difficulty": null,
          "id": "proj_internet-archive_2025_005",
          "mentor_names": [],
          "project_url": "https://summerofcode.withgoogle.com/programs/2025/projects/afDanpOP",
          "proposal_id": "L68s9cOL",
          "short_description": "Enhance Zeno by implementing headless browsing via Rod for improved archiving of dynamic websites, developing a precise CSS parser for accurate URL...",
          "slug": "zeno-v2-enhancement-proposal-headless-mode-css-parser-and-more",
          "status": "in-progress",
          "student_name": "yzqzss",
          "student_profile": null,
          "tags": [
            "web",
            "ai",
            "backend"
          ],
          "title": "Zeno v2 Enhancement Proposal: Headless Mode, CSS Parser, and More"
        }
      ],
      "projects_url": "https://summerofcode.withgoogle.com/programs/2025/organizations/internet-archive"
    }
  },
  "first_time": false,
  "contact": {
    "email": "info@archive.org",
    "guide_url": null,
    "ideas_url": "https://docs.google.com/document/d/1oHNwPNYmHV5q3puBfv6IQFs-4gTe9XLN2iz2Lgse-1k/edit?tab=t.0",
    "irc_channel": null,
    "mailing_list": null
  },
  "social": {
    "blog": "http://blog.archive.org",
    "discord": null,
    "facebook": null,
    "github": null,
    "gitlab": null,
    "instagram": null,
    "linkedin": null,
    "mastodon": null,
    "medium": null,
    "reddit": null,
    "slack": null,
    "stackoverflow": null,
    "twitch": null,
    "twitter": "https://twitter.com/internetarchive",
    "youtube": null
  },
  "meta": {
    "version": 1,
    "generated_at": "2026-01-25T15:28:53.274Z"
  }
}